# Arbor Base - Development baseline configuration
# Medium-sized model for development and experimentation

model:
  vocab_size: 10000
  dim: 512
  num_layers: 8
  num_heads: 8
  ffn_dim: 2048
  max_seq_length: 1024
  dropout: 0.1
  attention_dropout: 0.1
  ffn_dropout: 0.1
  layer_norm_eps: 1e-5
  activation: "gelu"
  causal: true
  tie_word_embeddings: true

# Dataset configuration
data:
  type: "synthetic"
  vocab_size: 10000
  seq_length: 1024
  num_sequences: 50000
  batch_size: 32
  val_split: 0.1
  test_split: 0.05

# Training configuration
training:
  max_steps: 20000
  learning_rate: 1e-4
  weight_decay: 0.01
  optimizer: "adamw"
  betas: [0.9, 0.95]
  eps: 1e-8
  
  # Mixed precision and efficiency
  use_amp: true
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  
  # Scheduling
  use_scheduler: true
  warmup_steps: 2000
  
  # Logging and checkpointing
  log_every: 100
  eval_every: 1000
  save_every: 2000

# Growth configuration (disabled for baseline)
growth:
  enabled: false
  add_hidden: 256
  max_events: 6
  cooldown_steps: 2000
  layer_selection: "uniform"
  layers_per_event: 2
  new_param_lr_multiplier: 0.5
  
  triggers:
    - type: "plateau"
      window_steps: 1000
      eps: 0.001
    - type: "gradnorm"
      threshold: 1e-3
      window_steps: 500

# Infrastructure
infrastructure:
  device: "auto"
  num_workers: 4
  pin_memory: true
  
# Distributed training (optional)
distributed:
  enabled: false
  backend: "nccl"
  world_size: 1
  
# Logging
logging:
  log_level: "INFO"
  use_wandb: true
  wandb_project: "arbor-o1-base"
  checkpoint_dir: "checkpoints"
  
# Experiment tracking
experiment:
  name: "arbor_base_fixed"
  tags: ["development", "baseline", "fixed-size"]
  notes: "Medium-sized baseline model without growth for comparison"
