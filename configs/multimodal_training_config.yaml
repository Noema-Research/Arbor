# üß™ Multimodal Training Configuration

# This configuration extends the base Arbor training to support multimodal inputs
# Including images, audio, and video alongside text.

# üß† Base Model Configuration
model:
  vocab_size: 128000
  hidden_size: 1024
  num_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  max_position_embeddings: 8192
  
  # Dynamic growth settings
  growth:
    enabled: true
    factor: 2.0
    threshold: 0.95
    
  # Attention configuration
  attention_dropout: 0.1
  hidden_dropout: 0.1
  layer_norm_epsilon: 1e-5
  
# üé≠ Multimodal Configuration
multimodal:
  # Enable specific modalities
  enable_vision: true
  enable_audio: true
  enable_video: false  # Disabled for now
  
  # üñºÔ∏è Vision Configuration
  vision:
    encoder: "clip"              # Options: "clip", "eva", "siglip"
    image_size: 224
    patch_size: 16
    layers: 24
    embed_dim: 1024
    tokens_per_image: 256        # 14x14 patches + CLS token
    
    # Image preprocessing
    normalize_mean: [0.485, 0.456, 0.406]
    normalize_std: [0.229, 0.224, 0.225]
    
  # üéµ Audio Configuration  
  audio:
    encoder: "whisper"           # Options: "whisper", "wav2vec2"
    sample_rate: 16000
    chunk_length: 30             # seconds
    layers: 12
    embed_dim: 768
    tokens_per_second: 50
    
    # Audio preprocessing
    mel_bins: 80
    hop_length: 160
    
  # üé¨ Video Configuration (Future)
  video:
    encoder: "videomae"          # Options: "videomae", "timesformer" 
    frames: 16
    fps: 8
    layers: 12
    embed_dim: 768
    tokens_per_frame: 64
    
  # üîó Cross-Modal Fusion
  fusion:
    method: "attention"          # Options: "attention", "concat", "perceiver"
    layers: 4
    dropout: 0.1
    
    # Attention configuration
    num_heads: 16
    temperature: 0.07            # For contrastive learning
    
  # üìç Modality Embeddings
  modality_embeddings:
    text_id: 0
    image_id: 1
    audio_id: 2
    video_id: 3

# üìä Training Configuration
training:
  # Basic training settings
  learning_rate: 2e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Training schedule
  num_train_epochs: 3
  max_steps: 10000
  warmup_steps: 1000
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  
  # Batch configuration
  per_device_train_batch_size: 2    # Smaller for multimodal
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  dataloader_num_workers: 4
  
  # Mixed precision
  fp16: false
  bf16: true                        # Better for multimodal
  
  # Multimodal-specific settings
  multimodal_loss_weight: 1.0       # Weight for multimodal objectives
  contrastive_loss_weight: 0.1      # Weight for image-text contrastive loss
  
# üìÅ Dataset Configuration
datasets:
  # Multimodal datasets
  - name: "vision_language"
    source: "HuggingFaceM4/COCO"   # Example vision-language dataset
    streaming: true
    split: "train"
    text_column: "caption"
    image_column: "image"
    
  - name: "audio_language" 
    source: "librispeech_asr"       # Example audio-language dataset
    streaming: true
    split: "train"
    text_column: "text"
    audio_column: "audio"
    
  # Text-only datasets (for continued language modeling)
  - name: "text_only"
    source: "roneneldan/TinyStories"
    streaming: true
    split: "train"
    text_column: "text"
    weight: 0.3                     # 30% text-only data
    
# üîß Preprocessing Configuration
preprocessing:
  # Text preprocessing
  max_seq_length: 2048
  padding: "max_length"
  truncation: true
  
  # Image preprocessing
  image_augmentation:
    enabled: true
    random_crop: true
    random_flip: true
    color_jitter: 0.1
    
  # Audio preprocessing
  audio_augmentation:
    enabled: true
    noise_injection: 0.05
    speed_perturbation: [0.9, 1.1]
    
# üéØ Adaptive Context Configuration
adaptive_context:
  enabled: true
  
  # Context allocation per modality
  text_context_range: [512, 2048]
  image_context_per_image: 256
  audio_context_per_second: 50
  
  # Task-based context adaptation
  task_context_mapping:
    "multimodal_qa": 1024
    "image_captioning": 512
    "audio_transcription": 1024
    "video_understanding": 2048
    
# üíæ Saving and Checkpointing
saving:
  output_dir: "./checkpoints/arbor-multimodal"
  save_total_limit: 3
  save_safetensors: true
  save_strategy: "steps"
  
  # Multimodal-specific saving
  save_vision_encoder: true
  save_audio_encoder: true
  separate_modality_checkpoints: false
  
# üìä Evaluation Configuration
evaluation:
  # Evaluation tasks
  tasks:
    - "image_captioning"
    - "visual_question_answering" 
    - "audio_transcription"
    - "multimodal_reasoning"
    
  # Evaluation datasets
  eval_datasets:
    - name: "coco_captions"
      task: "image_captioning"
      metrics: ["bleu", "rouge", "cider"]
      
    - name: "vqa_v2"
      task: "visual_question_answering"
      metrics: ["accuracy", "f1"]
      
# ü§ó HuggingFace Integration
huggingface:
  upload:
    enabled: true
    repository: "noema-research/arbor-multimodal"
    token: "${HF_TOKEN}"
    private: false
    
  # Model card configuration
  model_card:
    model_type: "multimodal-transformer"
    modalities: ["text", "image", "audio"]
    languages: ["en"]
    license: "apache-2.0"
    
# üìà Monitoring and Logging
logging:
  # Weights & Biases
  wandb:
    enabled: true
    project: "arbor-multimodal"
    entity: "noema-research"
    tags: ["multimodal", "vision-language", "audio-language"]
    
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "./logs/arbor-multimodal"
    
  # Custom metrics
  custom_metrics:
    - "multimodal_alignment_score"
    - "cross_modal_retrieval_accuracy"
    - "vision_language_consistency"
    
# üîß Advanced Configuration
advanced:
  # Memory optimization
  gradient_checkpointing: true
  use_cpu_offload: false
  pin_memory: true
  
  # Multimodal optimization
  modality_parallel: true          # Process modalities in parallel
  late_fusion: true               # Fuse modalities late in the network
  sparse_attention: false         # Use sparse attention for efficiency
  
  # Debugging
  debug_mode: false
  log_attention_weights: false
  visualize_embeddings: false

# üéõÔ∏è Hardware Configuration
hardware:
  # GPU settings
  num_gpus: 1
  gpu_memory_fraction: 0.9
  mixed_precision_training: true
  
  # CPU settings
  num_cpu_cores: 8
  cpu_memory_limit: "32GB"
  
  # Multimodal-specific
  vision_encoder_device: "cuda:0"
  audio_encoder_device: "cuda:0"
  text_model_device: "cuda:0"

# Example usage:
# python train.py configs/multimodal_training_config.yaml
