{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09ada8f",
   "metadata": {},
   "source": [
    "# Arbor YAML Trainer Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the Arbor YAML training system with adaptive context windows. We'll show you how to:\n",
    "\n",
    "1. **Create and customize training configurations**\n",
    "2. **Train models with dynamic growth and adaptive context**\n",
    "3. **Monitor training progress and model adaptation**\n",
    "4. **Test the trained model with different task types**\n",
    "\n",
    "The YAML trainer makes it incredibly easy to train Arbor models - just edit a configuration file and run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add arbor to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if not Path(\"arbor\").exists():\n",
    "    print(\"‚ùå Please run this notebook from the arbor-o1-living-ai root directory\")\n",
    "    print(\"Current directory:\", Path.cwd())\n",
    "else:\n",
    "    print(\"‚úÖ Found arbor directory\")\n",
    "    print(\"üìç Working directory:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40776a90",
   "metadata": {},
   "source": [
    "## Step 1: Create Your Training Configuration\n",
    "\n",
    "The YAML trainer uses configuration files to specify everything about your training run. Let's start by examining and customizing a training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a72895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the example configuration first\n",
    "config_path = Path(\"configs/example_config.yaml\")\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_content = f.read()\n",
    "    \n",
    "    print(\"üìã Example Configuration Structure:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show first 30 lines to get an overview\n",
    "    lines = config_content.split('\\n')\n",
    "    for i, line in enumerate(lines[:30]):\n",
    "        print(f\"{i+1:2d}: {line}\")\n",
    "    \n",
    "    if len(lines) > 30:\n",
    "        print(f\"... ({len(lines) - 30} more lines)\")\n",
    "else:\n",
    "    print(\"‚ùå Example config not found. Let's create one!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom training configuration for this tutorial\n",
    "tutorial_config = {\n",
    "    'model': {\n",
    "        'vocab_size': 128000,\n",
    "        'hidden_size': 768,      # Smaller for tutorial\n",
    "        'num_layers': 12,        # Fewer layers for faster training\n",
    "        'num_heads': 12,\n",
    "        'intermediate_size': 3072,\n",
    "        'max_position_embeddings': 32768,  # 32K context for tutorial\n",
    "        \n",
    "        'growth': {\n",
    "            'enabled': True,\n",
    "            'factor': 1.5,         # Moderate growth\n",
    "            'max_steps': 4,\n",
    "            'threshold': 0.9\n",
    "        },\n",
    "        \n",
    "        'adaptive_context': {\n",
    "            'enabled': True,\n",
    "            'min_context_length': 512,\n",
    "            'max_context_length': 32768,\n",
    "            'context_router_layers': 3,\n",
    "            'task_types': ['chat', 'code', 'reasoning', 'document'],\n",
    "            'context_lengths': [512, 1024, 2048, 4096, 8192, 16384, 32768],\n",
    "            'hardware_aware': True,\n",
    "            'memory_threshold': 0.8\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'datasets': [\n",
    "        {\n",
    "            'name': 'tiny_stories',\n",
    "            'source': 'roneneldan/TinyStories',\n",
    "            'split': 'train[:1000]',  # Small subset for tutorial\n",
    "            'text_column': 'text',\n",
    "            'preprocessing': {\n",
    "                'prefix': 'Story: ',\n",
    "                'suffix': '',\n",
    "                'max_length': 1024\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'code_samples',\n",
    "            'source': 'codeparrot/github-code-clean',\n",
    "            'split': 'train[:500]',   # Small subset\n",
    "            'text_column': 'code',\n",
    "            'preprocessing': {\n",
    "                'prefix': '# Python Code:\\n',\n",
    "                'suffix': '\\n# End',\n",
    "                'max_length': 2048\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    'training': {\n",
    "        'output_dir': './tutorial_output',\n",
    "        'learning_rate': 3e-5,\n",
    "        'warmup_steps': 50,\n",
    "        'steps_per_dataset': 100,  # Short training for tutorial\n",
    "        'per_device_train_batch_size': 2,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'eval_steps': 25,\n",
    "        'save_steps': 50,\n",
    "        'logging_steps': 10,\n",
    "        'fp16': True,\n",
    "        'gradient_checkpointing': True\n",
    "    },\n",
    "    \n",
    "    'logging': {\n",
    "        'wandb': {\n",
    "            'enabled': False,  # Disabled for tutorial\n",
    "            'project': 'arbor-tutorial'\n",
    "        },\n",
    "        'console': {\n",
    "            'enabled': True,\n",
    "            'level': 'INFO'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'huggingface': {\n",
    "        'upload': {\n",
    "            'enabled': False,  # Disabled for tutorial\n",
    "            'repository': 'your-username/arbor-tutorial'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the tutorial configuration\n",
    "tutorial_config_path = Path(\"configs/tutorial_config.yaml\")\n",
    "with open(tutorial_config_path, 'w') as f:\n",
    "    yaml.dump(tutorial_config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Created tutorial configuration:\")\n",
    "print(f\"üìÅ Saved to: {tutorial_config_path}\")\n",
    "print(\"\\nüîß Configuration highlights:\")\n",
    "print(f\"   Model size: ~350M parameters (smaller for tutorial)\")\n",
    "print(f\"   Context range: 512 - 32K tokens\")\n",
    "print(f\"   Datasets: TinyStories + Code samples\")\n",
    "print(f\"   Training: 200 steps total (100 per dataset)\")\n",
    "print(f\"   Growth: Enabled with 1.5x factor\")\n",
    "print(f\"   Adaptive context: Enabled with 4 task types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177fc01",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the YAML Configuration\n",
    "\n",
    "Let's examine the key sections of our configuration and what they control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore each section of our configuration\n",
    "print(\"üîç Configuration Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Model configuration\n",
    "model_config = tutorial_config['model']\n",
    "print(\"\\nü§ñ MODEL CONFIGURATION:\")\n",
    "print(f\"   Vocabulary: {model_config['vocab_size']:,} tokens (Hermes-4-405B)\")\n",
    "print(f\"   Architecture: {model_config['num_layers']} layers √ó {model_config['hidden_size']} dim\")\n",
    "print(f\"   Parameters: ~{(model_config['hidden_size'] * model_config['num_layers'] * 4) / 1e6:.0f}M\")\n",
    "\n",
    "# Growth settings\n",
    "growth = model_config['growth']\n",
    "print(f\"\\nüå± GROWTH SETTINGS:\")\n",
    "print(f\"   Enabled: {growth['enabled']}\")\n",
    "print(f\"   Growth factor: {growth['factor']}x\")\n",
    "print(f\"   Max growth steps: {growth['max_steps']}\")\n",
    "print(f\"   Trigger threshold: {growth['threshold']}\")\n",
    "\n",
    "# Adaptive context\n",
    "adaptive = model_config['adaptive_context']\n",
    "print(f\"\\nüß† ADAPTIVE CONTEXT:\")\n",
    "print(f\"   Enabled: {adaptive['enabled']}\")\n",
    "print(f\"   Context range: {adaptive['min_context_length']:,} - {adaptive['max_context_length']:,}\")\n",
    "print(f\"   Task types: {len(adaptive['task_types'])} ({', '.join(adaptive['task_types'])})\")\n",
    "print(f\"   Context options: {len(adaptive['context_lengths'])} levels\")\n",
    "\n",
    "# Datasets\n",
    "datasets = tutorial_config['datasets']\n",
    "print(f\"\\nüìö DATASETS:\")\n",
    "for i, dataset in enumerate(datasets, 1):\n",
    "    print(f\"   {i}. {dataset['name']}: {dataset['source']} ({dataset['split']})\")\n",
    "    print(f\"      Max length: {dataset['preprocessing']['max_length']} tokens\")\n",
    "\n",
    "# Training\n",
    "training = tutorial_config['training']\n",
    "print(f\"\\nüéØ TRAINING:\")\n",
    "print(f\"   Learning rate: {training['learning_rate']}\")\n",
    "print(f\"   Batch size: {training['per_device_train_batch_size']} √ó {training['gradient_accumulation_steps']} = {training['per_device_train_batch_size'] * training['gradient_accumulation_steps']}\")\n",
    "print(f\"   Total steps: {len(datasets)} √ó {training['steps_per_dataset']} = {len(datasets) * training['steps_per_dataset']}\")\n",
    "print(f\"   Mixed precision: {training['fp16']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c365cf",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the YAML Trainer\n",
    "\n",
    "Now let's create and initialize the YAML trainer with our configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eaed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the YAML trainer\n",
    "try:\n",
    "    from arbor.train.yaml_trainer import ArborYAMLTrainer\n",
    "    print(\"‚úÖ Successfully imported ArborYAMLTrainer\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Make sure you're in the correct directory and arbor is in the path\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b52b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "print(\"üöÄ Initializing YAML Trainer...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    trainer = ArborYAMLTrainer(str(tutorial_config_path))\n",
    "    print(\"‚úÖ Trainer initialized successfully!\")\n",
    "    \n",
    "    # The trainer automatically validates the configuration\n",
    "    print(\"\\nüìã Configuration loaded and validated\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Trainer initialization failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da191e",
   "metadata": {},
   "source": [
    "## Step 4: Setup Components\n",
    "\n",
    "The trainer needs to setup several components before training. Let's do this step by step to see what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a464b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a: Setup tokenizer\n",
    "print(\"üì• Setting up tokenizer...\")\n",
    "try:\n",
    "    trainer.setup_tokenizer()\n",
    "    print(f\"‚úÖ Tokenizer ready: {len(trainer.tokenizer):,} vocabulary\")\n",
    "    \n",
    "    # Test the tokenizer\n",
    "    test_text = \"Hello, this is a test of the Hermes tokenizer!\"\n",
    "    tokens = trainer.tokenizer.encode(test_text)\n",
    "    print(f\"üß™ Test encoding: '{test_text}' ‚Üí {len(tokens)} tokens\")\n",
    "    print(f\"   First 10 tokens: {tokens[:10]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Tokenizer setup failed: {e}\")\n",
    "    print(\"This might be due to internet connectivity or HuggingFace access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20447431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b: Setup model\n",
    "print(\"\\nü§ñ Setting up Arbor model...\")\n",
    "try:\n",
    "    trainer.setup_model()\n",
    "    print(f\"‚úÖ Model created: {trainer.model.param_count():,} parameters\")\n",
    "    \n",
    "    # Show model architecture details\n",
    "    config = trainer.model.config\n",
    "    print(f\"\\nüìä Model details:\")\n",
    "    print(f\"   Architecture: {config.num_layers} layers\")\n",
    "    print(f\"   Hidden size: {config.dim}\")\n",
    "    print(f\"   Attention heads: {config.num_heads}\")\n",
    "    print(f\"   FFN dimension: {config.ffn_dim}\")\n",
    "    print(f\"   Max sequence length: {config.max_seq_length:,}\")\n",
    "    \n",
    "    # Show adaptive context info\n",
    "    if hasattr(trainer.model, 'get_context_info'):\n",
    "        context_info = trainer.model.get_context_info()\n",
    "        print(f\"\\nüß† Adaptive context info:\")\n",
    "        print(f\"   Enabled: {context_info['adaptive_context_enabled']}\")\n",
    "        if context_info['adaptive_context_enabled']:\n",
    "            print(f\"   Current context: {context_info['current_context_length']:,}\")\n",
    "            print(f\"   Context range: {context_info['min_context_length']:,} - {context_info['max_context_length']:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model setup failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4c: Load datasets\n",
    "print(\"\\nüìö Loading datasets...\")\n",
    "try:\n",
    "    trainer.load_datasets()\n",
    "    \n",
    "    print(f\"‚úÖ Datasets loaded: {len(trainer.datasets)}\")\n",
    "    \n",
    "    # Show dataset info\n",
    "    for name, dataset in trainer.datasets.items():\n",
    "        print(f\"\\nüìä Dataset: {name}\")\n",
    "        print(f\"   Size: {len(dataset):,} examples\")\n",
    "        \n",
    "        # Show a sample\n",
    "        if len(dataset) > 0:\n",
    "            sample = dataset[0]\n",
    "            input_ids = sample['input_ids']\n",
    "            decoded = trainer.tokenizer.decode(input_ids[:50])  # First 50 tokens\n",
    "            print(f\"   Sample: {decoded}...\")\n",
    "            print(f\"   Token length: {len(input_ids)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataset loading failed: {e}\")\n",
    "    print(\"This might be due to internet connectivity or dataset access issues\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539c8f4",
   "metadata": {},
   "source": [
    "## Step 5: Training Setup\n",
    "\n",
    "Before we start training, let's setup logging and create the trainer objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "print(\"üìä Setting up logging...\")\n",
    "try:\n",
    "    trainer.setup_logging()\n",
    "    print(\"‚úÖ Logging configured\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Logging setup had issues: {e}\")\n",
    "    print(\"Training can continue without full logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e943737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer for the first dataset to inspect the setup\n",
    "if trainer.datasets:\n",
    "    dataset_name = list(trainer.datasets.keys())[0]\n",
    "    print(f\"üîß Creating trainer for dataset: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        hf_trainer = trainer.create_trainer(dataset_name)\n",
    "        print(f\"‚úÖ HuggingFace trainer created\")\n",
    "        print(f\"   Training dataset: {len(hf_trainer.train_dataset):,} examples\")\n",
    "        if hf_trainer.eval_dataset:\n",
    "            print(f\"   Eval dataset: {len(hf_trainer.eval_dataset):,} examples\")\n",
    "        \n",
    "        # Show training arguments\n",
    "        args = hf_trainer.args\n",
    "        print(f\"\\n‚öôÔ∏è  Training arguments:\")\n",
    "        print(f\"   Output dir: {args.output_dir}\")\n",
    "        print(f\"   Learning rate: {args.learning_rate}\")\n",
    "        print(f\"   Batch size: {args.per_device_train_batch_size}\")\n",
    "        print(f\"   Max steps: {args.max_steps}\")\n",
    "        print(f\"   Save steps: {args.save_steps}\")\n",
    "        print(f\"   Eval steps: {args.eval_steps}\")\n",
    "        print(f\"   FP16: {args.fp16}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Trainer creation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374699f",
   "metadata": {},
   "source": [
    "## Step 6: Test Adaptive Context System\n",
    "\n",
    "Before training, let's test the adaptive context system with different types of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the adaptive context system\n",
    "print(\"üß† Testing Adaptive Context System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different types of inputs\n",
    "test_inputs = {\n",
    "    \"simple_chat\": \"Hello! How are you today?\",\n",
    "    \n",
    "    \"code_task\": \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# This is a recursive implementation\n",
    "# Could be optimized with dynamic programming\n",
    "for i in range(10):\n",
    "    print(f\"fib({i}) = {fibonacci(i)}\")\n",
    "\"\"\",\n",
    "    \n",
    "    \"reasoning_task\": \"\"\"\n",
    "Let me think through this step by step. If we have a logical puzzle where:\n",
    "1. All cats are animals\n",
    "2. Some animals are pets  \n",
    "3. No pets are wild\n",
    "4. Some cats are wild\n",
    "\n",
    "We need to determine if there's a contradiction. Let me analyze each statement carefully\n",
    "and see if they can all be true simultaneously. This requires careful logical reasoning\n",
    "to avoid making invalid inferences.\n",
    "\"\"\",\n",
    "    \n",
    "    \"long_document\": \"\"\"\n",
    "This is a comprehensive research paper on machine learning that covers multiple aspects\n",
    "of the field. The introduction provides background on artificial intelligence and its\n",
    "historical development. The methodology section describes various approaches including\n",
    "supervised learning, unsupervised learning, and reinforcement learning paradigms.\n",
    "\"\"\" + \" The paper continues with detailed analysis.\" * 50  # Make it longer\n",
    "}\n",
    "\n",
    "# Test each input type\n",
    "for task_type, text in test_inputs.items():\n",
    "    print(f\"\\nüîç Testing: {task_type}\")\n",
    "    print(f\"Input length: {len(text)} characters\")\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = trainer.tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    token_count = input_ids.shape[1]\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    \n",
    "    # Get current context info\n",
    "    initial_context = trainer.model.get_context_info()['current_context_length']\n",
    "    \n",
    "    # Test the model (this should trigger adaptive context)\n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # This forward pass will trigger context adaptation\n",
    "            outputs = trainer.model(input_ids, return_dict=True)\n",
    "            \n",
    "            # Check if context adapted\n",
    "            final_context = trainer.model.get_context_info()['current_context_length']\n",
    "            \n",
    "            print(f\"Context: {initial_context:,} ‚Üí {final_context:,} tokens\")\n",
    "            if final_context != initial_context:\n",
    "                print(f\"‚úÖ Context adapted for {task_type}\")\n",
    "            else:\n",
    "                print(f\"‚Üí Context unchanged for {task_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {task_type}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc04629",
   "metadata": {},
   "source": [
    "## Step 7: Run Training\n",
    "\n",
    "Now let's run the actual training! We'll train for a short period to demonstrate the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33735d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This will actually train the model!\n",
    "print(\"‚ö†Ô∏è  TRAINING WARNING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"The next cell will run actual training.\")\n",
    "print(\"This may take several minutes and will:\")\n",
    "print(\"‚Ä¢ Download datasets from HuggingFace\")\n",
    "print(\"‚Ä¢ Train the model for 200 steps\")\n",
    "print(\"‚Ä¢ Show parameter growth during training\")\n",
    "print(\"‚Ä¢ Save model checkpoints\")\n",
    "print(\"\")\n",
    "print(\"Set RUN_TRAINING = True to proceed\")\n",
    "\n",
    "RUN_TRAINING = False  # Set to True to actually run training\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    print(\"üöÄ Starting training pipeline...\")\n",
    "else:\n",
    "    print(\"üõë Training skipped (set RUN_TRAINING = True to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ef955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training if enabled\n",
    "if RUN_TRAINING:\n",
    "    print(\"üöÄ Starting Arbor YAML Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # This runs the complete training pipeline\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"\\nüéâ Training completed successfully!\")\n",
    "        \n",
    "        # Show final model stats\n",
    "        final_params = trainer.model.param_count()\n",
    "        print(f\"üìä Final model size: {final_params:,} parameters\")\n",
    "        \n",
    "        # Show training outputs\n",
    "        output_dir = Path(trainer.config.training_config['output_dir'])\n",
    "        if output_dir.exists():\n",
    "            saved_models = list(output_dir.glob(\"*/\"))\n",
    "            print(f\"üíæ Saved {len(saved_models)} model checkpoints:\")\n",
    "            for model_dir in saved_models:\n",
    "                print(f\"   üìÅ {model_dir.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    # Simulate what training would show\n",
    "    print(\"üìã Training simulation (would show):\")\n",
    "    print(\"üå± Initialized Arbor trainer with config: configs/tutorial_config.yaml\")\n",
    "    print(\"üì• Downloading fresh Hermes-4-405B tokenizer...\")\n",
    "    print(\"‚úÖ Successfully loaded fresh Hermes-4-405B tokenizer\")\n",
    "    print(\"‚úÖ Created Arbor model: 347,394,048 parameters\")\n",
    "    print(\"üß† Adaptive context enabled:\")\n",
    "    print(\"   Range: 512 - 32,768\")\n",
    "    print(\"   Supported tasks: 4\")\n",
    "    print(\"üå± Growth monitoring enabled:\")\n",
    "    print(\"   Factor: 1.5x\")\n",
    "    print(\"   Max steps: 4\")\n",
    "    print(\"üìö Loading datasets...\")\n",
    "    print(\"   ‚úÖ tiny_stories: 1,000 examples\")\n",
    "    print(\"   ‚úÖ code_samples: 500 examples\")\n",
    "    print(\"\")\n",
    "    print(\"üéØ Training on tiny_stories...\")\n",
    "    print(\"   üìä Parameters: 347,394,048 ‚Üí 347,894,048 (growth occurred)\")\n",
    "    print(\"   ‚úÖ tiny_stories complete!\")\n",
    "    print(\"\")\n",
    "    print(\"üéØ Training on code_samples...\")\n",
    "    print(\"   üìä Parameters: 347,894,048 ‚Üí 348,394,048 (growth occurred)\")\n",
    "    print(\"   ‚úÖ code_samples complete!\")\n",
    "    print(\"\")\n",
    "    print(\"üéâ Training pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfa1c1",
   "metadata": {},
   "source": [
    "## Step 8: Test the Trained Model\n",
    "\n",
    "Let's test our model (or demonstrate what testing would look like) with different task types to see how the adaptive context system works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model generation with different tasks\n",
    "print(\"üß™ Testing Trained Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_prompts = {\n",
    "    \"story\": \"Once upon a time, in a magical forest\",\n",
    "    \"code\": \"# Python function to calculate factorial\\ndef factorial(n):\",\n",
    "    \"reasoning\": \"Let me solve this step by step. The problem is:\",\n",
    "    \"chat\": \"User: What's the weather like today?\\nAssistant:\"\n",
    "}\n",
    "\n",
    "for task_type, prompt in test_prompts.items():\n",
    "    print(f\"\\nüéØ Testing {task_type} task:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = trainer.tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Show what would happen with adaptive context\n",
    "    print(f\"Input tokens: {input_ids.shape[1]}\")\n",
    "    \n",
    "    if RUN_TRAINING:\n",
    "        # Actually test the trained model\n",
    "        trainer.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Generate response\n",
    "                generated = trainer.model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=50,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                \n",
    "                # Decode response\n",
    "                response = trainer.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                print(f\"Generated: {response[len(prompt):]}\")\n",
    "                \n",
    "                # Show context info\n",
    "                context_info = trainer.model.get_context_info()\n",
    "                print(f\"Context used: {context_info['current_context_length']:,} tokens\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Generation failed: {e}\")\n",
    "    else:\n",
    "        # Simulate what would happen\n",
    "        simulated_contexts = {\"story\": 2048, \"code\": 4096, \"reasoning\": 8192, \"chat\": 1024}\n",
    "        print(f\"Would adapt context to: {simulated_contexts[task_type]:,} tokens\")\n",
    "        print(f\"Would generate appropriate {task_type} response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46680c",
   "metadata": {},
   "source": [
    "## Step 9: Configuration Tips and Best Practices\n",
    "\n",
    "Here are some tips for customizing your YAML training configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration tips and best practices\n",
    "print(\"üí° YAML Configuration Tips\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tips = {\n",
    "    \"Model Size\": {\n",
    "        \"Small (100M)\": \"hidden_size: 512, num_layers: 12\",\n",
    "        \"Medium (500M)\": \"hidden_size: 1024, num_layers: 24\", \n",
    "        \"Large (1B)\": \"hidden_size: 1536, num_layers: 32\"\n",
    "    },\n",
    "    \n",
    "    \"Context Lengths\": {\n",
    "        \"Short tasks\": \"max 4K tokens (chat, Q&A)\",\n",
    "        \"Medium tasks\": \"4K-16K tokens (code, creative)\",\n",
    "        \"Long tasks\": \"16K+ tokens (documents, reasoning)\"\n",
    "    },\n",
    "    \n",
    "    \"Growth Settings\": {\n",
    "        \"Conservative\": \"factor: 1.25, threshold: 0.95\",\n",
    "        \"Moderate\": \"factor: 1.5, threshold: 0.9\",\n",
    "        \"Aggressive\": \"factor: 2.0, threshold: 0.85\"\n",
    "    },\n",
    "    \n",
    "    \"Training Speed\": {\n",
    "        \"Fast prototyping\": \"small datasets, few steps\",\n",
    "        \"Full training\": \"complete datasets, many steps\", \n",
    "        \"Production\": \"multiple epochs, careful validation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, options in tips.items():\n",
    "    print(f\"\\nüîß {category}:\")\n",
    "    for option, description in options.items():\n",
    "        print(f\"   {option}: {description}\")\n",
    "\n",
    "print(f\"\\nüìã Common YAML patterns:\")\n",
    "print(\"\"\"\n",
    "# Enable everything for research\n",
    "adaptive_context: \n",
    "  enabled: true\n",
    "growth:\n",
    "  enabled: true\n",
    "logging:\n",
    "  wandb:\n",
    "    enabled: true\n",
    "\n",
    "# Minimal setup for testing  \n",
    "adaptive_context:\n",
    "  enabled: false\n",
    "growth:\n",
    "  enabled: false\n",
    "datasets:\n",
    "  - name: \"test\"\n",
    "    source: \"roneneldan/TinyStories\"\n",
    "    split: \"train[:100]\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97ae4e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned how to use the Arbor YAML training system. Here's what we covered:\n",
    "\n",
    "### ‚úÖ **What You Learned:**\n",
    "\n",
    "1. **üìã YAML Configuration** - How to create and customize training configs\n",
    "2. **üß† Adaptive Context** - Task-aware context window adaptation  \n",
    "3. **üå± Dynamic Growth** - Parameter expansion during training\n",
    "4. **üöÄ Easy Training** - One-command training with `python train.py config.yaml`\n",
    "5. **üß™ Testing & Validation** - How to test trained models\n",
    "\n",
    "### üéØ **Key Benefits:**\n",
    "\n",
    "- **Simple**: Just edit YAML, no complex code\n",
    "- **Powerful**: Full control over model architecture and training\n",
    "- **Smart**: Automatic context adaptation and parameter growth\n",
    "- **Production Ready**: HuggingFace integration and monitoring\n",
    "\n",
    "### üöÄ **Next Steps:**\n",
    "\n",
    "1. **Customize** your own YAML config for your use case\n",
    "2. **Train** with real datasets for your domain\n",
    "3. **Monitor** training with WandB integration\n",
    "4. **Deploy** trained models to HuggingFace Hub\n",
    "\n",
    "The YAML trainer makes it incredibly easy to experiment with cutting-edge transformer architectures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c65eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and final info\n",
    "print(\"üßπ Cleanup and Final Info\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show created files\n",
    "created_files = [\n",
    "    \"configs/tutorial_config.yaml\",\n",
    "    \"tutorial_output/\" if RUN_TRAINING else \"tutorial_output/ (would be created)\"\n",
    "]\n",
    "\n",
    "print(\"üìÅ Files created during this tutorial:\")\n",
    "for file in created_files:\n",
    "    if Path(file).exists() or \"would be\" in file:\n",
    "        print(f\"   ‚úÖ {file}\")\n",
    "\n",
    "print(f\"\\nüéØ To run training yourself:\")\n",
    "print(f\"   1. Set RUN_TRAINING = True in cell 15\")\n",
    "print(f\"   2. Or run: python train.py configs/tutorial_config.yaml\")\n",
    "\n",
    "print(f\"\\nüîß To customize:\")\n",
    "print(f\"   1. Edit configs/tutorial_config.yaml\")\n",
    "print(f\"   2. Adjust model size, datasets, training steps\")\n",
    "print(f\"   3. Enable/disable adaptive context and growth\")\n",
    "\n",
    "print(f\"\\nüìö For more examples:\")\n",
    "print(f\"   ‚Ä¢ Check configs/example_config.yaml\")\n",
    "print(f\"   ‚Ä¢ Run python demo_adaptive_context.py\")\n",
    "print(f\"   ‚Ä¢ See README.md for full documentation\")\n",
    "\n",
    "print(f\"\\nüå± Happy training with Arbor!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
