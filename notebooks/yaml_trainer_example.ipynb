{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09ada8f",
   "metadata": {},
   "source": [
    "# Arbor YAML Trainer Tutorial with Live Dashboard\n",
    "\n",
    "This notebook demonstrates how to use the Arbor YAML training system with adaptive context windows and **real-time monitoring dashboard**. We'll show you how to:\n",
    "\n",
    "1. **Create and customize training configurations**\n",
    "2. **Train models with dynamic growth and adaptive context**\n",
    "3. **Monitor training progress with live dashboard visualization**\n",
    "4. **Track model architecture changes in real-time**\n",
    "5. **Set up alerts and performance monitoring**\n",
    "6. **Test the trained model with different task types**\n",
    "\n",
    "## ğŸ†• **New Dashboard Features:**\n",
    "\n",
    "- **ğŸ“Š Live Metrics**: Real-time training loss, learning rate, and gradient monitoring\n",
    "- **ğŸ—ï¸ Architecture Visualization**: Interactive model structure with layer utilization heatmaps\n",
    "- **ğŸŒ± Growth Tracking**: Timeline of parameter and layer growth events\n",
    "- **ğŸ”” Alert System**: Automatic notifications for training anomalies and milestones\n",
    "- **ğŸ“ˆ Analytics**: Performance statistics, trends, and exportable reports\n",
    "\n",
    "The YAML trainer with Arbor dashboard makes it incredibly easy to train and monitor adaptive transformer models!\n",
    "\n",
    "## ğŸš€ **Quick Start:**\n",
    "1. Run this notebook to configure training\n",
    "2. Start the dashboard: `streamlit run arbor/tracking/dashboard.py`\n",
    "3. Watch your model train and grow in real-time at http://localhost:8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add arbor to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import Arbor tracking system\n",
    "try:\n",
    "    from arbor.tracking import TrainingMonitor\n",
    "    print(\"âœ… Arbor tracking system imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Arbor tracking not available: {e}\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if not Path(\"arbor\").exists():\n",
    "    print(\"âŒ Please run this notebook from the arbor-o1-living-ai root directory\")\n",
    "    print(\"Current directory:\", Path.cwd())\n",
    "else:\n",
    "    print(\"âœ… Found arbor directory\")\n",
    "    print(\"ğŸ“ Working directory:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40776a90",
   "metadata": {},
   "source": [
    "## Step 1: Create Your Training Configuration\n",
    "\n",
    "The YAML trainer uses configuration files to specify everything about your training run. Let's start by examining and customizing a training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a72895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the example configuration first\n",
    "config_path = Path(\"configs/example_config.yaml\")\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_content = f.read()\n",
    "    \n",
    "    print(\"ğŸ“‹ Example Configuration Structure:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show first 30 lines to get an overview\n",
    "    lines = config_content.split('\\n')\n",
    "    for i, line in enumerate(lines[:30]):\n",
    "        print(f\"{i+1:2d}: {line}\")\n",
    "    \n",
    "    if len(lines) > 30:\n",
    "        print(f\"... ({len(lines) - 30} more lines)\")\n",
    "else:\n",
    "    print(\"âŒ Example config not found. Let's create one!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "    'logging': {\n",
    "        'arbor_tracking': {\n",
    "            'enabled': True,\n",
    "            'save_dir': './training_logs',\n",
    "            'update_interval': 1.0,\n",
    "            'dashboard_port': 8501,\n",
    "            'alerts': {\n",
    "                'enabled': True,\n",
    "                'email_notifications': False,  # Set to True and configure for email alerts\n",
    "                'webhook_url': None  # Add webhook URL for notifications\n",
    "            }\n",
    "        },\n",
    "        'console': {\n",
    "            'enabled': True,\n",
    "            'level': 'INFO'\n",
    "        }\n",
    "    },"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177fc01",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the YAML Configuration\n",
    "\n",
    "Let's examine the key sections of our configuration and what they control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore each section of our configuration\n",
    "print(\"ğŸ” Configuration Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Model configuration\n",
    "model_config = tutorial_config['model']\n",
    "print(\"\\nğŸ¤– MODEL CONFIGURATION:\")\n",
    "print(f\"   Vocabulary: {model_config['vocab_size']:,} tokens (Hermes-4-405B)\")\n",
    "print(f\"   Architecture: {model_config['num_layers']} layers Ã— {model_config['hidden_size']} dim\")\n",
    "print(f\"   Parameters: ~{(model_config['hidden_size'] * model_config['num_layers'] * 4) / 1e6:.0f}M\")\n",
    "\n",
    "# Growth settings\n",
    "growth = model_config['growth']\n",
    "print(f\"\\nğŸŒ± GROWTH SETTINGS:\")\n",
    "print(f\"   Enabled: {growth['enabled']}\")\n",
    "print(f\"   Growth factor: {growth['factor']}x\")\n",
    "print(f\"   Max growth steps: {growth['max_steps']}\")\n",
    "print(f\"   Trigger threshold: {growth['threshold']}\")\n",
    "\n",
    "# Adaptive context\n",
    "adaptive = model_config['adaptive_context']\n",
    "print(f\"\\nğŸ§  ADAPTIVE CONTEXT:\")\n",
    "print(f\"   Enabled: {adaptive['enabled']}\")\n",
    "print(f\"   Context range: {adaptive['min_context_length']:,} - {adaptive['max_context_length']:,}\")\n",
    "print(f\"   Task types: {len(adaptive['task_types'])} ({', '.join(adaptive['task_types'])})\")\n",
    "print(f\"   Context options: {len(adaptive['context_lengths'])} levels\")\n",
    "\n",
    "# Datasets\n",
    "datasets = tutorial_config['datasets']\n",
    "print(f\"\\nğŸ“š DATASETS:\")\n",
    "for i, dataset in enumerate(datasets, 1):\n",
    "    print(f\"   {i}. {dataset['name']}: {dataset['source']} ({dataset['split']})\")\n",
    "    print(f\"      Max length: {dataset['preprocessing']['max_length']} tokens\")\n",
    "\n",
    "# Tracking and logging\n",
    "logging_config = tutorial_config['logging']\n",
    "print(f\"\\nğŸ“Š ARBOR TRACKING & LOGGING:\")\n",
    "arbor_tracking = logging_config['arbor_tracking']\n",
    "print(f\"   Dashboard enabled: {arbor_tracking['enabled']}\")\n",
    "print(f\"   Save directory: {arbor_tracking['save_dir']}\")\n",
    "print(f\"   Dashboard port: {arbor_tracking['dashboard_port']}\")\n",
    "print(f\"   Live monitoring: {arbor_tracking['update_interval']}s intervals\")\n",
    "print(f\"   Alerts enabled: {arbor_tracking['alerts']['enabled']}\")\n",
    "print(f\"   Console logging: {logging_config['console']['enabled']} ({logging_config['console']['level']})\")\n",
    "\n",
    "# Training\n",
    "training = tutorial_config['training']\n",
    "print(f\"\\nğŸ¯ TRAINING:\")\n",
    "print(f\"   Learning rate: {training['learning_rate']}\")\n",
    "print(f\"   Batch size: {training['per_device_train_batch_size']} Ã— {training['gradient_accumulation_steps']} = {training['per_device_train_batch_size'] * training['gradient_accumulation_steps']}\")\n",
    "print(f\"   Total steps: {len(datasets)} Ã— {training['steps_per_dataset']} = {len(datasets) * training['steps_per_dataset']}\")\n",
    "print(f\"   Mixed precision: {training['fp16']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c365cf",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the YAML Trainer\n",
    "\n",
    "Now let's create and initialize the YAML trainer with our configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eaed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the YAML trainer\n",
    "try:\n",
    "    from arbor.train.yaml_trainer import ArborYAMLTrainer\n",
    "    print(\"âœ… Successfully imported ArborYAMLTrainer\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"Make sure you're in the correct directory and arbor is in the path\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b52b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "print(\"ğŸš€ Initializing YAML Trainer...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    trainer = ArborYAMLTrainer(str(tutorial_config_path))\n",
    "    print(\"âœ… Trainer initialized successfully!\")\n",
    "    \n",
    "    # The trainer automatically validates the configuration\n",
    "    print(\"\\nğŸ“‹ Configuration loaded and validated\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Trainer initialization failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da191e",
   "metadata": {},
   "source": [
    "## Step 4: Setup Components\n",
    "\n",
    "The trainer needs to setup several components before training. Let's do this step by step to see what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a464b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a: Setup tokenizer\n",
    "print(\"ğŸ“¥ Setting up tokenizer...\")\n",
    "try:\n",
    "    trainer.setup_tokenizer()\n",
    "    print(f\"âœ… Tokenizer ready: {len(trainer.tokenizer):,} vocabulary\")\n",
    "    \n",
    "    # Test the tokenizer\n",
    "    test_text = \"Hello, this is a test of the Hermes tokenizer!\"\n",
    "    tokens = trainer.tokenizer.encode(test_text)\n",
    "    print(f\"ğŸ§ª Test encoding: '{test_text}' â†’ {len(tokens)} tokens\")\n",
    "    print(f\"   First 10 tokens: {tokens[:10]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Tokenizer setup failed: {e}\")\n",
    "    print(\"This might be due to internet connectivity or HuggingFace access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20447431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b: Setup model\n",
    "print(\"\\nğŸ¤– Setting up Arbor model...\")\n",
    "try:\n",
    "    trainer.setup_model()\n",
    "    print(f\"âœ… Model created: {trainer.model.param_count():,} parameters\")\n",
    "    \n",
    "    # Show model architecture details\n",
    "    config = trainer.model.config\n",
    "    print(f\"\\nğŸ“Š Model details:\")\n",
    "    print(f\"   Architecture: {config.num_layers} layers\")\n",
    "    print(f\"   Hidden size: {config.dim}\")\n",
    "    print(f\"   Attention heads: {config.num_heads}\")\n",
    "    print(f\"   FFN dimension: {config.ffn_dim}\")\n",
    "    print(f\"   Max sequence length: {config.max_seq_length:,}\")\n",
    "    \n",
    "    # Show adaptive context info\n",
    "    if hasattr(trainer.model, 'get_context_info'):\n",
    "        context_info = trainer.model.get_context_info()\n",
    "        print(f\"\\nğŸ§  Adaptive context info:\")\n",
    "        print(f\"   Enabled: {context_info['adaptive_context_enabled']}\")\n",
    "        if context_info['adaptive_context_enabled']:\n",
    "            print(f\"   Current context: {context_info['current_context_length']:,}\")\n",
    "            print(f\"   Context range: {context_info['min_context_length']:,} - {context_info['max_context_length']:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Model setup failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4c: Load datasets\n",
    "print(\"\\nğŸ“š Loading datasets...\")\n",
    "try:\n",
    "    trainer.load_datasets()\n",
    "    \n",
    "    print(f\"âœ… Datasets loaded: {len(trainer.datasets)}\")\n",
    "    \n",
    "    # Show dataset info\n",
    "    for name, dataset in trainer.datasets.items():\n",
    "        print(f\"\\nğŸ“Š Dataset: {name}\")\n",
    "        print(f\"   Size: {len(dataset):,} examples\")\n",
    "        \n",
    "        # Show a sample\n",
    "        if len(dataset) > 0:\n",
    "            sample = dataset[0]\n",
    "            input_ids = sample['input_ids']\n",
    "            decoded = trainer.tokenizer.decode(input_ids[:50])  # First 50 tokens\n",
    "            print(f\"   Sample: {decoded}...\")\n",
    "            print(f\"   Token length: {len(input_ids)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Dataset loading failed: {e}\")\n",
    "    print(\"This might be due to internet connectivity or dataset access issues\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539c8f4",
   "metadata": {},
   "source": [
    "## Step 5: Training Setup\n",
    "\n",
    "Before we start training, let's setup logging and create the trainer objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Arbor tracking and logging\n",
    "print(\"ğŸ“Š Setting up Arbor tracking system...\")\n",
    "try:\n",
    "    # Initialize TrainingMonitor with configuration\n",
    "    tracking_config = tutorial_config['logging']['arbor_tracking']\n",
    "    \n",
    "    # Create training monitor\n",
    "    training_monitor = TrainingMonitor(\n",
    "        save_dir=tracking_config['save_dir'],\n",
    "        update_interval=tracking_config['update_interval']\n",
    "    )\n",
    "    \n",
    "    # Setup alert system if enabled\n",
    "    if tracking_config['alerts']['enabled']:\n",
    "        training_monitor.setup_alerts(\n",
    "            email_enabled=tracking_config['alerts']['email_notifications'],\n",
    "            webhook_url=tracking_config['alerts']['webhook_url']\n",
    "        )\n",
    "    \n",
    "    print(\"âœ… Arbor tracking system initialized\")\n",
    "    print(f\"   ğŸ“Š Dashboard will be available at: http://localhost:{tracking_config['dashboard_port']}\")\n",
    "    print(f\"   ğŸ’¾ Metrics saved to: {tracking_config['save_dir']}\")\n",
    "    print(f\"   ğŸ”” Alerts enabled: {tracking_config['alerts']['enabled']}\")\n",
    "    \n",
    "    # Setup traditional logging as well\n",
    "    trainer.setup_logging()\n",
    "    print(\"âœ… Console logging configured\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Tracking setup had issues: {e}\")\n",
    "    print(\"Training can continue with basic logging\")\n",
    "    # Fallback to basic logging\n",
    "    try:\n",
    "        trainer.setup_logging()\n",
    "        print(\"âœ… Basic logging configured\")\n",
    "    except:\n",
    "        print(\"âš ï¸ Even basic logging had issues - continuing anyway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e02f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Arbor Dashboard (optional)\n",
    "print(\"\\nğŸŒ Starting Arbor Dashboard...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dashboard_port = tutorial_config['logging']['arbor_tracking']['dashboard_port']\n",
    "\n",
    "print(\"To monitor training in real-time:\")\n",
    "print(\"1. ğŸš€ Option A - Use the launcher:\")\n",
    "print(\"   python launch_training_dashboard.py\")\n",
    "print()\n",
    "print(\"2. ğŸ“Š Option B - Start dashboard manually:\")\n",
    "print(f\"   streamlit run arbor/tracking/dashboard.py --server.port={dashboard_port}\")\n",
    "print()\n",
    "print(\"3. ğŸŒ Then open your browser to:\")\n",
    "print(f\"   http://localhost:{dashboard_port}\")\n",
    "print()\n",
    "print(\"The dashboard will show:\")\n",
    "print(\"   â€¢ ğŸ“ˆ Live training metrics (loss, learning rate)\")\n",
    "print(\"   â€¢ ğŸ—ï¸ Model architecture with layer utilization\")  \n",
    "print(\"   â€¢ ğŸŒ± Growth tracking (parameters and layers)\")\n",
    "print(\"   â€¢ ğŸ”” Training alerts and notifications\")\n",
    "print(\"   â€¢ ğŸ“Š Performance analytics and system monitoring\")\n",
    "print()\n",
    "print(\"ğŸ’¡ Tip: Start the dashboard in another terminal before running training!\")\n",
    "print(\"    The dashboard will automatically pick up training data as it's generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e943737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer for the first dataset to inspect the setup\n",
    "if trainer.datasets:\n",
    "    dataset_name = list(trainer.datasets.keys())[0]\n",
    "    print(f\"ğŸ”§ Creating trainer for dataset: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        hf_trainer = trainer.create_trainer(dataset_name)\n",
    "        print(f\"âœ… HuggingFace trainer created\")\n",
    "        print(f\"   Training dataset: {len(hf_trainer.train_dataset):,} examples\")\n",
    "        if hf_trainer.eval_dataset:\n",
    "            print(f\"   Eval dataset: {len(hf_trainer.eval_dataset):,} examples\")\n",
    "        \n",
    "        # Show training arguments\n",
    "        args = hf_trainer.args\n",
    "        print(f\"\\nâš™ï¸  Training arguments:\")\n",
    "        print(f\"   Output dir: {args.output_dir}\")\n",
    "        print(f\"   Learning rate: {args.learning_rate}\")\n",
    "        print(f\"   Batch size: {args.per_device_train_batch_size}\")\n",
    "        print(f\"   Max steps: {args.max_steps}\")\n",
    "        print(f\"   Save steps: {args.save_steps}\")\n",
    "        print(f\"   Eval steps: {args.eval_steps}\")\n",
    "        print(f\"   FP16: {args.fp16}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Trainer creation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374699f",
   "metadata": {},
   "source": [
    "## Step 6: Test Adaptive Context System\n",
    "\n",
    "Before training, let's test the adaptive context system with different types of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the adaptive context system\n",
    "print(\"ğŸ§  Testing Adaptive Context System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different types of inputs\n",
    "test_inputs = {\n",
    "    \"simple_chat\": \"Hello! How are you today?\",\n",
    "    \n",
    "    \"code_task\": \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# This is a recursive implementation\n",
    "# Could be optimized with dynamic programming\n",
    "for i in range(10):\n",
    "    print(f\"fib({i}) = {fibonacci(i)}\")\n",
    "\"\"\",\n",
    "    \n",
    "    \"reasoning_task\": \"\"\"\n",
    "Let me think through this step by step. If we have a logical puzzle where:\n",
    "1. All cats are animals\n",
    "2. Some animals are pets  \n",
    "3. No pets are wild\n",
    "4. Some cats are wild\n",
    "\n",
    "We need to determine if there's a contradiction. Let me analyze each statement carefully\n",
    "and see if they can all be true simultaneously. This requires careful logical reasoning\n",
    "to avoid making invalid inferences.\n",
    "\"\"\",\n",
    "    \n",
    "    \"long_document\": \"\"\"\n",
    "This is a comprehensive research paper on machine learning that covers multiple aspects\n",
    "of the field. The introduction provides background on artificial intelligence and its\n",
    "historical development. The methodology section describes various approaches including\n",
    "supervised learning, unsupervised learning, and reinforcement learning paradigms.\n",
    "\"\"\" + \" The paper continues with detailed analysis.\" * 50  # Make it longer\n",
    "}\n",
    "\n",
    "# Test each input type\n",
    "for task_type, text in test_inputs.items():\n",
    "    print(f\"\\nğŸ” Testing: {task_type}\")\n",
    "    print(f\"Input length: {len(text)} characters\")\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = trainer.tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    token_count = input_ids.shape[1]\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    \n",
    "    # Get current context info\n",
    "    initial_context = trainer.model.get_context_info()['current_context_length']\n",
    "    \n",
    "    # Test the model (this should trigger adaptive context)\n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # This forward pass will trigger context adaptation\n",
    "            outputs = trainer.model(input_ids, return_dict=True)\n",
    "            \n",
    "            # Check if context adapted\n",
    "            final_context = trainer.model.get_context_info()['current_context_length']\n",
    "            \n",
    "            print(f\"Context: {initial_context:,} â†’ {final_context:,} tokens\")\n",
    "            if final_context != initial_context:\n",
    "                print(f\"âœ… Context adapted for {task_type}\")\n",
    "            else:\n",
    "                print(f\"â†’ Context unchanged for {task_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {task_type}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc04629",
   "metadata": {},
   "source": [
    "## Step 7: Run Training\n",
    "\n",
    "Now let's run the actual training! We'll train for a short period to demonstrate the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33735d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This will actually train the model!\n",
    "print(\"âš ï¸  TRAINING WARNING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"The next cell will run actual training.\")\n",
    "print(\"This may take several minutes and will:\")\n",
    "print(\"â€¢ Download datasets from HuggingFace\")\n",
    "print(\"â€¢ Train the model for 200 steps\")\n",
    "print(\"â€¢ Show parameter growth during training\")\n",
    "print(\"â€¢ Save model checkpoints\")\n",
    "print(\"\")\n",
    "print(\"Set RUN_TRAINING = True to proceed\")\n",
    "\n",
    "RUN_TRAINING = False  # Set to True to actually run training\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    print(\"ğŸš€ Starting training pipeline...\")\n",
    "else:\n",
    "    print(\"ğŸ›‘ Training skipped (set RUN_TRAINING = True to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ef955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training if enabled\n",
    "if RUN_TRAINING:\n",
    "    print(\"ğŸš€ Starting Arbor YAML Training Pipeline with Live Dashboard\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Start monitoring\n",
    "        if 'training_monitor' in locals():\n",
    "            training_monitor.start_monitoring()\n",
    "            print(\"ğŸ“Š Training monitor started - metrics will be saved for dashboard\")\n",
    "        \n",
    "        # This runs the complete training pipeline\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"\\nğŸ‰ Training completed successfully!\")\n",
    "        \n",
    "        # Show final model stats\n",
    "        final_params = trainer.model.param_count()\n",
    "        print(f\"ğŸ“Š Final model size: {final_params:,} parameters\")\n",
    "        \n",
    "        # Show training outputs\n",
    "        output_dir = Path(trainer.config.training_config['output_dir'])\n",
    "        if output_dir.exists():\n",
    "            saved_models = list(output_dir.glob(\"*/\"))\n",
    "            print(f\"ğŸ’¾ Saved {len(saved_models)} model checkpoints:\")\n",
    "            for model_dir in saved_models:\n",
    "                print(f\"   ğŸ“ {model_dir.name}\")\n",
    "        \n",
    "        # Stop monitoring and generate report\n",
    "        if 'training_monitor' in locals():\n",
    "            training_monitor.stop_monitoring()\n",
    "            report_file = training_monitor.export_training_report()\n",
    "            print(f\"ğŸ“„ Training report saved: {report_file}\")\n",
    "            \n",
    "            dashboard_port = tutorial_config['logging']['arbor_tracking']['dashboard_port']\n",
    "            print(f\"\\nğŸŒ View detailed metrics at: http://localhost:{dashboard_port}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Stop monitoring on error\n",
    "        if 'training_monitor' in locals():\n",
    "            training_monitor.stop_monitoring()\n",
    "        \n",
    "else:\n",
    "    # Simulate what training would show with Arbor tracking\n",
    "    print(\"ğŸ“‹ Training simulation (would show):\")\n",
    "    print(\"ğŸŒ± Initialized Arbor trainer with config: configs/tutorial_config.yaml\")\n",
    "    print(\"\udcca Arbor tracking system initialized:\")\n",
    "    print(\"   â€¢ Dashboard ready at http://localhost:8501\")\n",
    "    print(\"   â€¢ Metrics saved to: ./training_logs\")\n",
    "    print(\"   â€¢ Real-time monitoring enabled\")\n",
    "    print(\"   â€¢ Alert system active\")\n",
    "    print(\"\ud83dğŸ“¥ Downloading fresh Hermes-4-405B tokenizer...\")\n",
    "    print(\"âœ… Successfully loaded fresh Hermes-4-405B tokenizer\")\n",
    "    print(\"âœ… Created Arbor model: 347,394,048 parameters\")\n",
    "    print(\"ğŸ§  Adaptive context enabled:\")\n",
    "    print(\"   Range: 512 - 32,768\")\n",
    "    print(\"   Supported tasks: 4\")\n",
    "    print(\"ğŸŒ± Growth monitoring enabled:\")\n",
    "    print(\"   Factor: 1.5x\")\n",
    "    print(\"   Max steps: 4\")\n",
    "    print(\"ğŸ“š Loading datasets...\")\n",
    "    print(\"   âœ… tiny_stories: 1,000 examples\")\n",
    "    print(\"   âœ… code_samples: 500 examples\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ¯ Training on tiny_stories...\")\n",
    "    print(\"   ğŸ“Š Dashboard: Real-time loss curves, layer utilization heatmaps\")\n",
    "    print(\"   ğŸ“Š Parameters: 347,394,048 â†’ 347,894,048 (growth occurred)\")\n",
    "    print(\"   ğŸ”” Alert: Layer growth event detected\")\n",
    "    print(\"   âœ… tiny_stories complete!\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ¯ Training on code_samples...\")\n",
    "    print(\"   ğŸ“Š Dashboard: Architecture visualization updated\")\n",
    "    print(\"   ğŸ“Š Parameters: 347,894,048 â†’ 348,394,048 (growth occurred)\")\n",
    "    print(\"   ğŸ”” Alert: Performance threshold reached\")\n",
    "    print(\"   âœ… code_samples complete!\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ‰ Training pipeline complete!\")\n",
    "    print(\"ğŸ“Š Final dashboard shows comprehensive training analytics\")\n",
    "    print(\"ğŸ“„ Training report exported with growth timeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfa1c1",
   "metadata": {},
   "source": [
    "## Step 8: Test the Trained Model\n",
    "\n",
    "Let's test our model (or demonstrate what testing would look like) with different task types to see how the adaptive context system works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model generation with different tasks\n",
    "print(\"ğŸ§ª Testing Trained Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_prompts = {\n",
    "    \"story\": \"Once upon a time, in a magical forest\",\n",
    "    \"code\": \"# Python function to calculate factorial\\ndef factorial(n):\",\n",
    "    \"reasoning\": \"Let me solve this step by step. The problem is:\",\n",
    "    \"chat\": \"User: What's the weather like today?\\nAssistant:\"\n",
    "}\n",
    "\n",
    "for task_type, prompt in test_prompts.items():\n",
    "    print(f\"\\nğŸ¯ Testing {task_type} task:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = trainer.tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Show what would happen with adaptive context\n",
    "    print(f\"Input tokens: {input_ids.shape[1]}\")\n",
    "    \n",
    "    if RUN_TRAINING:\n",
    "        # Actually test the trained model\n",
    "        trainer.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Generate response\n",
    "                generated = trainer.model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=50,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                \n",
    "                # Decode response\n",
    "                response = trainer.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                print(f\"Generated: {response[len(prompt):]}\")\n",
    "                \n",
    "                # Show context info\n",
    "                context_info = trainer.model.get_context_info()\n",
    "                print(f\"Context used: {context_info['current_context_length']:,} tokens\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Generation failed: {e}\")\n",
    "    else:\n",
    "        # Simulate what would happen\n",
    "        simulated_contexts = {\"story\": 2048, \"code\": 4096, \"reasoning\": 8192, \"chat\": 1024}\n",
    "        print(f\"Would adapt context to: {simulated_contexts[task_type]:,} tokens\")\n",
    "        print(f\"Would generate appropriate {task_type} response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46680c",
   "metadata": {},
   "source": [
    "## Step 9: Configuration Tips and Best Practices\n",
    "\n",
    "Here are some tips for customizing your YAML training configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration tips and best practices\n",
    "print(\"ğŸ’¡ YAML Configuration Tips\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tips = {\n",
    "    \"Model Size\": {\n",
    "        \"Small (100M)\": \"hidden_size: 512, num_layers: 12\",\n",
    "        \"Medium (500M)\": \"hidden_size: 1024, num_layers: 24\", \n",
    "        \"Large (1B)\": \"hidden_size: 1536, num_layers: 32\"\n",
    "    },\n",
    "    \n",
    "    \"Context Lengths\": {\n",
    "        \"Short tasks\": \"max 4K tokens (chat, Q&A)\",\n",
    "        \"Medium tasks\": \"4K-16K tokens (code, creative)\",\n",
    "        \"Long tasks\": \"16K+ tokens (documents, reasoning)\"\n",
    "    },\n",
    "    \n",
    "    \"Growth Settings\": {\n",
    "        \"Conservative\": \"factor: 1.25, threshold: 0.95\",\n",
    "        \"Moderate\": \"factor: 1.5, threshold: 0.9\",\n",
    "        \"Aggressive\": \"factor: 2.0, threshold: 0.85\"\n",
    "    },\n",
    "    \n",
    "    \"Training Speed\": {\n",
    "        \"Fast prototyping\": \"small datasets, few steps\",\n",
    "        \"Full training\": \"complete datasets, many steps\", \n",
    "        \"Production\": \"multiple epochs, careful validation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, options in tips.items():\n",
    "    print(f\"\\nğŸ”§ {category}:\")\n",
    "    for option, description in options.items():\n",
    "        print(f\"   {option}: {description}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Common YAML patterns:\")\n",
    "print(\"\"\"\n",
    "# Full monitoring setup for research\n",
    "adaptive_context: \n",
    "  enabled: true\n",
    "growth:\n",
    "  enabled: true\n",
    "logging:\n",
    "  arbor_tracking:\n",
    "    enabled: true\n",
    "    save_dir: './training_logs'\n",
    "    dashboard_port: 8501\n",
    "    alerts:\n",
    "      enabled: true\n",
    "\n",
    "# Minimal setup for testing  \n",
    "adaptive_context:\n",
    "  enabled: false\n",
    "growth:\n",
    "  enabled: false\n",
    "logging:\n",
    "  arbor_tracking:\n",
    "    enabled: false\n",
    "  console:\n",
    "    enabled: true\n",
    "datasets:\n",
    "  - name: \"test\"\n",
    "    source: \"roneneldan/TinyStories\"\n",
    "    split: \"train[:100]\"\n",
    "\n",
    "# Production setup with full tracking\n",
    "logging:\n",
    "  arbor_tracking:\n",
    "    enabled: true\n",
    "    save_dir: './production_logs'\n",
    "    update_interval: 0.5\n",
    "    dashboard_port: 8501\n",
    "    alerts:\n",
    "      enabled: true\n",
    "      email_notifications: true\n",
    "      webhook_url: \"https://your-webhook-url.com\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97ae4e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned how to use the Arbor YAML training system with real-time monitoring. Here's what we covered:\n",
    "\n",
    "### âœ… **What You Learned:**\n",
    "\n",
    "1. **ğŸ“‹ YAML Configuration** - How to create and customize training configs\n",
    "2. **ğŸ§  Adaptive Context** - Task-aware context window adaptation  \n",
    "3. **ğŸŒ± Dynamic Growth** - Parameter expansion during training\n",
    "4. **ğŸš€ Easy Training** - One-command training with `python train.py config.yaml`\n",
    "5. **ğŸ“Š Live Monitoring** - Real-time dashboard with training visualization\n",
    "6. **ğŸ§ª Testing & Validation** - How to test trained models\n",
    "\n",
    "### ğŸ¯ **Key Benefits:**\n",
    "\n",
    "- **Simple**: Just edit YAML, no complex code\n",
    "- **Powerful**: Full control over model architecture and training\n",
    "- **Smart**: Automatic context adaptation and parameter growth\n",
    "- **Visual**: Real-time dashboard with live metrics and architecture visualization\n",
    "- **Production Ready**: HuggingFace integration and comprehensive monitoring\n",
    "\n",
    "### ğŸš€ **Next Steps:**\n",
    "\n",
    "1. **Customize** your own YAML config for your use case\n",
    "2. **Train** with real datasets for your domain\n",
    "3. **Monitor** training with the Arbor dashboard at http://localhost:8501\n",
    "4. **Deploy** trained models to HuggingFace Hub\n",
    "\n",
    "### ğŸ“Š **Dashboard Features:**\n",
    "\n",
    "- **Live Metrics**: Training loss, learning rate, gradient norms in real-time\n",
    "- **Architecture View**: Interactive model visualization with layer utilization\n",
    "- **Growth Tracking**: Parameter and layer growth timeline\n",
    "- **Alert System**: Automatic notifications for training events\n",
    "- **Analytics**: Performance statistics and comprehensive reports\n",
    "\n",
    "The YAML trainer with Arbor dashboard makes it incredibly easy to experiment with cutting-edge transformer architectures while monitoring everything in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c65eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and final info\n",
    "print(\"ğŸ§¹ Cleanup and Final Info\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show created files\n",
    "created_files = [\n",
    "    \"configs/tutorial_config.yaml\",\n",
    "    \"training_logs/\" if 'training_monitor' in locals() else \"training_logs/ (would be created)\",\n",
    "    \"tutorial_output/\" if RUN_TRAINING else \"tutorial_output/ (would be created)\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Files created during this tutorial:\")\n",
    "for file in created_files:\n",
    "    if Path(file).exists() or \"would be\" in file:\n",
    "        print(f\"   âœ… {file}\")\n",
    "\n",
    "dashboard_port = tutorial_config['logging']['arbor_tracking']['dashboard_port']\n",
    "\n",
    "print(f\"\\nğŸ¯ To run training yourself:\")\n",
    "print(f\"   1. Set RUN_TRAINING = True in cell 16\")\n",
    "print(f\"   2. Or run: python train.py configs/tutorial_config.yaml\")\n",
    "\n",
    "print(f\"\\nğŸŒ To access the Arbor Dashboard:\")\n",
    "print(f\"   1. Start dashboard: streamlit run arbor/tracking/dashboard.py --server.port={dashboard_port}\")\n",
    "print(f\"   2. Open browser: http://localhost:{dashboard_port}\")\n",
    "print(f\"   3. Or use launcher: python launch_training_dashboard.py\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dashboard Features:\")\n",
    "print(f\"   â€¢ Live training metrics with real-time updates\")\n",
    "print(f\"   â€¢ Interactive model architecture visualization\")\n",
    "print(f\"   â€¢ Parameter and layer growth tracking\")\n",
    "print(f\"   â€¢ Alert system with configurable notifications\")\n",
    "print(f\"   â€¢ Performance analytics and export capabilities\")\n",
    "\n",
    "print(f\"\\nğŸ”§ To customize:\")\n",
    "print(f\"   1. Edit configs/tutorial_config.yaml\")\n",
    "print(f\"   2. Adjust model size, datasets, training steps\")\n",
    "print(f\"   3. Enable/disable adaptive context and growth\")\n",
    "print(f\"   4. Configure dashboard alerts and monitoring\")\n",
    "\n",
    "print(f\"\\nğŸ“š For more examples:\")\n",
    "print(f\"   â€¢ Check configs/example_config.yaml\")\n",
    "print(f\"   â€¢ Run python examples/training_with_dashboard.py\")\n",
    "print(f\"   â€¢ See COMPLETE_USAGE_GUIDE.md for full documentation\")\n",
    "\n",
    "print(f\"\\nğŸŒ± Happy training with Arbor and live monitoring!\")\n",
    "print(f\"ğŸš€ The future of adaptive AI training is here!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
