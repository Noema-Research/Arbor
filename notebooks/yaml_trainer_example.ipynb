{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09ada8f",
   "metadata": {},
   "source": [
    "# Arbor YAML Trainer Tutorial with Live Dashboard\n",
    "\n",
    "This notebook demonstrates how to use the Arbor YAML training system with adaptive context windows and **real-time monitoring dashboard**. Works perfectly in both **local environments** and **Google Colab** with public URL access!\n",
    "\n",
    "## üéØ **What You'll Learn:**\n",
    "\n",
    "1. **Create and customize training configurations**\n",
    "2. **Train models with dynamic growth and adaptive context**\n",
    "3. **Monitor training progress with live dashboard visualization**\n",
    "4. **Track model architecture changes in real-time**\n",
    "5. **Set up alerts and performance monitoring**\n",
    "6. **Test the trained model with different task types**\n",
    "\n",
    "## üÜï **Dashboard Features:**\n",
    "\n",
    "- **üìä Live Metrics**: Real-time training loss, learning rate, and gradient monitoring\n",
    "- **üèóÔ∏è Architecture Visualization**: Interactive model structure with layer utilization heatmaps\n",
    "- **üå± Growth Tracking**: Timeline of parameter and layer growth events\n",
    "- **üîî Alert System**: Automatic notifications for training anomalies and milestones\n",
    "- **üìà Analytics**: Performance statistics, trends, and exportable reports\n",
    "\n",
    "## üåê **Environment Support:**\n",
    "\n",
    "### üíª **Local Development:**\n",
    "- Dashboard at `http://localhost:8501`\n",
    "- Full feature access\n",
    "- Private and secure\n",
    "\n",
    "### ‚òÅÔ∏è **Google Colab:**\n",
    "- **Public URL** via ngrok (e.g., `https://abc123.ngrok.io`)\n",
    "- **Mobile-responsive** dashboard\n",
    "- **Shareable** with team members\n",
    "- **Identical features** to local setup\n",
    "\n",
    "## üöÄ **Quick Start:**\n",
    "\n",
    "### Local:\n",
    "1. Run this notebook to configure training\n",
    "2. Start dashboard: `streamlit run arbor/tracking/dashboard.py`\n",
    "3. Monitor at http://localhost:8501\n",
    "\n",
    "### Google Colab:\n",
    "1. Run environment setup cells (auto-detects Colab)\n",
    "2. Get free ngrok token from [ngrok.com](https://ngrok.com)\n",
    "3. Run dashboard startup cell with your token\n",
    "4. Monitor at your unique public URL!\n",
    "\n",
    "The YAML trainer with Arbor dashboard makes adaptive transformer training accessible everywhere! üåç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add arbor to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import Arbor tracking system\n",
    "try:\n",
    "    from arbor.tracking import TrainingMonitor\n",
    "    print(\"‚úÖ Arbor tracking system imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Arbor tracking not available: {e}\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if not Path(\"arbor\").exists():\n",
    "    print(\"‚ùå Please run this notebook from the arbor-o1-living-ai root directory\")\n",
    "    print(\"Current directory:\", Path.cwd())\n",
    "else:\n",
    "    print(\"‚úÖ Found arbor directory\")\n",
    "    print(\"üìç Working directory:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "print(\"üì¶ Installing Required Dependencies...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Core dependencies for Arbor training\n",
    "dependencies = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"transformers>=4.30.0\", \n",
    "    \"datasets>=2.14.0\",\n",
    "    \"accelerate>=0.20.0\",\n",
    "    \"peft>=0.4.0\",\n",
    "    \"bitsandbytes>=0.41.0\",\n",
    "    \"streamlit>=1.25.0\",\n",
    "    \"plotly>=5.15.0\",\n",
    "    \"pyngrok>=6.0.0\",  # For Google Colab public URLs\n",
    "    \"tensorboard>=2.13.0\",\n",
    "    \"wandb>=0.15.0\",  # Optional but useful\n",
    "    \"scipy>=1.10.0\",\n",
    "    \"scikit-learn>=1.3.0\"\n",
    "]\n",
    "\n",
    "# Install other dependencies first\n",
    "print(\"\\nüì¶ Installing core dependencies...\")\n",
    "for dep in dependencies:\n",
    "    try:\n",
    "        print(f\"   Installing {dep}...\")\n",
    "        !pip install -q {dep}\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Warning: Could not install {dep}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Core dependencies installed!\")\n",
    "\n",
    "# Now handle Arbor installation with improved logic\n",
    "print(\"\\nüå± Installing Arbor package...\")\n",
    "print(\"   Checking installation options...\")\n",
    "\n",
    "arbor_installed = False\n",
    "\n",
    "# Option 1: Try GitHub installation (most common failure point)\n",
    "print(\"\\nüîó Attempting GitHub installation...\")\n",
    "try:\n",
    "    !pip install -q git+https://github.com/Noema-Research/Arbor.git\n",
    "    print(\"‚úÖ Arbor installed successfully from GitHub!\")\n",
    "    arbor_installed = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GitHub installation failed: {e}\")\n",
    "    print(\"   This is common in cloud environments due to access restrictions\")\n",
    "\n",
    "# Option 2: Clone repo and install locally (for Colab)\n",
    "if not arbor_installed:\n",
    "    print(\"\\nüì• Trying to clone repository for local installation...\")\n",
    "    try:\n",
    "        # Check if we're in Colab and need to clone\n",
    "        import os\n",
    "        if not os.path.exists(\"arbor-o1-living-ai\"):\n",
    "            print(\"   Cloning Arbor repository...\")\n",
    "            !git clone https://github.com/Noema-Research/Arbor.git arbor-o1-living-ai\n",
    "        \n",
    "        # Change to the repo directory and install\n",
    "        if os.path.exists(\"arbor-o1-living-ai\"):\n",
    "            print(\"   Installing from cloned repository...\")\n",
    "            !cd arbor-o1-living-ai && pip install -q -e .\n",
    "            print(\"‚úÖ Arbor installed from cloned repository!\")\n",
    "            arbor_installed = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Repository clone/install failed: {e}\")\n",
    "\n",
    "# Option 3: Check if we're already in the Arbor directory\n",
    "if not arbor_installed:\n",
    "    print(\"\\nüìÅ Checking for local Arbor directory...\")\n",
    "    if Path(\"arbor\").exists() and Path(\"pyproject.toml\").exists():\n",
    "        print(\"   Found Arbor source - installing in development mode...\")\n",
    "        try:\n",
    "            !pip install -q -e .\n",
    "            print(\"‚úÖ Arbor installed in development mode!\")\n",
    "            arbor_installed = True\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Development installation failed: {e}\")\n",
    "\n",
    "# Option 4: Fallback - manual path setup\n",
    "if not arbor_installed:\n",
    "    print(\"\\nüîß Setting up manual path-based import...\")\n",
    "    import sys\n",
    "    \n",
    "    # Try multiple possible paths\n",
    "    possible_paths = [\n",
    "        \".\",\n",
    "        \"arbor-o1-living-ai\", \n",
    "        \"/content/arbor-o1-living-ai\",\n",
    "        str(Path.cwd())\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if Path(path, \"arbor\").exists():\n",
    "            sys.path.insert(0, str(Path(path).absolute()))\n",
    "            print(f\"   Added {path} to Python path\")\n",
    "            break\n",
    "    \n",
    "    print(\"   üìù Note: Using path-based import - some features may be limited\")\n",
    "\n",
    "# Verify installations\n",
    "print(\"\\nüîç Verifying installations...\")\n",
    "\n",
    "# Check key packages\n",
    "key_packages = {\n",
    "    'torch': 'PyTorch',\n",
    "    'transformers': 'HuggingFace Transformers', \n",
    "    'datasets': 'HuggingFace Datasets',\n",
    "    'streamlit': 'Streamlit Dashboard',\n",
    "    'plotly': 'Plotly Visualization'\n",
    "}\n",
    "\n",
    "for package, name in key_packages.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"   ‚úÖ {name}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå {name} - installation may have failed\")\n",
    "\n",
    "# Special comprehensive check for Arbor\n",
    "print(f\"\\nüå± Checking Arbor installation...\")\n",
    "arbor_available = False\n",
    "\n",
    "try:\n",
    "    # First try direct import\n",
    "    import arbor\n",
    "    print(f\"   ‚úÖ Arbor imported successfully (version: {getattr(arbor, '__version__', 'development')})\")\n",
    "    arbor_available = True\n",
    "except ImportError as e1:\n",
    "    print(f\"   ‚ö†Ô∏è Direct import failed: {e1}\")\n",
    "    \n",
    "    # Try with path manipulation\n",
    "    import sys\n",
    "    paths_to_try = [\".\", \"arbor-o1-living-ai\", \"/content/arbor-o1-living-ai\"]\n",
    "    \n",
    "    for path in paths_to_try:\n",
    "        try:\n",
    "            if Path(path, \"arbor\").exists():\n",
    "                if str(Path(path).absolute()) not in sys.path:\n",
    "                    sys.path.insert(0, str(Path(path).absolute()))\n",
    "                import arbor\n",
    "                print(f\"   ‚úÖ Arbor imported with path: {path}\")\n",
    "                arbor_available = True\n",
    "                break\n",
    "        except ImportError:\n",
    "            continue\n",
    "    \n",
    "    if not arbor_available:\n",
    "        print(f\"   ‚ùå Arbor not available - notebook will work with limited functionality\")\n",
    "\n",
    "print(\"\\nüéØ Installation Summary:\")\n",
    "if arbor_available:\n",
    "    print(\"   ‚Ä¢ ‚úÖ All systems ready - full functionality available!\")\n",
    "    print(\"   ‚Ä¢ ‚úÖ Arbor package properly installed and importable\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ ‚ö†Ô∏è Arbor not fully installed - some features may be limited\")\n",
    "    print(\"   ‚Ä¢ üìù Tutorial will continue with simulations where needed\")\n",
    "\n",
    "print(\"   ‚Ä¢ If you see any ‚ùå errors, try restarting runtime and re-running\")\n",
    "print(\"   ‚Ä¢ For Google Colab: Some packages may require runtime restart\")\n",
    "print(\"   ‚Ä¢ Cloud environments sometimes have installation restrictions\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to proceed with YAML training!\")\n",
    "\n",
    "# Environment-specific tips\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"\\nüí° Google Colab Tips:\")\n",
    "    print(\"   ‚Ä¢ Repository cloned to: /content/arbor-o1-living-ai\")\n",
    "    print(\"   ‚Ä¢ Use 'Runtime ‚Üí Restart runtime' if you encounter import issues\")\n",
    "    print(\"   ‚Ä¢ Some packages may require runtime restart to work properly\")\n",
    "    print(\"   ‚Ä¢ The notebook is designed to work even with limited Arbor functionality\")\n",
    "    \n",
    "    # Show current working directory\n",
    "    import os\n",
    "    print(f\"   ‚Ä¢ Current directory: {os.getcwd()}\")\n",
    "    if os.path.exists(\"arbor-o1-living-ai\"):\n",
    "        print(\"   ‚Ä¢ ‚úÖ Arbor repository is available\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ ‚ö†Ô∏è Arbor repository not found - some features may be limited\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"\\nüíª Local Environment:\")\n",
    "    print(\"   ‚Ä¢ Make sure you're running from the arbor-o1-living-ai directory\")\n",
    "    print(\"   ‚Ä¢ Or the notebook will clone the repository automatically\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Colab Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running locally\")\n",
    "\n",
    "# Setup for Google Colab\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Setting up Google Colab environment...\")\n",
    "    \n",
    "    # Install additional packages needed for Colab\n",
    "    print(\"üì¶ Installing required packages...\")\n",
    "    !pip install -q streamlit pyngrok plotly\n",
    "    \n",
    "    # Setup ngrok for public URL access\n",
    "    print(\"üåê Setting up public URL access with ngrok...\")\n",
    "    \n",
    "    # Import ngrok after installation\n",
    "    from pyngrok import ngrok\n",
    "    import getpass\n",
    "    \n",
    "    # Get ngrok auth token (user needs to provide this)\n",
    "    print(\"üîë To access the dashboard publicly, you need an ngrok auth token:\")\n",
    "    print(\"   1. Go to https://ngrok.com/ and sign up (free)\")\n",
    "    print(\"   2. Get your auth token from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "    print(\"   3. Enter it below (or set NGROK_AUTH_TOKEN environment variable)\")\n",
    "    \n",
    "    ngrok_token = os.getenv('NGROK_AUTH_TOKEN')\n",
    "    if not ngrok_token:\n",
    "        print(\"üí° Tip: Set NGROK_AUTH_TOKEN environment variable to skip this step\")\n",
    "        # For now, we'll handle this when starting the dashboard\n",
    "        pass\n",
    "    else:\n",
    "        ngrok.set_auth_token(ngrok_token)\n",
    "        print(\"‚úÖ ngrok authentication configured\")\n",
    "    \n",
    "    # Set Colab-specific configurations\n",
    "    DASHBOARD_PORT = 8501\n",
    "    PUBLIC_URL = None  # Will be set when dashboard starts\n",
    "    \n",
    "else:\n",
    "    # Local development setup\n",
    "    DASHBOARD_PORT = 8501\n",
    "    LOCAL_URL = f\"http://localhost:{DASHBOARD_PORT}\"\n",
    "    print(f\"üè† Local dashboard will be available at: {LOCAL_URL}\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40776a90",
   "metadata": {},
   "source": [
    "## Step 1: Create Your Training Configuration\n",
    "\n",
    "The YAML trainer uses configuration files to specify everything about your training run. Let's start by examining and customizing a training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a72895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the example configuration first\n",
    "config_path = Path(\"configs/example_config.yaml\")\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_content = f.read()\n",
    "    \n",
    "    print(\"üìã Example Configuration Structure:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show first 30 lines to get an overview\n",
    "    lines = config_content.split('\\n')\n",
    "    for i, line in enumerate(lines[:30]):\n",
    "        print(f\"{i+1:2d}: {line}\")\n",
    "    \n",
    "    if len(lines) > 30:\n",
    "        print(f\"... ({len(lines) - 30} more lines)\")\n",
    "else:\n",
    "    print(\"‚ùå Example config not found. Let's create one!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized T4 GPU configuration for 1B model with all features\n",
    "print(\"üöÄ Creating T4-Optimized 1B Model Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "t4_optimized_config = {\n",
    "    'model': {\n",
    "        'name': 'arbor-1b-t4-demo',\n",
    "        'vocab_size': 128256,\n",
    "        \n",
    "        # 1B parameter architecture optimized for T4\n",
    "        'hidden_size': 2048,      # Good balance for T4 memory\n",
    "        'num_layers': 24,         # Starting layers (will grow to 32)\n",
    "        'num_heads': 16,          # 2048 / 16 = 128 head dim\n",
    "        'ffn_dim': 5504,          # ~2.7x hidden_size for efficiency\n",
    "        'max_seq_length': 4096,   # Good for most tasks\n",
    "        \n",
    "        # Layer Growth Configuration\n",
    "        'layer_growth': {\n",
    "            'enabled': True,\n",
    "            'min_layers': 24,\n",
    "            'max_layers': 32,           # Conservative for T4 memory\n",
    "            'growth_factor': 2,         # Add 2 layers at a time\n",
    "            'growth_threshold': 0.88,   # Trigger when 88% utilized\n",
    "            'growth_cooldown': 100,     # Wait 100 steps between growth\n",
    "            'utilization_window': 10    # Average over 10 steps\n",
    "        },\n",
    "        \n",
    "        # Parameter Growth Configuration  \n",
    "        'growth': {\n",
    "            'enabled': True,\n",
    "            'factor': 1.5,              # Moderate growth for stability\n",
    "            'threshold': 0.85,          # Conservative threshold\n",
    "            'max_steps': 3,             # Max 3 growth events\n",
    "            'cooldown': 75              # Wait between parameter growth\n",
    "        },\n",
    "        \n",
    "        # Adaptive Context (great for demos)\n",
    "        'adaptive_context': {\n",
    "            'enabled': True,\n",
    "            'min_context_length': 512,\n",
    "            'max_context_length': 4096,\n",
    "            'context_lengths': [512, 1024, 2048, 4096],\n",
    "            'task_types': ['chat', 'code', 'reasoning', 'document'],\n",
    "            'adaptation_threshold': 0.7\n",
    "        },\n",
    "        \n",
    "        # T4 Memory Optimizations\n",
    "        'use_cache': False,           # Save memory during training\n",
    "        'gradient_checkpointing': True,\n",
    "        'attention_dropout': 0.1,\n",
    "        'hidden_dropout': 0.1\n",
    "    },\n",
    "    \n",
    "    # Required tokenizer section\n",
    "    'tokenizer': {\n",
    "        'name': 'NousResearch/Hermes-3-Llama-3.1-8B',\n",
    "        'model_max_length': 4096,\n",
    "        'padding_side': 'left',\n",
    "        'truncation_side': 'right',\n",
    "        'add_eos_token': True,\n",
    "        'add_bos_token': True,\n",
    "        'use_fast': True,\n",
    "        'trust_remote_code': False\n",
    "    },\n",
    "    \n",
    "    'datasets': [\n",
    "        {\n",
    "            'name': 'tinystories_demo',\n",
    "            'source': 'roneneldan/TinyStories',\n",
    "            'split': 'train[:5000]',    # Small for demo\n",
    "            'text_column': 'text',\n",
    "            'preprocessing': {\n",
    "                'prefix': 'üìö Story: ',\n",
    "                'suffix': ' [END]',\n",
    "                'max_length': 1024,     # Shorter for variety\n",
    "                'min_length': 100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'code_demo',\n",
    "            'source': 'codeparrot/github-code-clean',\n",
    "            'split': 'train[:2000]',    # Small code samples\n",
    "            'text_column': 'code',\n",
    "            'preprocessing': {\n",
    "                'prefix': 'üíª Code:\\n',\n",
    "                'suffix': '\\n# End of code',\n",
    "                'max_length': 2048,     # Longer for code\n",
    "                'filter_languages': ['python', 'javascript']\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'reasoning_demo',\n",
    "            'source': 'microsoft/orca-math-word-problems-200k',\n",
    "            'split': 'train[:1000]',    # Math reasoning\n",
    "            'text_column': 'question',\n",
    "            'preprocessing': {\n",
    "                'prefix': 'üß† Problem: ',\n",
    "                'suffix': ' [SOLVE]',\n",
    "                'max_length': 1536\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    'training': {\n",
    "        'output_dir': './arbor-1b-t4-demo',\n",
    "        \n",
    "        # T4-Optimized Training Settings\n",
    "        'learning_rate': 2e-4,            # Good starting LR for 1B\n",
    "        'warmup_steps': 200,              # Gradual warmup\n",
    "        'steps_per_dataset': 500,         # 1500 total steps across 3 datasets\n",
    "        'max_train_steps': 1500,\n",
    "        \n",
    "        # T4 Memory Management\n",
    "        'per_device_train_batch_size': 1,  # Small batch for T4\n",
    "        'gradient_accumulation_steps': 16, # Effective batch size = 16\n",
    "        'dataloader_num_workers': 2,       # Don't overwhelm T4\n",
    "        \n",
    "        # Mixed Precision & Optimization\n",
    "        'fp16': True,                      # Essential for T4\n",
    "        'bf16': False,                     # T4 doesn't support bf16\n",
    "        'gradient_checkpointing': True,    # Save memory\n",
    "        'optim': 'adamw_torch',\n",
    "        'weight_decay': 0.01,\n",
    "        'adam_beta1': 0.9,\n",
    "        'adam_beta2': 0.999,\n",
    "        'adam_epsilon': 1e-8,\n",
    "        'max_grad_norm': 1.0,\n",
    "        \n",
    "        # Evaluation & Saving\n",
    "        'eval_steps': 100,                 # Frequent evaluation\n",
    "        'save_steps': 250,                 # Save checkpoints\n",
    "        'save_total_limit': 3,\n",
    "        'logging_steps': 25,               # Detailed logging\n",
    "        \n",
    "        # Efficiency Settings\n",
    "        'remove_unused_columns': True,\n",
    "        'prediction_loss_only': True,\n",
    "        'use_mps_device': False           # T4 is CUDA\n",
    "    },\n",
    "    \n",
    "    'logging': {\n",
    "        'arbor_tracking': {\n",
    "            'enabled': True,\n",
    "            'save_dir': './t4_training_logs',\n",
    "            'update_interval': 0.5,        # Frequent updates for demo\n",
    "            'dashboard_port': 8502,        # Different port for demo\n",
    "            'alerts': {\n",
    "                'enabled': True,\n",
    "                'email_notifications': False,\n",
    "                'webhook_url': None,\n",
    "                'alert_thresholds': {\n",
    "                    'loss_spike': 2.0,     # Alert if loss > 2x avg\n",
    "                    'memory_usage': 0.9,   # Alert at 90% memory\n",
    "                    'gradient_norm': 10.0, # Alert for gradient explosion\n",
    "                    'learning_rate_drop': 0.5  # Alert for LR scheduler issues\n",
    "                }\n",
    "            },\n",
    "            'metrics': {\n",
    "                'track_gpu_memory': True,\n",
    "                'track_layer_utilization': True,\n",
    "                'track_gradient_norms': True,\n",
    "                'track_learning_rates': True,\n",
    "                'export_frequency': 'every_100_steps'\n",
    "            }\n",
    "        },\n",
    "        'console': {\n",
    "            'enabled': True,\n",
    "            'level': 'INFO',\n",
    "            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        },\n",
    "        'tensorboard': {\n",
    "            'enabled': True,\n",
    "            'log_dir': './t4_tensorboard_logs'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'huggingface': {\n",
    "        'upload': {\n",
    "            'enabled': False,              # Can enable for demo\n",
    "            'repository': 'your-username/arbor-1b-t4-demo',\n",
    "            'private': False,\n",
    "            'push_to_hub_frequency': 500\n",
    "        },\n",
    "        'token': None  # Set your HF token if uploading\n",
    "    },\n",
    "    \n",
    "    # Demo-specific features\n",
    "    'demo_features': {\n",
    "        'enable_growth_visualization': True,\n",
    "        'save_architecture_snapshots': True,\n",
    "        'generate_sample_outputs': True,\n",
    "        'create_training_video': False,    # Set True for video generation\n",
    "        'benchmark_performance': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the T4-optimized configuration\n",
    "t4_config_path = Path(\"configs/t4_1b_demo_config.yaml\")\n",
    "t4_config_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(t4_config_path, 'w') as f:\n",
    "    yaml.dump(t4_optimized_config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Created T4-optimized 1B model configuration!\")\n",
    "print(f\"üìÅ Saved to: {t4_config_path}\")\n",
    "print(\"\\nüéØ Configuration Highlights:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Model: ~1B parameters (2048 hidden, 24‚Üí32 layers)\")\n",
    "print(f\"üß† Memory: Optimized for T4 GPU (16GB)\")\n",
    "print(f\"üå± Growth: Layer growth (24‚Üí32) + parameter growth\")\n",
    "print(f\"üìà Dashboard: Enhanced tracking at http://localhost:8502\")\n",
    "print(f\"‚ö° Training: 1500 steps across 3 diverse datasets\")\n",
    "print(f\"üîÑ Context: Adaptive 512‚Üí4096 tokens\")\n",
    "print(f\"üíæ Batch: Effective size 16 (1√ó16 accumulation)\")\n",
    "print(f\"üéØ Features: All demo features enabled\")\n",
    "\n",
    "print(\"\\nüí° T4 Optimizations:\")\n",
    "print(\"  ‚Ä¢ FP16 precision for memory efficiency\")\n",
    "print(\"  ‚Ä¢ Gradient checkpointing enabled\")\n",
    "print(\"  ‚Ä¢ Small batch size with accumulation\")\n",
    "print(\"  ‚Ä¢ Conservative growth thresholds\")\n",
    "print(\"  ‚Ä¢ Frequent logging for monitoring\")\n",
    "\n",
    "print(\"\\nüöÄ Perfect for:\")\n",
    "print(\"  ‚Ä¢ Live training demonstrations\")\n",
    "print(\"  ‚Ä¢ Growth visualization\")\n",
    "print(\"  ‚Ä¢ Real-time dashboard monitoring\")\n",
    "print(\"  ‚Ä¢ T4/Colab compatibility\")\n",
    "print(\"  ‚Ä¢ Educational showcases\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration Structure:\")\n",
    "print(\"  ‚Ä¢ ‚úÖ model section - architecture and growth settings\")\n",
    "print(\"  ‚Ä¢ ‚úÖ tokenizer section - Hermes-3-Llama-3.1-8B configuration\")\n",
    "print(\"  ‚Ä¢ ‚úÖ datasets section - 3 diverse training datasets\")\n",
    "print(\"  ‚Ä¢ ‚úÖ training section - T4-optimized training parameters\")\n",
    "print(\"  ‚Ä¢ ‚úÖ logging section - comprehensive monitoring setup\")\n",
    "print(\"  ‚Ä¢ ‚úÖ All required sections for YAML trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beccf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use the T4-optimized configuration\n",
    "print(\"üéÆ How to Use the T4 Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"1. üöÄ Quick Start (Local with T4):\")\n",
    "print(\"   python train.py configs/t4_1b_demo_config.yaml\")\n",
    "print()\n",
    "\n",
    "print(\"2. üìä With Dashboard (Recommended):\")\n",
    "print(\"   # Terminal 1: Start dashboard\")\n",
    "print(\"   streamlit run arbor/tracking/dashboard.py --server.port=8502\")\n",
    "print(\"   # Terminal 2: Start training\")  \n",
    "print(\"   python train.py configs/t4_1b_demo_config.yaml\")\n",
    "print(\"   # View at: http://localhost:8502\")\n",
    "print()\n",
    "\n",
    "print(\"3. üåê Google Colab with T4:\")\n",
    "print(\"   # Enable T4 GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "print(\"   # Run the Colab setup cells above\")\n",
    "print(\"   # Use this config with the dashboard!\")\n",
    "print()\n",
    "\n",
    "print(\"4. üìà What You'll See:\")\n",
    "print(\"   ‚Ä¢ Real-time loss curves across 3 different datasets\")\n",
    "print(\"   ‚Ä¢ Layer growth from 24 ‚Üí 32 layers during training\")\n",
    "print(\"   ‚Ä¢ Parameter growth events (FFN expansion)\")\n",
    "print(\"   ‚Ä¢ Adaptive context switching between task types\")\n",
    "print(\"   ‚Ä¢ GPU memory utilization tracking\")\n",
    "print(\"   ‚Ä¢ Growth event alerts and notifications\")\n",
    "print()\n",
    "\n",
    "print(\"5. üíæ Expected Training Time:\")\n",
    "print(\"   ‚Ä¢ T4 GPU: ~45-60 minutes for full 1500 steps\")\n",
    "print(\"   ‚Ä¢ Checkpoints saved every 250 steps\")\n",
    "print(\"   ‚Ä¢ Dashboard updates every 0.5 seconds\")\n",
    "print(\"   ‚Ä¢ Growth events typically occur around steps 400, 800, 1200\")\n",
    "print()\n",
    "\n",
    "print(\"6. üéØ Demo Scenarios:\")\n",
    "print(\"   ‚Ä¢ Show adaptive context: Watch context length change by dataset\")\n",
    "print(\"   ‚Ä¢ Demonstrate growth: Model architecture evolves during training\")\n",
    "print(\"   ‚Ä¢ Monitor performance: Real-time GPU utilization and efficiency\")\n",
    "print(\"   ‚Ä¢ Alert system: Get notified of training events\")\n",
    "print()\n",
    "\n",
    "# Memory estimation\n",
    "print(\"üíæ Memory Requirements:\")\n",
    "print(\"=\" * 25)\n",
    "model_params = 1.0  # 1B parameters\n",
    "bytes_per_param = 4  # FP32\n",
    "fp16_savings = 0.5   # FP16 uses half memory\n",
    "gradient_multiplier = 2  # Gradients + optimizer states\n",
    "activation_memory = 2    # Activation memory (GB)\n",
    "\n",
    "base_memory = model_params * bytes_per_param * fp16_savings * gradient_multiplier\n",
    "total_memory = base_memory + activation_memory\n",
    "\n",
    "print(f\"Model parameters: {model_params:.1f}B\")\n",
    "print(f\"Base memory (FP16): {base_memory:.1f}GB\")\n",
    "print(f\"Activation memory: {activation_memory:.1f}GB\")\n",
    "print(f\"Total estimated: {total_memory:.1f}GB\")\n",
    "print(f\"T4 GPU memory: 16GB\")\n",
    "print(f\"Memory utilization: {(total_memory/16)*100:.1f}%\")\n",
    "print(\"‚úÖ Fits comfortably on T4!\")\n",
    "\n",
    "print(f\"\\nüîß To customize this config:\")\n",
    "print(f\"   ‚Ä¢ Edit configs/t4_1b_demo_config.yaml\")\n",
    "print(f\"   ‚Ä¢ Adjust batch_size/accumulation for your GPU\")\n",
    "print(f\"   ‚Ä¢ Change growth thresholds for more/less growth\")\n",
    "print(f\"   ‚Ä¢ Add your own datasets\")\n",
    "print(f\"   ‚Ä¢ Configure alerts and notifications\")\n",
    "\n",
    "# Set this as the active tutorial config\n",
    "tutorial_config = t4_optimized_config\n",
    "tutorial_config_path = t4_config_path\n",
    "\n",
    "print(f\"\\n‚úÖ T4 config is now active for the rest of this tutorial!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177fc01",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the YAML Configuration\n",
    "\n",
    "Let's examine the key sections of our configuration and what they control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore each section of our configuration\n",
    "print(\"üîç Configuration Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Model configuration\n",
    "model_config = tutorial_config['model']\n",
    "print(\"\\nü§ñ MODEL CONFIGURATION:\")\n",
    "print(f\"   Vocabulary: {model_config['vocab_size']:,} tokens (Hermes-4-405B)\")\n",
    "print(f\"   Architecture: {model_config['num_layers']} layers √ó {model_config['hidden_size']} dim\")\n",
    "print(f\"   Parameters: ~{(model_config['hidden_size'] * model_config['num_layers'] * 4) / 1e6:.0f}M\")\n",
    "\n",
    "# Growth settings\n",
    "growth = model_config['growth']\n",
    "print(f\"\\nüå± GROWTH SETTINGS:\")\n",
    "print(f\"   Enabled: {growth['enabled']}\")\n",
    "print(f\"   Growth factor: {growth['factor']}x\")\n",
    "print(f\"   Max growth steps: {growth['max_steps']}\")\n",
    "print(f\"   Trigger threshold: {growth['threshold']}\")\n",
    "\n",
    "# Adaptive context\n",
    "adaptive = model_config['adaptive_context']\n",
    "print(f\"\\nüß† ADAPTIVE CONTEXT:\")\n",
    "print(f\"   Enabled: {adaptive['enabled']}\")\n",
    "print(f\"   Context range: {adaptive['min_context_length']:,} - {adaptive['max_context_length']:,}\")\n",
    "print(f\"   Task types: {len(adaptive['task_types'])} ({', '.join(adaptive['task_types'])})\")\n",
    "print(f\"   Context options: {len(adaptive['context_lengths'])} levels\")\n",
    "\n",
    "# Datasets\n",
    "datasets = tutorial_config['datasets']\n",
    "print(f\"\\nüìö DATASETS:\")\n",
    "for i, dataset in enumerate(datasets, 1):\n",
    "    print(f\"   {i}. {dataset['name']}: {dataset['source']} ({dataset['split']})\")\n",
    "    print(f\"      Max length: {dataset['preprocessing']['max_length']} tokens\")\n",
    "\n",
    "# Tracking and logging\n",
    "logging_config = tutorial_config['logging']\n",
    "print(f\"\\nüìä ARBOR TRACKING & LOGGING:\")\n",
    "arbor_tracking = logging_config['arbor_tracking']\n",
    "print(f\"   Dashboard enabled: {arbor_tracking['enabled']}\")\n",
    "print(f\"   Save directory: {arbor_tracking['save_dir']}\")\n",
    "print(f\"   Dashboard port: {arbor_tracking['dashboard_port']}\")\n",
    "print(f\"   Live monitoring: {arbor_tracking['update_interval']}s intervals\")\n",
    "print(f\"   Alerts enabled: {arbor_tracking['alerts']['enabled']}\")\n",
    "print(f\"   Console logging: {logging_config['console']['enabled']} ({logging_config['console']['level']})\")\n",
    "\n",
    "# Training\n",
    "training = tutorial_config['training']\n",
    "print(f\"\\nüéØ TRAINING:\")\n",
    "print(f\"   Learning rate: {training['learning_rate']}\")\n",
    "print(f\"   Batch size: {training['per_device_train_batch_size']} √ó {training['gradient_accumulation_steps']} = {training['per_device_train_batch_size'] * training['gradient_accumulation_steps']}\")\n",
    "print(f\"   Total steps: {len(datasets)} √ó {training['steps_per_dataset']} = {len(datasets) * training['steps_per_dataset']}\")\n",
    "print(f\"   Mixed precision: {training['fp16']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c365cf",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the YAML Trainer\n",
    "\n",
    "Now let's create and initialize the YAML trainer with our configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eaed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the YAML trainer with robust error handling\n",
    "print(\"üì• Importing YAML Trainer...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First, ensure we have the necessary imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to import the YAML trainer with fallbacks\n",
    "trainer_imported = False\n",
    "import_error = None\n",
    "\n",
    "try:\n",
    "    from arbor.train.yaml_trainer import ArborYAMLTrainer\n",
    "    print(\"‚úÖ Successfully imported ArborYAMLTrainer\")\n",
    "    trainer_imported = True\n",
    "except ImportError as e:\n",
    "    import_error = e\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    \n",
    "    # Try adding paths and importing again\n",
    "    possible_paths = [\".\", \"arbor-o1-living-ai\", \"/content/arbor-o1-living-ai\"]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if Path(path, \"arbor\").exists():\n",
    "            print(f\"   Trying path: {path}\")\n",
    "            if str(Path(path).absolute()) not in sys.path:\n",
    "                sys.path.insert(0, str(Path(path).absolute()))\n",
    "            try:\n",
    "                from arbor.train.yaml_trainer import ArborYAMLTrainer\n",
    "                print(f\"‚úÖ Successfully imported ArborYAMLTrainer from {path}\")\n",
    "                trainer_imported = True\n",
    "                break\n",
    "            except ImportError as e2:\n",
    "                print(f\"   Failed from {path}: {e2}\")\n",
    "                continue\n",
    "\n",
    "if not trainer_imported:\n",
    "    print(\"\\nüîß Trainer import failed - trying alternative approaches...\")\n",
    "    \n",
    "    # Check if we're in Colab and need to install/setup\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"üåê Detected Google Colab environment\")\n",
    "        print(\"üí° Suggestions:\")\n",
    "        print(\"   1. Make sure you ran the dependencies installation cell successfully\")\n",
    "        print(\"   2. Try restarting the runtime: Runtime ‚Üí Restart runtime\")\n",
    "        print(\"   3. Re-run all cells from the beginning\")\n",
    "        print(\"   4. Check that Arbor was installed properly\")\n",
    "    except ImportError:\n",
    "        print(\"üíª Local environment detected\")\n",
    "        print(\"üí° Suggestions:\")\n",
    "        print(\"   1. Make sure you're in the arbor-o1-living-ai directory\")\n",
    "        print(\"   2. Try: pip install -e .\")\n",
    "        print(\"   3. Check that all dependencies are installed\")\n",
    "    \n",
    "    print(f\"\\n‚ùå Could not import ArborYAMLTrainer\")\n",
    "    print(f\"   Original error: {import_error}\")\n",
    "    print(\"   The notebook will continue with limited functionality\")\n",
    "    \n",
    "    # Create a mock trainer class for demonstration\n",
    "    class MockArborYAMLTrainer:\n",
    "        def __init__(self, config_path):\n",
    "            self.config_path = config_path\n",
    "            print(\"üîß Using mock trainer for demonstration\")\n",
    "            \n",
    "        def setup_tokenizer(self):\n",
    "            print(\"üîß Mock: Would setup tokenizer\")\n",
    "            \n",
    "        def setup_model(self):\n",
    "            print(\"üîß Mock: Would setup model\")\n",
    "            \n",
    "        def load_datasets(self):\n",
    "            print(\"üîß Mock: Would load datasets\")\n",
    "            \n",
    "        def train(self):\n",
    "            print(\"üîß Mock: Would run training\")\n",
    "    \n",
    "    ArborYAMLTrainer = MockArborYAMLTrainer\n",
    "    print(\"‚úÖ Mock trainer class created for demonstration purposes\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚úÖ Trainer import successful - full functionality available!\")\n",
    "\n",
    "print(\"\\nüìã Next steps:\")\n",
    "if trainer_imported:\n",
    "    print(\"   ‚Ä¢ Ready to initialize trainer with configuration\")\n",
    "    print(\"   ‚Ä¢ All Arbor features will be available\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ Will use mock trainer for demonstration\")\n",
    "    print(\"   ‚Ä¢ Fix import issues for full functionality\")\n",
    "    print(\"   ‚Ä¢ Check installation and restart runtime if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b52b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer with robust error handling\n",
    "print(\"üöÄ Initializing YAML Trainer...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if we have the required variables\n",
    "config_available = False\n",
    "config_path_to_use = None\n",
    "\n",
    "# Check for tutorial config path\n",
    "if 'tutorial_config_path' in globals():\n",
    "    config_path_to_use = tutorial_config_path\n",
    "    config_available = True\n",
    "    print(f\"‚úÖ Found tutorial_config_path: {tutorial_config_path}\")\n",
    "elif 'tutorial_config' in globals():\n",
    "    # If we have the config dict but not the path, create a temporary config file\n",
    "    import tempfile\n",
    "    import yaml\n",
    "    \n",
    "    temp_config = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False)\n",
    "    yaml.dump(tutorial_config, temp_config, default_flow_style=False)\n",
    "    temp_config.close()\n",
    "    \n",
    "    config_path_to_use = Path(temp_config.name)\n",
    "    config_available = True\n",
    "    print(f\"‚úÖ Created temporary config file: {config_path_to_use}\")\n",
    "else:\n",
    "    # Create a minimal config for demonstration with all required sections\n",
    "    print(\"‚ö†Ô∏è No tutorial config found - creating minimal config for demonstration\")\n",
    "    \n",
    "    minimal_config = {\n",
    "        'model': {\n",
    "            'name': 'demo-model',\n",
    "            'vocab_size': 50257,\n",
    "            'hidden_size': 512,\n",
    "            'num_layers': 6,\n",
    "            'num_heads': 8,\n",
    "            'max_seq_length': 1024,\n",
    "            'growth': {\n",
    "                'enabled': False\n",
    "            },\n",
    "            'adaptive_context': {\n",
    "                'enabled': False\n",
    "            }\n",
    "        },\n",
    "        # Required tokenizer section\n",
    "        'tokenizer': {\n",
    "            'name': 'gpt2',\n",
    "            'model_max_length': 1024,\n",
    "            'padding_side': 'left',\n",
    "            'add_eos_token': True,\n",
    "            'use_fast': True\n",
    "        },\n",
    "        'datasets': [{\n",
    "            'name': 'demo_dataset',\n",
    "            'source': 'roneneldan/TinyStories',\n",
    "            'split': 'train[:100]',\n",
    "            'text_column': 'text',\n",
    "            'preprocessing': {\n",
    "                'max_length': 512\n",
    "            }\n",
    "        }],\n",
    "        'training': {\n",
    "            'output_dir': './demo_output',\n",
    "            'learning_rate': 5e-5,\n",
    "            'per_device_train_batch_size': 1,\n",
    "            'max_train_steps': 10,\n",
    "            'logging_steps': 5,\n",
    "            'save_steps': 10\n",
    "        },\n",
    "        'logging': {\n",
    "            'console': {\n",
    "                'enabled': True,\n",
    "                'level': 'INFO'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    import tempfile\n",
    "    import yaml\n",
    "    \n",
    "    temp_config = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False)\n",
    "    yaml.dump(minimal_config, temp_config, default_flow_style=False)\n",
    "    temp_config.close()\n",
    "    \n",
    "    config_path_to_use = Path(temp_config.name)\n",
    "    config_available = True\n",
    "    print(f\"‚úÖ Created minimal demo config: {config_path_to_use}\")\n",
    "    print(\"   üìã Includes all required sections: model, tokenizer, datasets, training\")\n",
    "\n",
    "# Now try to initialize the trainer\n",
    "trainer = None\n",
    "trainer_initialized = False\n",
    "\n",
    "if config_available and 'ArborYAMLTrainer' in globals():\n",
    "    try:\n",
    "        trainer = ArborYAMLTrainer(str(config_path_to_use))\n",
    "        print(\"‚úÖ Trainer initialized successfully!\")\n",
    "        trainer_initialized = True\n",
    "        \n",
    "        # The trainer automatically validates the configuration\n",
    "        print(\"üìã Configuration loaded and validated\")\n",
    "        \n",
    "        # Show config info if available\n",
    "        if hasattr(trainer, 'config'):\n",
    "            print(f\"üìä Config loaded from: {config_path_to_use}\")\n",
    "            \n",
    "            # Show which sections were loaded\n",
    "            config_sections = []\n",
    "            if hasattr(trainer.config, 'model_config'):\n",
    "                config_sections.append(\"model\")\n",
    "            if hasattr(trainer.config, 'tokenizer_config'):\n",
    "                config_sections.append(\"tokenizer\")\n",
    "            if hasattr(trainer.config, 'dataset_config'):\n",
    "                config_sections.append(\"datasets\")\n",
    "            if hasattr(trainer.config, 'training_config'):\n",
    "                config_sections.append(\"training\")\n",
    "            \n",
    "            print(f\"üìã Loaded sections: {', '.join(config_sections)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Trainer initialization failed: {e}\")\n",
    "        print(\"üîß Creating mock trainer for demonstration...\")\n",
    "        \n",
    "        # Create a simple mock trainer with basic functionality\n",
    "        class DemoTrainer:\n",
    "            def __init__(self, config_path):\n",
    "                self.config_path = config_path\n",
    "                self.datasets = {}\n",
    "                self.model = None\n",
    "                self.tokenizer = None\n",
    "                \n",
    "            def setup_tokenizer(self):\n",
    "                print(\"üîß Demo: Setting up tokenizer...\")\n",
    "                self.tokenizer = MockTokenizer()\n",
    "                \n",
    "            def setup_model(self):\n",
    "                print(\"üîß Demo: Setting up model...\")\n",
    "                self.model = MockModel()\n",
    "                \n",
    "            def load_datasets(self):\n",
    "                print(\"üîß Demo: Loading datasets...\")\n",
    "                self.datasets = {\"demo\": MockDataset()}\n",
    "                \n",
    "            def train(self):\n",
    "                print(\"üîß Demo: Would run training pipeline...\")\n",
    "        \n",
    "        class MockTokenizer:\n",
    "            def encode(self, text):\n",
    "                return list(range(len(text.split())))\n",
    "            def decode(self, tokens):\n",
    "                return \" \".join([f\"token_{i}\" for i in tokens])\n",
    "            def __len__(self):\n",
    "                return 50000\n",
    "        \n",
    "        class MockModel:\n",
    "            def param_count(self):\n",
    "                return 1000000\n",
    "            def get_context_info(self):\n",
    "                return {\n",
    "                    'adaptive_context_enabled': True,\n",
    "                    'current_context_length': 1024,\n",
    "                    'min_context_length': 512,\n",
    "                    'max_context_length': 4096\n",
    "                }\n",
    "        \n",
    "        class MockDataset:\n",
    "            def __len__(self):\n",
    "                return 100\n",
    "            def __getitem__(self, idx):\n",
    "                return {'input_ids': [1, 2, 3, 4, 5]}\n",
    "        \n",
    "        trainer = DemoTrainer(str(config_path_to_use))\n",
    "        print(\"‚úÖ Demo trainer created for tutorial purposes\")\n",
    "        trainer_initialized = True\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize trainer:\")\n",
    "    if not config_available:\n",
    "        print(\"   ‚Ä¢ No configuration available\")\n",
    "    if 'ArborYAMLTrainer' not in globals():\n",
    "        print(\"   ‚Ä¢ ArborYAMLTrainer not imported\")\n",
    "    \n",
    "    print(\"\\nüí° To fix this:\")\n",
    "    print(\"   1. Make sure you ran the dependencies installation cell\")\n",
    "    print(\"   2. Run the configuration creation cell\")\n",
    "    print(\"   3. Check that imports worked properly\")\n",
    "    print(\"   4. Consider restarting runtime and running all cells\")\n",
    "\n",
    "print(\"\\nüìä Trainer Status:\")\n",
    "if trainer_initialized:\n",
    "    print(\"   ‚úÖ Trainer ready for use\")\n",
    "    print(\"   ‚úÖ Can proceed with tutorial steps\")\n",
    "    if hasattr(trainer, 'config'):\n",
    "        print(\"   ‚úÖ Configuration validation passed\")\n",
    "else:\n",
    "    print(\"   ‚ùå Trainer not available\")\n",
    "    print(\"   ‚ö†Ô∏è Tutorial will have limited functionality\")\n",
    "\n",
    "# Make trainer available globally\n",
    "if trainer:\n",
    "    globals()['trainer'] = trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da191e",
   "metadata": {},
   "source": [
    "## Step 4: Setup Components\n",
    "\n",
    "The trainer needs to setup several components before training. Let's do this step by step to see what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a464b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a: Setup tokenizer\n",
    "print(\"üì• Setting up tokenizer...\")\n",
    "try:\n",
    "    trainer.setup_tokenizer()\n",
    "    print(f\"‚úÖ Tokenizer ready: {len(trainer.tokenizer):,} vocabulary\")\n",
    "    \n",
    "    # Test the tokenizer\n",
    "    test_text = \"Hello, this is a test of the Hermes tokenizer!\"\n",
    "    tokens = trainer.tokenizer.encode(test_text)\n",
    "    print(f\"üß™ Test encoding: '{test_text}' ‚Üí {len(tokens)} tokens\")\n",
    "    print(f\"   First 10 tokens: {tokens[:10]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Tokenizer setup failed: {e}\")\n",
    "    print(\"This might be due to internet connectivity or HuggingFace access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20447431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b: Setup model\n",
    "print(\"\\nü§ñ Setting up Arbor model...\")\n",
    "try:\n",
    "    trainer.setup_model()\n",
    "    print(f\"‚úÖ Model created: {trainer.model.param_count():,} parameters\")\n",
    "    \n",
    "    # Show model architecture details\n",
    "    config = trainer.model.config\n",
    "    print(f\"\\nüìä Model details:\")\n",
    "    print(f\"   Architecture: {config.num_layers} layers\")\n",
    "    print(f\"   Hidden size: {config.dim}\")\n",
    "    print(f\"   Attention heads: {config.num_heads}\")\n",
    "    print(f\"   FFN dimension: {config.ffn_dim}\")\n",
    "    print(f\"   Max sequence length: {config.max_seq_length:,}\")\n",
    "    \n",
    "    # Show adaptive context info\n",
    "    if hasattr(trainer.model, 'get_context_info'):\n",
    "        context_info = trainer.model.get_context_info()\n",
    "        print(f\"\\nüß† Adaptive context info:\")\n",
    "        print(f\"   Enabled: {context_info['adaptive_context_enabled']}\")\n",
    "        if context_info['adaptive_context_enabled']:\n",
    "            print(f\"   Current context: {context_info['current_context_length']:,}\")\n",
    "            print(f\"   Context range: {context_info['min_context_length']:,} - {context_info['max_context_length']:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model setup failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4c: Load datasets with robust error handling\n",
    "print(\"\\nüìö Loading datasets...\")\n",
    "\n",
    "# Check if trainer is available\n",
    "if 'trainer' not in globals() or trainer is None:\n",
    "    print(\"‚ùå Trainer not available - cannot load datasets\")\n",
    "    print(\"üí° Please run the trainer initialization cells first\")\n",
    "else:\n",
    "    try:\n",
    "        # Check if this is a real trainer or mock trainer\n",
    "        trainer_type = type(trainer).__name__\n",
    "        print(f\"üîç Using trainer type: {trainer_type}\")\n",
    "        \n",
    "        if hasattr(trainer, 'load_datasets') and callable(trainer.load_datasets):\n",
    "            trainer.load_datasets()\n",
    "            \n",
    "            # Check if datasets were loaded successfully\n",
    "            if hasattr(trainer, 'datasets') and trainer.datasets:\n",
    "                print(f\"‚úÖ Datasets loaded: {len(trainer.datasets)}\")\n",
    "                \n",
    "                # Show dataset info\n",
    "                for name, dataset in trainer.datasets.items():\n",
    "                    print(f\"\\nüìä Dataset: {name}\")\n",
    "                    print(f\"   Size: {len(dataset):,} examples\")\n",
    "                    \n",
    "                    # Show a sample if possible\n",
    "                    if len(dataset) > 0:\n",
    "                        try:\n",
    "                            sample = dataset[0]\n",
    "                            if isinstance(sample, dict) and 'input_ids' in sample:\n",
    "                                input_ids = sample['input_ids']\n",
    "                                if hasattr(trainer, 'tokenizer') and trainer.tokenizer:\n",
    "                                    try:\n",
    "                                        decoded = trainer.tokenizer.decode(input_ids[:50])  # First 50 tokens\n",
    "                                        print(f\"   Sample: {decoded}...\")\n",
    "                                        print(f\"   Token length: {len(input_ids)}\")\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"   Sample tokens: {input_ids[:10]}...\")\n",
    "                                        print(f\"   Token length: {len(input_ids)}\")\n",
    "                                else:\n",
    "                                    print(f\"   Sample tokens: {input_ids[:10]}...\")\n",
    "                                    print(f\"   Token length: {len(input_ids)}\")\n",
    "                            else:\n",
    "                                print(f\"   Sample: {str(sample)[:100]}...\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"   Sample preview failed: {e}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No datasets found in trainer\")\n",
    "                \n",
    "        else:\n",
    "            print(\"üîß Mock trainer detected - simulating dataset loading...\")\n",
    "            \n",
    "            # Create mock datasets for demonstration\n",
    "            class MockDataset:\n",
    "                def __init__(self, name, size=1000):\n",
    "                    self.name = name\n",
    "                    self.size = size\n",
    "                    \n",
    "                def __len__(self):\n",
    "                    return self.size\n",
    "                    \n",
    "                def __getitem__(self, idx):\n",
    "                    return {\n",
    "                        'input_ids': list(range(idx, idx + 20)),  # Mock token sequence\n",
    "                        'attention_mask': [1] * 20\n",
    "                    }\n",
    "            \n",
    "            # Simulate the datasets from the config\n",
    "            mock_datasets = {\n",
    "                'tinystories_demo': MockDataset('tinystories_demo', 5000),\n",
    "                'code_demo': MockDataset('code_demo', 2000),\n",
    "                'reasoning_demo': MockDataset('reasoning_demo', 1000)\n",
    "            }\n",
    "            \n",
    "            trainer.datasets = mock_datasets\n",
    "            print(\"‚úÖ Mock datasets created for demonstration\")\n",
    "            \n",
    "            for name, dataset in mock_datasets.items():\n",
    "                print(f\"\\nüìä Dataset: {name}\")\n",
    "                print(f\"   Size: {len(dataset):,} examples\")\n",
    "                print(f\"   Sample: Mock data with token sequences\")\n",
    "                print(f\"   Token length: 20 (simulated)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Dataset loading failed: {e}\")\n",
    "        print(\"This might be due to:\")\n",
    "        print(\"   ‚Ä¢ Internet connectivity issues\")\n",
    "        print(\"   ‚Ä¢ HuggingFace dataset access problems\")\n",
    "        print(\"   ‚Ä¢ Missing dependencies\")\n",
    "        print(\"   ‚Ä¢ Configuration errors\")\n",
    "        \n",
    "        import traceback\n",
    "        print(f\"\\nDetailed error:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\nüîß Creating fallback mock datasets...\")\n",
    "        \n",
    "        # Create simple mock datasets as fallback\n",
    "        class SimpleDataset:\n",
    "            def __init__(self, size):\n",
    "                self.size = size\n",
    "                \n",
    "            def __len__(self):\n",
    "                return self.size\n",
    "                \n",
    "            def __getitem__(self, idx):\n",
    "                return {'input_ids': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "        \n",
    "        if hasattr(trainer, 'datasets'):\n",
    "            trainer.datasets = {\n",
    "                'demo_dataset': SimpleDataset(100)\n",
    "            }\n",
    "            print(\"‚úÖ Fallback datasets created\")\n",
    "        else:\n",
    "            print(\"‚ùå Cannot create fallback datasets - trainer structure unknown\")\n",
    "\n",
    "print(\"\\nüìä Dataset Loading Summary:\")\n",
    "if 'trainer' in globals() and hasattr(trainer, 'datasets') and trainer.datasets:\n",
    "    print(f\"   ‚úÖ {len(trainer.datasets)} datasets available\")\n",
    "    for name in trainer.datasets.keys():\n",
    "        print(f\"   ‚Ä¢ {name}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No datasets available\")\n",
    "    print(\"   ‚Ä¢ Check trainer initialization\")\n",
    "    print(\"   ‚Ä¢ Verify internet connectivity\")\n",
    "    print(\"   ‚Ä¢ Consider using mock data for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539c8f4",
   "metadata": {},
   "source": [
    "## Step 5: Training Setup\n",
    "\n",
    "Before we start training, let's setup logging and create the trainer objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Arbor tracking and logging\n",
    "print(\"üìä Setting up Arbor tracking system...\")\n",
    "try:\n",
    "    # Initialize TrainingMonitor with configuration\n",
    "    tracking_config = tutorial_config['logging']['arbor_tracking']\n",
    "    \n",
    "    # Create training monitor\n",
    "    training_monitor = TrainingMonitor(\n",
    "        save_dir=tracking_config['save_dir'],\n",
    "        update_interval=tracking_config['update_interval']\n",
    "    )\n",
    "    \n",
    "    # Setup alert system if enabled\n",
    "    if tracking_config['alerts']['enabled']:\n",
    "        training_monitor.setup_alerts(\n",
    "            email_enabled=tracking_config['alerts']['email_notifications'],\n",
    "            webhook_url=tracking_config['alerts']['webhook_url']\n",
    "        )\n",
    "    \n",
    "    print(\"‚úÖ Arbor tracking system initialized\")\n",
    "    print(f\"   üìä Dashboard will be available at: http://localhost:{tracking_config['dashboard_port']}\")\n",
    "    print(f\"   üíæ Metrics saved to: {tracking_config['save_dir']}\")\n",
    "    print(f\"   üîî Alerts enabled: {tracking_config['alerts']['enabled']}\")\n",
    "    \n",
    "    # Setup traditional logging as well\n",
    "    trainer.setup_logging()\n",
    "    print(\"‚úÖ Console logging configured\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Tracking setup had issues: {e}\")\n",
    "    print(\"Training can continue with basic logging\")\n",
    "    # Fallback to basic logging\n",
    "    try:\n",
    "        trainer.setup_logging()\n",
    "        print(\"‚úÖ Basic logging configured\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Even basic logging had issues - continuing anyway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e02f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Arbor Dashboard\n",
    "print(\"\\nüåê Starting Arbor Dashboard...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dashboard_port = tutorial_config['logging']['arbor_tracking']['dashboard_port']\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üåê Google Colab Setup:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(\"To access the dashboard in Colab, we'll create a public URL using ngrok.\")\n",
    "    print(\"\")\n",
    "    print(\"üìã Steps to access the dashboard:\")\n",
    "    print(\"1. üîë Get an ngrok token from https://ngrok.com (free signup)\")\n",
    "    print(\"2. üöÄ Run the dashboard startup cell below\")\n",
    "    print(\"3. üåê Click the public ngrok URL to access the dashboard\")\n",
    "    print(\"\")\n",
    "    print(\"üí° The dashboard will show:\")\n",
    "    print(\"   ‚Ä¢ üìà Live training metrics (loss, learning rate)\")\n",
    "    print(\"   ‚Ä¢ üèóÔ∏è Model architecture with layer utilization\")  \n",
    "    print(\"   ‚Ä¢ üå± Growth tracking (parameters and layers)\")\n",
    "    print(\"   ‚Ä¢ üîî Training alerts and notifications\")\n",
    "    print(\"   ‚Ä¢ üìä Performance analytics and system monitoring\")\n",
    "    print(\"\")\n",
    "    print(\"‚ö†Ô∏è Note: In Colab, the dashboard will be accessible via a public ngrok URL\")\n",
    "    print(\"        Anyone with the URL can access it (ngrok free tier limitation)\")\n",
    "else:\n",
    "    print(\"üíª Local Development Setup:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(\"To monitor training locally:\")\n",
    "    print(\"1. üöÄ Option A - Use the launcher:\")\n",
    "    print(\"   python launch_training_dashboard.py\")\n",
    "    print()\n",
    "    print(\"2. üìä Option B - Start dashboard manually:\")\n",
    "    print(f\"   streamlit run arbor/tracking/dashboard.py --server.port={dashboard_port}\")\n",
    "    print()\n",
    "    print(\"3. üåê Then open your browser to:\")\n",
    "    print(f\"   http://localhost:{dashboard_port}\")\n",
    "\n",
    "print()\n",
    "print(\"üìä Dashboard Features:\")\n",
    "print(\"   ‚Ä¢ üìà Live training metrics with real-time updates\")\n",
    "print(\"   ‚Ä¢ üèóÔ∏è Interactive model architecture visualization\")\n",
    "print(\"   ‚Ä¢ üå± Parameter and layer growth timeline\")\n",
    "print(\"   ‚Ä¢ üîî Alert system with configurable notifications\")\n",
    "print(\"   ‚Ä¢ üìä Performance analytics and export capabilities\")\n",
    "print()\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Next: Run the 'Start Dashboard in Colab' cell below to get your public URL!\")\n",
    "else:\n",
    "    print(\"üí° Tip: Start the dashboard in another terminal before running training!\")\n",
    "    print(\"    The dashboard will automatically pick up training data as it's generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78deb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dashboard in Colab (with public URL)\n",
    "if IN_COLAB:\n",
    "    print(\"üöÄ Starting Arbor Dashboard in Google Colab\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if ngrok auth token is available\n",
    "    ngrok_token = os.getenv('NGROK_AUTH_TOKEN')\n",
    "    if not ngrok_token:\n",
    "        print(\"üîë Please enter your ngrok auth token:\")\n",
    "        print(\"   Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "        ngrok_token = getpass.getpass(\"Enter ngrok auth token: \")\n",
    "        \n",
    "        if ngrok_token.strip():\n",
    "            ngrok.set_auth_token(ngrok_token.strip())\n",
    "            print(\"‚úÖ ngrok authentication configured\")\n",
    "        else:\n",
    "            print(\"‚ùå No token provided - dashboard will only be accessible locally\")\n",
    "            print(\"   You can still run the dashboard, but won't get a public URL\")\n",
    "    \n",
    "    # Start the dashboard with threading to run in background\n",
    "    import threading\n",
    "    import subprocess\n",
    "    import time\n",
    "    \n",
    "    # Create a function to run streamlit\n",
    "    def run_streamlit():\n",
    "        try:\n",
    "            # Change to the correct directory and run streamlit\n",
    "            dashboard_cmd = [\n",
    "                sys.executable, \"-m\", \"streamlit\", \"run\", \n",
    "                \"arbor/tracking/dashboard.py\",\n",
    "                \"--server.port\", str(DASHBOARD_PORT),\n",
    "                \"--server.headless\", \"true\",\n",
    "                \"--server.enableCORS\", \"false\",\n",
    "                \"--browser.gatherUsageStats\", \"false\"\n",
    "            ]\n",
    "            \n",
    "            print(f\"üé¨ Starting Streamlit dashboard...\")\n",
    "            subprocess.run(dashboard_cmd)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to start dashboard: {e}\")\n",
    "    \n",
    "    # Start streamlit in background thread\n",
    "    dashboard_thread = threading.Thread(target=run_streamlit, daemon=True)\n",
    "    dashboard_thread.start()\n",
    "    \n",
    "    # Wait a moment for streamlit to start\n",
    "    print(\"‚è≥ Waiting for dashboard to initialize...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Create ngrok tunnel if token is available\n",
    "    if ngrok_token and ngrok_token.strip():\n",
    "        try:\n",
    "            print(\"üåê Creating public tunnel with ngrok...\")\n",
    "            public_url = ngrok.connect(DASHBOARD_PORT)\n",
    "            PUBLIC_URL = public_url\n",
    "            \n",
    "            print(\"üéâ Dashboard is ready!\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"üåê Public URL: {public_url}\")\n",
    "            print(f\"üì± Mobile-friendly: {public_url}\")\n",
    "            print(\"üîó Click the link above to access your dashboard\")\n",
    "            print()\n",
    "            print(\"‚ö†Ô∏è Security Note: This URL is public and accessible to anyone\")\n",
    "            print(\"   Consider using ngrok's paid plan for password protection\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create ngrok tunnel: {e}\")\n",
    "            print(f\"üè† Dashboard available locally at: http://localhost:{DASHBOARD_PORT}\")\n",
    "            print(\"   (Not accessible outside Colab without tunnel)\")\n",
    "    else:\n",
    "        print(f\"üè† Dashboard running locally at: http://localhost:{DASHBOARD_PORT}\")\n",
    "        print(\"   (Not accessible outside Colab - need ngrok token for public access)\")\n",
    "\n",
    "else:\n",
    "    print(\"üíª Local Environment: Use the previous instructions to start the dashboard manually\")\n",
    "    print(\"    This cell is only for Google Colab automatic setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e943737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer for the first dataset to inspect the setup\n",
    "if trainer.datasets:\n",
    "    dataset_name = list(trainer.datasets.keys())[0]\n",
    "    print(f\"üîß Creating trainer for dataset: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        hf_trainer = trainer.create_trainer(dataset_name)\n",
    "        print(f\"‚úÖ HuggingFace trainer created\")\n",
    "        print(f\"   Training dataset: {len(hf_trainer.train_dataset):,} examples\")\n",
    "        if hf_trainer.eval_dataset:\n",
    "            print(f\"   Eval dataset: {len(hf_trainer.eval_dataset):,} examples\")\n",
    "        \n",
    "        # Show training arguments\n",
    "        args = hf_trainer.args\n",
    "        print(f\"\\n‚öôÔ∏è  Training arguments:\")\n",
    "        print(f\"   Output dir: {args.output_dir}\")\n",
    "        print(f\"   Learning rate: {args.learning_rate}\")\n",
    "        print(f\"   Batch size: {args.per_device_train_batch_size}\")\n",
    "        print(f\"   Max steps: {args.max_steps}\")\n",
    "        print(f\"   Save steps: {args.save_steps}\")\n",
    "        print(f\"   Eval steps: {args.eval_steps}\")\n",
    "        print(f\"   FP16: {args.fp16}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Trainer creation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374699f",
   "metadata": {},
   "source": [
    "## Step 6: Test Adaptive Context System\n",
    "\n",
    "Before training, let's test the adaptive context system with different types of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the adaptive context system\n",
    "print(\"üß† Testing Adaptive Context System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different types of inputs\n",
    "test_inputs = {\n",
    "    \"simple_chat\": \"Hello! How are you today?\",\n",
    "    \n",
    "    \"code_task\": \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# This is a recursive implementation\n",
    "# Could be optimized with dynamic programming\n",
    "for i in range(10):\n",
    "    print(f\"fib({i}) = {fibonacci(i)}\")\n",
    "\"\"\",\n",
    "    \n",
    "    \"reasoning_task\": \"\"\"\n",
    "Let me think through this step by step. If we have a logical puzzle where:\n",
    "1. All cats are animals\n",
    "2. Some animals are pets  \n",
    "3. No pets are wild\n",
    "4. Some cats are wild\n",
    "\n",
    "We need to determine if there's a contradiction. Let me analyze each statement carefully\n",
    "and see if they can all be true simultaneously. This requires careful logical reasoning\n",
    "to avoid making invalid inferences.\n",
    "\"\"\",\n",
    "    \n",
    "    \"long_document\": \"\"\"\n",
    "This is a comprehensive research paper on machine learning that covers multiple aspects\n",
    "of the field. The introduction provides background on artificial intelligence and its\n",
    "historical development. The methodology section describes various approaches including\n",
    "supervised learning, unsupervised learning, and reinforcement learning paradigms.\n",
    "\"\"\" + \" The paper continues with detailed analysis.\" * 50  # Make it longer\n",
    "}\n",
    "\n",
    "# Test each input type\n",
    "for task_type, text in test_inputs.items():\n",
    "    print(f\"\\nüîç Testing: {task_type}\")\n",
    "    print(f\"Input length: {len(text)} characters\")\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = trainer.tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    token_count = input_ids.shape[1]\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    \n",
    "    # Get current context info\n",
    "    initial_context = trainer.model.get_context_info()['current_context_length']\n",
    "    \n",
    "    # Test the model (this should trigger adaptive context)\n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # This forward pass will trigger context adaptation\n",
    "            outputs = trainer.model(input_ids, return_dict=True)\n",
    "            \n",
    "            # Check if context adapted\n",
    "            final_context = trainer.model.get_context_info()['current_context_length']\n",
    "            \n",
    "            print(f\"Context: {initial_context:,} ‚Üí {final_context:,} tokens\")\n",
    "            if final_context != initial_context:\n",
    "                print(f\"‚úÖ Context adapted for {task_type}\")\n",
    "            else:\n",
    "                print(f\"‚Üí Context unchanged for {task_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {task_type}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc04629",
   "metadata": {},
   "source": [
    "## Step 7: Run Training\n",
    "\n",
    "Now let's run the actual training! We'll train for a short period to demonstrate the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33735d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This will actually train the model!\n",
    "print(\"‚ö†Ô∏è  TRAINING WARNING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"The next cell will run actual training.\")\n",
    "print(\"This may take several minutes and will:\")\n",
    "print(\"‚Ä¢ Download datasets from HuggingFace\")\n",
    "print(\"‚Ä¢ Train the model for 200 steps\")\n",
    "print(\"‚Ä¢ Show parameter growth during training\")\n",
    "print(\"‚Ä¢ Save model checkpoints\")\n",
    "print(\"\")\n",
    "print(\"Set RUN_TRAINING = True to proceed\")\n",
    "\n",
    "RUN_TRAINING = False  # Set to True to actually run training\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    print(\"üöÄ Starting training pipeline...\")\n",
    "else:\n",
    "    print(\"üõë Training skipped (set RUN_TRAINING = True to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ef955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training if enabled\n",
    "if RUN_TRAINING:\n",
    "    print(\"üöÄ Starting Arbor YAML Training Pipeline with Live Dashboard\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Start monitoring\n",
    "        if 'training_monitor' in locals():\n",
    "            training_monitor.start_monitoring()\n",
    "            print(\"üìä Training monitor started - metrics will be saved for dashboard\")\n",
    "        \n",
    "        # Show dashboard access info\n",
    "        if IN_COLAB and 'PUBLIC_URL' in globals() and PUBLIC_URL:\n",
    "            print(f\"üåê Monitor training at: {PUBLIC_URL}\")\n",
    "        elif not IN_COLAB:\n",
    "            dashboard_port = tutorial_config['logging']['arbor_tracking']['dashboard_port']\n",
    "            print(f\"üåê Monitor training at: http://localhost:{dashboard_port}\")\n",
    "        else:\n",
    "            print(\"üìä Dashboard metrics being collected (check dashboard startup cells for URL)\")\n",
    "        \n",
    "        # This runs the complete training pipeline\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"\\nüéâ Training completed successfully!\")\n",
    "        \n",
    "        # Show final model stats\n",
    "        final_params = trainer.model.param_count()\n",
    "        print(f\"üìä Final model size: {final_params:,} parameters\")\n",
    "        \n",
    "        # Show training outputs\n",
    "        output_dir = Path(trainer.config.training_config['output_dir'])\n",
    "        if output_dir.exists():\n",
    "            saved_models = list(output_dir.glob(\"*/\"))\n",
    "            print(f\"üíæ Saved {len(saved_models)} model checkpoints:\")\n",
    "            for model_dir in saved_models:\n",
    "                print(f\"   üìÅ {model_dir.name}\")\n",
    "        \n",
    "        # Stop monitoring and generate report\n",
    "        if 'training_monitor' in locals():\n",
    "            training_monitor.stop_monitoring()\n",
    "            report_file = training_monitor.export_training_report()\n",
    "            print(f\"üìÑ Training report saved: {report_file}\")\n",
    "            \n",
    "            # Show final dashboard link\n",
    "            if IN_COLAB and 'PUBLIC_URL' in globals() and PUBLIC_URL:\n",
    "                print(f\"\\nüåê View final metrics at: {PUBLIC_URL}\")\n",
    "            elif not IN_COLAB:\n",
    "                dashboard_port = tutorial_config['logging']['arbor_tracking']['dashboard_port']\n",
    "                print(f\"\\nüåê View final metrics at: http://localhost:{dashboard_port}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Stop monitoring on error\n",
    "        if 'training_monitor' in locals():\n",
    "            training_monitor.stop_monitoring()\n",
    "        \n",
    "else:\n",
    "    # Simulate what training would show with Arbor tracking\n",
    "    print(\"üìã Training simulation (would show):\")\n",
    "    print(\"üå± Initialized Arbor trainer with config: configs/tutorial_config.yaml\")\n",
    "    print(\"üìä Arbor tracking system initialized:\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        print(\"   ‚Ä¢ Dashboard ready with public ngrok URL\")\n",
    "        if 'PUBLIC_URL' in globals() and PUBLIC_URL:\n",
    "            print(f\"   ‚Ä¢ Access at: {PUBLIC_URL}\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ Public URL will be generated when dashboard starts\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Dashboard ready at http://localhost:8501\")\n",
    "    \n",
    "    print(\"   ‚Ä¢ Metrics saved to: ./training_logs\")\n",
    "    print(\"   ‚Ä¢ Real-time monitoring enabled\")\n",
    "    print(\"   ‚Ä¢ Alert system active\")\n",
    "    print(\" Downloading fresh Hermes-4-405B tokenizer...\")\n",
    "    print(\"‚úÖ Successfully loaded fresh Hermes-4-405B tokenizer\")\n",
    "    print(\"‚úÖ Created Arbor model: 347,394,048 parameters\")\n",
    "    print(\"üß† Adaptive context enabled:\")\n",
    "    print(\"   Range: 512 - 32,768\")\n",
    "    print(\"   Supported tasks: 4\")\n",
    "    print(\"üå± Growth monitoring enabled:\")\n",
    "    print(\"   Factor: 1.5x\")\n",
    "    print(\"   Max steps: 4\")\n",
    "    print(\"üìö Loading datasets...\")\n",
    "    print(\"   ‚úÖ tiny_stories: 1,000 examples\")\n",
    "    print(\"   ‚úÖ code_samples: 500 examples\")\n",
    "    print(\"\")\n",
    "    print(\"üéØ Training on tiny_stories...\")\n",
    "    print(\"   üìä Dashboard: Real-time loss curves, layer utilization heatmaps\")\n",
    "    print(\"   üìä Parameters: 347,394,048 ‚Üí 347,894,048 (growth occurred)\")\n",
    "    print(\"   üîî Alert: Layer growth event detected\")\n",
    "    print(\"   ‚úÖ tiny_stories complete!\")\n",
    "    print(\"\")\n",
    "    print(\"üéØ Training on code_samples...\")\n",
    "    print(\"   üìä Dashboard: Architecture visualization updated\")\n",
    "    print(\"   üìä Parameters: 347,894,048 ‚Üí 348,394,048 (growth occurred)\")\n",
    "    print(\"   üîî Alert: Performance threshold reached\")\n",
    "    print(\"   ‚úÖ code_samples complete!\")\n",
    "    print(\"\")\n",
    "    print(\"üéâ Training pipeline complete!\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        print(\"üåê View comprehensive analytics at your public dashboard URL\")\n",
    "    else:\n",
    "        print(\"üìä Final dashboard shows comprehensive training analytics\")\n",
    "    \n",
    "    print(\"üìÑ Training report exported with growth timeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfa1c1",
   "metadata": {},
   "source": [
    "## Step 8: Test the Trained Model\n",
    "\n",
    "Let's test our model (or demonstrate what testing would look like) with different task types to see how the adaptive context system works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model generation with different tasks\n",
    "print(\"üß™ Testing Trained Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_prompts = {\n",
    "    \"story\": \"Once upon a time, in a magical forest\",\n",
    "    \"code\": \"# Python function to calculate factorial\\ndef factorial(n):\",\n",
    "    \"reasoning\": \"Let me solve this step by step. The problem is:\",\n",
    "    \"chat\": \"User: What's the weather like today?\\nAssistant:\"\n",
    "}\n",
    "\n",
    "for task_type, prompt in test_prompts.items():\n",
    "    print(f\"\\nüéØ Testing {task_type} task:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = trainer.tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Show what would happen with adaptive context\n",
    "    print(f\"Input tokens: {input_ids.shape[1]}\")\n",
    "    \n",
    "    if RUN_TRAINING:\n",
    "        # Actually test the trained model\n",
    "        trainer.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Generate response\n",
    "                generated = trainer.model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=50,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                \n",
    "                # Decode response\n",
    "                response = trainer.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                print(f\"Generated: {response[len(prompt):]}\")\n",
    "                \n",
    "                # Show context info\n",
    "                context_info = trainer.model.get_context_info()\n",
    "                print(f\"Context used: {context_info['current_context_length']:,} tokens\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Generation failed: {e}\")\n",
    "    else:\n",
    "        # Simulate what would happen\n",
    "        simulated_contexts = {\"story\": 2048, \"code\": 4096, \"reasoning\": 8192, \"chat\": 1024}\n",
    "        print(f\"Would adapt context to: {simulated_contexts[task_type]:,} tokens\")\n",
    "        print(f\"Would generate appropriate {task_type} response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46680c",
   "metadata": {},
   "source": [
    "## Step 9: Configuration Tips and Best Practices\n",
    "\n",
    "Here are some tips for customizing your YAML training configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration tips and best practices\n",
    "print(\"üí° YAML Configuration Tips\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tips = {\n",
    "    \"Model Size\": {\n",
    "        \"Small (100M)\": \"hidden_size: 512, num_layers: 12\",\n",
    "        \"Medium (500M)\": \"hidden_size: 1024, num_layers: 24\", \n",
    "        \"Large (1B)\": \"hidden_size: 1536, num_layers: 32\"\n",
    "    },\n",
    "    \n",
    "    \"Context Lengths\": {\n",
    "        \"Short tasks\": \"max 4K tokens (chat, Q&A)\",\n",
    "        \"Medium tasks\": \"4K-16K tokens (code, creative)\",\n",
    "        \"Long tasks\": \"16K+ tokens (documents, reasoning)\"\n",
    "    },\n",
    "    \n",
    "    \"Growth Settings\": {\n",
    "        \"Conservative\": \"factor: 1.25, threshold: 0.95\",\n",
    "        \"Moderate\": \"factor: 1.5, threshold: 0.9\",\n",
    "        \"Aggressive\": \"factor: 2.0, threshold: 0.85\"\n",
    "    },\n",
    "    \n",
    "    \"Training Speed\": {\n",
    "        \"Fast prototyping\": \"small datasets, few steps\",\n",
    "        \"Full training\": \"complete datasets, many steps\", \n",
    "        \"Production\": \"multiple epochs, careful validation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, options in tips.items():\n",
    "    print(f\"\\nüîß {category}:\")\n",
    "    for option, description in options.items():\n",
    "        print(f\"   {option}: {description}\")\n",
    "\n",
    "print(f\"\\nüìã Common YAML patterns:\")\n",
    "print(\"\"\"\n",
    "# Full monitoring setup for research\n",
    "adaptive_context: \n",
    "  enabled: true\n",
    "growth:\n",
    "  enabled: true\n",
    "logging:\n",
    "  arbor_tracking:\n",
    "    enabled: true\n",
    "    save_dir: './training_logs'\n",
    "    dashboard_port: 8501\n",
    "    alerts:\n",
    "      enabled: true\n",
    "\n",
    "# Minimal setup for testing  \n",
    "adaptive_context:\n",
    "  enabled: false\n",
    "growth:\n",
    "  enabled: false\n",
    "logging:\n",
    "  arbor_tracking:\n",
    "    enabled: false\n",
    "  console:\n",
    "    enabled: true\n",
    "datasets:\n",
    "  - name: \"test\"\n",
    "    source: \"roneneldan/TinyStories\"\n",
    "    split: \"train[:100]\"\n",
    "\n",
    "# Production setup with full tracking\n",
    "logging:\n",
    "  arbor_tracking:\n",
    "    enabled: true\n",
    "    save_dir: './production_logs'\n",
    "    update_interval: 0.5\n",
    "    dashboard_port: 8501\n",
    "    alerts:\n",
    "      enabled: true\n",
    "      email_notifications: true\n",
    "      webhook_url: \"https://your-webhook-url.com\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97ae4e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned how to use the Arbor YAML training system with real-time monitoring. Here's what we covered:\n",
    "\n",
    "### ‚úÖ **What You Learned:**\n",
    "\n",
    "1. **üìã YAML Configuration** - How to create and customize training configs\n",
    "2. **üß† Adaptive Context** - Task-aware context window adaptation  \n",
    "3. **üå± Dynamic Growth** - Parameter expansion during training\n",
    "4. **üöÄ Easy Training** - One-command training with `python train.py config.yaml`\n",
    "5. **üìä Live Monitoring** - Real-time dashboard with training visualization\n",
    "6. **üß™ Testing & Validation** - How to test trained models\n",
    "\n",
    "### üéØ **Key Benefits:**\n",
    "\n",
    "- **Simple**: Just edit YAML, no complex code\n",
    "- **Powerful**: Full control over model architecture and training\n",
    "- **Smart**: Automatic context adaptation and parameter growth\n",
    "- **Visual**: Real-time dashboard with live metrics and architecture visualization\n",
    "- **Production Ready**: HuggingFace integration and comprehensive monitoring\n",
    "\n",
    "### üöÄ **Next Steps:**\n",
    "\n",
    "1. **Customize** your own YAML config for your use case\n",
    "2. **Train** with real datasets for your domain\n",
    "3. **Monitor** training with the Arbor dashboard at http://localhost:8501\n",
    "4. **Deploy** trained models to HuggingFace Hub\n",
    "\n",
    "### üìä **Dashboard Features:**\n",
    "\n",
    "- **Live Metrics**: Training loss, learning rate, gradient norms in real-time\n",
    "- **Architecture View**: Interactive model visualization with layer utilization\n",
    "- **Growth Tracking**: Parameter and layer growth timeline\n",
    "- **Alert System**: Automatic notifications for training events\n",
    "- **Analytics**: Performance statistics and comprehensive reports\n",
    "\n",
    "The YAML trainer with Arbor dashboard makes it incredibly easy to experiment with cutting-edge transformer architectures while monitoring everything in real-time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa08be",
   "metadata": {},
   "source": [
    "## üåê Google Colab Users - Public Dashboard Access\n",
    "\n",
    "### üéØ **For Google Colab Users:**\n",
    "\n",
    "Since Google Colab doesn't allow direct access to `localhost`, we use **ngrok** to create a public tunnel to your dashboard.\n",
    "\n",
    "#### üöÄ **Quick Setup:**\n",
    "\n",
    "1. **Get ngrok token** (free): Go to [ngrok.com](https://ngrok.com) ‚Üí Sign up ‚Üí Get your auth token\n",
    "2. **Run the environment setup cells** above to detect Colab and install packages\n",
    "3. **Run the dashboard startup cell** - enter your ngrok token when prompted\n",
    "4. **Get your public URL** - a unique URL like `https://abc123.ngrok.io`\n",
    "5. **Monitor training** - click the public URL to access your dashboard from anywhere!\n",
    "\n",
    "#### üìä **Dashboard Features in Colab:**\n",
    "\n",
    "- **üì± Mobile-Responsive**: Perfect for monitoring on your phone\n",
    "- **üåê Public Access**: Share the URL with team members\n",
    "- **üîÑ Real-Time Updates**: Just like local development\n",
    "- **üìà Full Analytics**: All features work identically\n",
    "\n",
    "#### ‚ö†Ô∏è **Important Notes:**\n",
    "\n",
    "- **Public URL**: Free ngrok URLs are accessible by anyone with the link\n",
    "- **Session Timeout**: URLs expire when you restart the Colab session\n",
    "- **Data Privacy**: Consider ngrok Pro for password protection in production\n",
    "\n",
    "#### üí° **Pro Tips:**\n",
    "\n",
    "- Keep this notebook tab open while training runs\n",
    "- Bookmark the ngrok URL for easy access\n",
    "- Save important results to Google Drive\n",
    "- Consider upgrading to ngrok Pro for private URLs\n",
    "\n",
    "### üîó **URL Structure:**\n",
    "```\n",
    "Local: http://localhost:8501\n",
    "Colab: https://[random].ngrok.io\n",
    "```\n",
    "\n",
    "Both URLs provide identical dashboard functionality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c65eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and final info\n",
    "print(\"üßπ Cleanup and Final Info\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show created files\n",
    "created_files = [\n",
    "    \"configs/tutorial_config.yaml\",\n",
    "    \"training_logs/\" if 'training_monitor' in locals() else \"training_logs/ (would be created)\",\n",
    "    \"tutorial_output/\" if RUN_TRAINING else \"tutorial_output/ (would be created)\"\n",
    "]\n",
    "\n",
    "print(\"üìÅ Files created during this tutorial:\")\n",
    "for file in created_files:\n",
    "    if Path(file).exists() or \"would be\" in file:\n",
    "        print(f\"   ‚úÖ {file}\")\n",
    "\n",
    "# Environment-specific instructions\n",
    "if IN_COLAB:\n",
    "    print(f\"\\nüåê Google Colab Instructions:\")\n",
    "    print(f\"=\" * 30)\n",
    "    \n",
    "    print(f\"üéØ To run training yourself:\")\n",
    "    print(f\"   1. Set RUN_TRAINING = True in the training execution cell\")\n",
    "    print(f\"   2. Make sure you've run the dashboard startup cells above\")\n",
    "    print(f\"   3. Use the public ngrok URL to monitor training\")\n",
    "    \n",
    "    print(f\"\\nüåê Dashboard Access:\")\n",
    "    if 'PUBLIC_URL' in globals() and PUBLIC_URL:\n",
    "        print(f\"   üîó Current public URL: {PUBLIC_URL}\")\n",
    "        print(f\"   üì± Mobile-friendly and shareable\")\n",
    "    else:\n",
    "        print(f\"   üöÄ Run the 'Start Dashboard in Colab' cell to get public URL\")\n",
    "        print(f\"   üîë Remember to set your ngrok auth token\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Colab Limitations:\")\n",
    "    print(f\"   ‚Ä¢ Dashboard URL changes each time you restart\")\n",
    "    print(f\"   ‚Ä¢ Free ngrok URLs are public (anyone with link can access)\")\n",
    "    print(f\"   ‚Ä¢ Session will timeout if inactive for too long\")\n",
    "    \n",
    "    print(f\"\\nüí° Tips for Colab:\")\n",
    "    print(f\"   ‚Ä¢ Keep this notebook tab open while training\")\n",
    "    print(f\"   ‚Ä¢ Save important results to Google Drive\")\n",
    "    print(f\"   ‚Ä¢ Consider ngrok Pro for password-protected URLs\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nüíª Local Development Instructions:\")\n",
    "    print(f\"=\" * 35)\n",
    "    \n",
    "    dashboard_port = tutorial_config['logging']['arbor_tracking']['dashboard_port']\n",
    "    \n",
    "    print(f\"üéØ To run training yourself:\")\n",
    "    print(f\"   1. Set RUN_TRAINING = True in cell 23\")\n",
    "    print(f\"   2. Or run: python train.py configs/tutorial_config.yaml\")\n",
    "\n",
    "    print(f\"\\nüåê Dashboard Access (Local):\")\n",
    "    print(f\"   1. Start dashboard: streamlit run arbor/tracking/dashboard.py --server.port={dashboard_port}\")\n",
    "    print(f\"   2. Open browser: http://localhost:{dashboard_port}\")\n",
    "    print(f\"   3. Or use launcher: python launch_training_dashboard.py\")\n",
    "\n",
    "print(f\"\\nüìä Dashboard Features (All Environments):\")\n",
    "print(f\"   ‚Ä¢ üìà Live training metrics with real-time updates\")\n",
    "print(f\"   ‚Ä¢ üèóÔ∏è Interactive model architecture visualization\")\n",
    "print(f\"   ‚Ä¢ üå± Parameter and layer growth tracking\")\n",
    "print(f\"   ‚Ä¢ üîî Alert system with configurable notifications\")\n",
    "print(f\"   ‚Ä¢ üìä Performance analytics and export capabilities\")\n",
    "print(f\"   ‚Ä¢ üì± Mobile-responsive design (great for Colab!)\")\n",
    "\n",
    "print(f\"\\nüîß To customize:\")\n",
    "print(f\"   1. Edit configs/tutorial_config.yaml\")\n",
    "print(f\"   2. Adjust model size, datasets, training steps\")\n",
    "print(f\"   3. Enable/disable adaptive context and growth\")\n",
    "print(f\"   4. Configure dashboard alerts and monitoring\")\n",
    "\n",
    "print(f\"\\nüìö For more examples:\")\n",
    "print(f\"   ‚Ä¢ Check configs/example_config.yaml\")\n",
    "if not IN_COLAB:\n",
    "    print(f\"   ‚Ä¢ Run python examples/training_with_dashboard.py\")\n",
    "    print(f\"   ‚Ä¢ See COMPLETE_USAGE_GUIDE.md for full documentation\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Upload examples to Colab for local exploration\")\n",
    "    print(f\"   ‚Ä¢ Check the GitHub repo for complete documentation\")\n",
    "\n",
    "print(f\"\\nüå± Happy training with Arbor and live monitoring!\")\n",
    "if IN_COLAB:\n",
    "    print(f\"üåê Perfect for Google Colab with public dashboard access!\")\n",
    "else:\n",
    "    print(f\"üöÄ The future of adaptive AI training is here!\")\n",
    "\n",
    "# Show ngrok tunnel info if in Colab\n",
    "if IN_COLAB and 'PUBLIC_URL' in globals() and PUBLIC_URL:\n",
    "    print(f\"\\nüîó Current Dashboard: {PUBLIC_URL}\")\n",
    "    print(f\"üíæ Bookmark this URL to monitor your training!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
