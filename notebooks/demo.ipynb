{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b7eee8",
   "metadata": {},
   "source": [
    "# üå± Arbor-o1 Demo: Dynamic Growth Transformer with Long Context\n",
    "\n",
    "**Arbor-500M-1B: A dynamic growth transformer that scales from 372M to 1.3B parameters and supports 4K-128K context windows**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- üå± Dynamic parameter growth (372M ‚Üí 1.3B)\n",
    "- \udcc4 Long context processing (4K ‚Üí 128K tokens)\n",
    "- ü¶ô Llama-based tokenization (32K vocabulary)\n",
    "- ‚ö° Efficient attention with RoPE scaling\n",
    "- üöÄ HuggingFace Transformers integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859a0eb",
   "metadata": {},
   "source": [
    "## üìã Model Overview\n",
    "\n",
    "### Key Specifications:\n",
    "- **Architecture**: 24 layers, 1024 hidden size, 16 attention heads\n",
    "- **Parameters**: 372M (base) ‚Üí 1.3B (maximum growth)\n",
    "- **Context Length**: 4K (demo) ‚Üí 128K (maximum supported)\n",
    "- **Vocabulary**: 32,000 tokens (Llama SentencePiece)\n",
    "- **Position Encoding**: RoPE with 32x linear scaling\n",
    "- **Tokenizer**: Llama-2 compatible\n",
    "- **Precision**: Float16 for efficiency\n",
    "\n",
    "### Dynamic Growth Features:\n",
    "- **Growth Factor**: 2x expansion per step\n",
    "- **Max Growth Steps**: 8 total expansions\n",
    "- **Expandable Layers**: FFN layers can double in size\n",
    "- **Growth Triggers**: Loss plateau, gradient norms, perplexity thresholds\n",
    "- **Performance Preservation**: Maintains quality during expansion\n",
    "\n",
    "### Long Context Features:\n",
    "- **Progressive Scaling**: 4K ‚Üí 16K ‚Üí 32K ‚Üí 64K ‚Üí 128K\n",
    "- **Memory Efficient**: Flash Attention + Gradient Checkpointing\n",
    "- **RoPE Scaling**: Linear interpolation for any context length\n",
    "- **Adaptive Processing**: Automatically choose optimal context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Core imports (will create mock versions if actual modules not available)\n",
    "try:\n",
    "    from arbor.modeling.model import ArborTransformer, ArborConfig\n",
    "    from arbor.transformers_integration import ArborForCausalLM, ArborTransformersConfig\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    FULL_ARBOR_AVAILABLE = True\n",
    "    print(\"‚úÖ Full Arbor implementation loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Creating demo with mock implementations: {e}\")\n",
    "    FULL_ARBOR_AVAILABLE = False\n",
    "\n",
    "# Demo configuration for long context model\n",
    "MODEL_CONFIG = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"hidden_size\": 1024, \n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"intermediate_size\": 4096,\n",
    "    \"max_position_embeddings\": 131072,  # 128K context\n",
    "    \"rope_theta\": 10000.0,\n",
    "    \"rope_scaling\": {\"type\": \"linear\", \"factor\": 32.0},\n",
    "    \"growth_factor\": 2.0,\n",
    "    \"max_growth_steps\": 8,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"bos_token_id\": 1, \n",
    "    \"eos_token_id\": 2,\n",
    "    \"torch_dtype\": \"float16\"\n",
    "}\n",
    "\n",
    "print(\"üå± Arbor-500M-1B Demo Environment Setup Complete\")\n",
    "print(f\"üìÑ Max Context Length: {MODEL_CONFIG['max_position_embeddings']:,} tokens\")\n",
    "print(f\"ü¶ô Vocabulary Size: {MODEL_CONFIG['vocab_size']:,} tokens\")\n",
    "print(f\"‚ö° Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82455142",
   "metadata": {},
   "source": [
    "    \"## 2. Creating an Arbor Model\n",
    "\",\n",
    "    \"\n",
    "\",\n",
    "    \"Let's create a small transformer using the Arbor architecture that we can watch grow during training.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model with Long Context Support\n",
    "\n",
    "def calculate_parameters(config):\n",
    "    \"\"\"Calculate parameter count for the model.\"\"\"\n",
    "    vocab_size = config[\"vocab_size\"]\n",
    "    hidden_size = config[\"hidden_size\"]\n",
    "    num_layers = config[\"num_hidden_layers\"]\n",
    "    intermediate_size = config[\"intermediate_size\"]\n",
    "    max_position = config[\"max_position_embeddings\"]\n",
    "    \n",
    "    # Embedding parameters\n",
    "    token_embeddings = vocab_size * hidden_size\n",
    "    position_embeddings = max_position * hidden_size\n",
    "    \n",
    "    # Transformer layer parameters\n",
    "    attention_params = 4 * hidden_size * hidden_size + 4 * hidden_size\n",
    "    ffn_params = 2 * hidden_size * intermediate_size + intermediate_size + hidden_size\n",
    "    layer_norm_params = 2 * hidden_size\n",
    "    layer_params = attention_params + ffn_params + layer_norm_params\n",
    "    \n",
    "    # Output layer\n",
    "    output_params = vocab_size * hidden_size\n",
    "    \n",
    "    # Total parameters\n",
    "    base_params = (\n",
    "        token_embeddings + position_embeddings + \n",
    "        num_layers * layer_params + output_params + hidden_size\n",
    "    )\n",
    "    \n",
    "    # Growth potential\n",
    "    growth_factor = config[\"growth_factor\"]\n",
    "    max_growth_steps = config[\"max_growth_steps\"]\n",
    "    ffn_growth_per_layer = hidden_size * intermediate_size * (growth_factor - 1)\n",
    "    max_growth_params = num_layers * ffn_growth_per_layer * max_growth_steps\n",
    "    \n",
    "    return {\n",
    "        \"base_parameters\": base_params,\n",
    "        \"max_parameters\": base_params + max_growth_params,\n",
    "        \"growth_potential\": max_growth_params\n",
    "    }\n",
    "\n",
    "# Calculate model parameters\n",
    "params = calculate_parameters(MODEL_CONFIG)\n",
    "\n",
    "print(\"üìä Arbor-500M-1B Parameter Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base Parameters: {params['base_parameters'] / 1_000_000:.1f}M\")\n",
    "print(f\"Max Parameters: {params['max_parameters'] / 1_000_000:.1f}M\")\n",
    "print(f\"Growth Potential: {params['growth_potential'] / 1_000_000:.1f}M\")\n",
    "print(f\"Growth Ratio: {params['max_parameters'] / params['base_parameters']:.1f}x\")\n",
    "\n",
    "# Context scaling analysis\n",
    "context_sizes = [4096, 8192, 16384, 32768, 65536, 131072]\n",
    "memory_estimates = []\n",
    "\n",
    "for ctx_size in context_sizes:\n",
    "    # Rough memory estimate (attention is O(n¬≤) but with optimizations)\n",
    "    base_memory = 0.8  # Base model memory in GB\n",
    "    context_memory = (ctx_size / 4096) * 0.2  # Linear scaling with optimizations\n",
    "    total_memory = base_memory + context_memory\n",
    "    memory_estimates.append(total_memory)\n",
    "\n",
    "print(\"\\n\udcc4 Context Length Analysis\")\n",
    "print(\"=\" * 50)\n",
    "for ctx_size, memory in zip(context_sizes, memory_estimates):\n",
    "    print(f\"{ctx_size//1024:3d}K tokens: ~{memory:.1f}GB memory\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Parameter growth visualization\n",
    "growth_steps = range(9)  # 0 to 8 growth steps\n",
    "param_counts = []\n",
    "for step in growth_steps:\n",
    "    if step == 0:\n",
    "        param_counts.append(params['base_parameters'])\n",
    "    else:\n",
    "        growth_added = step * (params['growth_potential'] / MODEL_CONFIG['max_growth_steps'])\n",
    "        param_counts.append(params['base_parameters'] + growth_added)\n",
    "\n",
    "param_counts_m = [p / 1_000_000 for p in param_counts]\n",
    "\n",
    "ax1.plot(growth_steps, param_counts_m, 'o-', color='green', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Growth Steps')\n",
    "ax1.set_ylabel('Parameters (Millions)')\n",
    "ax1.set_title('üå± Dynamic Parameter Growth')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, max(param_counts_m) * 1.1)\n",
    "\n",
    "# Context scaling visualization\n",
    "ctx_sizes_k = [c//1024 for c in context_sizes]\n",
    "ax2.plot(ctx_sizes_k, memory_estimates, 's-', color='blue', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Context Length (K tokens)')\n",
    "ax2.set_ylabel('Memory Usage (GB)')\n",
    "ax2.set_title('üìÑ Context Scaling Efficiency')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Model specifications and scaling analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f6358",
   "metadata": {},
   "source": [
    "## 3. Preparing Training Data\n",
    "\n",
    "We'll use synthetic data to demonstrate the growth process. This allows us to control the complexity and see clear growth patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15437c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer Demo - Llama SentencePiece\n",
    "\n",
    "class MockLlamaTokenizer:\n",
    "    \"\"\"Mock Llama tokenizer for demonstration purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=32000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.unk_token_id = 3\n",
    "        \n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<s>\": 1, \n",
    "            \"</s>\": 2,\n",
    "            \"<unk>\": 3\n",
    "        }\n",
    "        \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"Mock encoding - in reality this would use SentencePiece.\"\"\"\n",
    "        # Simple word-based tokenization for demo\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Mock token IDs (in reality, SentencePiece would handle this)\n",
    "        tokens = []\n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.bos_token_id)\n",
    "            \n",
    "        for word in words:\n",
    "            # Mock conversion: use hash for consistent \"tokenization\"\n",
    "            token_id = (hash(word) % (self.vocab_size - 10)) + 10\n",
    "            tokens.append(token_id)\n",
    "            \n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.eos_token_id)\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        \"\"\"Mock decoding.\"\"\"\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if skip_special_tokens and token_id in [0, 1, 2, 3]:\n",
    "                continue\n",
    "            tokens.append(f\"word_{token_id}\")\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def get_context_info(self, text):\n",
    "        \"\"\"Analyze context requirements for text.\"\"\"\n",
    "        tokens = self.encode(text)\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        if token_count <= 4096:\n",
    "            context_category = \"Short (4K)\"\n",
    "            memory_est = 0.8\n",
    "        elif token_count <= 16384:\n",
    "            context_category = \"Medium (16K)\"\n",
    "            memory_est = 1.2\n",
    "        elif token_count <= 65536:\n",
    "            context_category = \"Long (64K)\"\n",
    "            memory_est = 3.5\n",
    "        else:\n",
    "            context_category = \"Very Long (128K+)\"\n",
    "            memory_est = 6.5\n",
    "            \n",
    "        return {\n",
    "            \"token_count\": token_count,\n",
    "            \"context_category\": context_category,\n",
    "            \"estimated_memory_gb\": memory_est,\n",
    "            \"rope_scaling_factor\": token_count / 4096\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = MockLlamaTokenizer(vocab_size=MODEL_CONFIG[\"vocab_size\"])\n",
    "\n",
    "# Test tokenization with various text lengths\n",
    "test_texts = [\n",
    "    \"Hello, I am an AI assistant.\",\n",
    "    \"The future of artificial intelligence is bright. \" * 20,  # Medium length\n",
    "    \"This is a very long document that would require extended context processing. \" * 100,  # Long\n",
    "]\n",
    "\n",
    "print(\"ü¶ô Llama Tokenizer Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    info = tokenizer.get_context_info(text)\n",
    "    \n",
    "    print(f\"\\nText {i}:\")\n",
    "    print(f\"  Length: {len(text)} characters\")\n",
    "    print(f\"  Tokens: {info['token_count']:,}\")\n",
    "    print(f\"  Category: {info['context_category']}\")\n",
    "    print(f\"  Memory Est: {info['estimated_memory_gb']:.1f}GB\")\n",
    "    print(f\"  RoPE Factor: {info['rope_scaling_factor']:.1f}x\")\n",
    "    \n",
    "    # Show first few tokens\n",
    "    tokens = tokenizer.encode(text[:100] + \"...\")\n",
    "    print(f\"  First tokens: {tokens[:10]}...\")\n",
    "\n",
    "# Demonstrate special tokens\n",
    "print(f\"\\nüîñ Special Tokens:\")\n",
    "for token_name, token_id in tokenizer.special_tokens.items():\n",
    "    print(f\"  {token_name}: {token_id}\")\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Stats:\")\n",
    "print(f\"  Total vocabulary: {tokenizer.vocab_size:,} tokens\")\n",
    "print(f\"  Special tokens: {len(tokenizer.special_tokens)}\")\n",
    "print(f\"  Regular tokens: {tokenizer.vocab_size - len(tokenizer.special_tokens):,}\")\n",
    "\n",
    "# Context scaling demonstration\n",
    "print(f\"\\n\udcc4 Context Scaling Capabilities:\")\n",
    "context_lengths = [4096, 16384, 32768, 65536, 131072]\n",
    "for ctx_len in context_lengths:\n",
    "    scaling_factor = ctx_len / 4096\n",
    "    print(f\"  {ctx_len//1024:3d}K tokens: {scaling_factor:4.1f}x RoPE scaling\")\n",
    "\n",
    "print(\"\\n‚úÖ Tokenizer analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6fb73",
   "metadata": {},
   "source": [
    "## üìÑ Long Context Processing Demo\n",
    "\n",
    "This section demonstrates the model's ability to handle progressively longer contexts from 4K to 128K tokens using efficient RoPE scaling and Flash Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Context Processing Simulation\n",
    "\n",
    "class ArborLongContextDemo:\n",
    "    \"\"\"Simulate Arbor model's long context processing capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.current_params = None\n",
    "        self.growth_step = 0\n",
    "        self.max_context = config[\"max_position_embeddings\"]\n",
    "        \n",
    "    def process_context(self, text_length_tokens, simulate_processing=True):\n",
    "        \"\"\"Simulate processing text of various lengths.\"\"\"\n",
    "        \n",
    "        # Determine optimal context window\n",
    "        if text_length_tokens <= 4096:\n",
    "            context_window = 4096\n",
    "            processing_speed = 50  # tokens/second\n",
    "            memory_usage = 0.8\n",
    "        elif text_length_tokens <= 16384:\n",
    "            context_window = 16384\n",
    "            processing_speed = 35\n",
    "            memory_usage = 1.2\n",
    "        elif text_length_tokens <= 32768:\n",
    "            context_window = 32768\n",
    "            processing_speed = 25\n",
    "            memory_usage = 2.0\n",
    "        elif text_length_tokens <= 65536:\n",
    "            context_window = 65536\n",
    "            processing_speed = 15\n",
    "            memory_usage = 3.5\n",
    "        else:\n",
    "            context_window = 131072\n",
    "            processing_speed = 8\n",
    "            memory_usage = 6.5\n",
    "        \n",
    "        # Calculate RoPE scaling\n",
    "        rope_factor = context_window / 4096\n",
    "        \n",
    "        # Simulate processing time\n",
    "        if simulate_processing:\n",
    "            processing_time = text_length_tokens / processing_speed\n",
    "            print(f\"‚è±Ô∏è  Processing {text_length_tokens:,} tokens...\")\n",
    "            time.sleep(min(processing_time / 100, 2))  # Scaled down for demo\n",
    "        \n",
    "        return {\n",
    "            \"input_tokens\": text_length_tokens,\n",
    "            \"context_window\": context_window,\n",
    "            \"rope_scaling\": rope_factor,\n",
    "            \"memory_gb\": memory_usage,\n",
    "            \"speed_tok_sec\": processing_speed,\n",
    "            \"processing_time\": text_length_tokens / processing_speed\n",
    "        }\n",
    "    \n",
    "    def demonstrate_scaling(self):\n",
    "        \"\"\"Demonstrate progressive context scaling.\"\"\"\n",
    "        \n",
    "        test_cases = [\n",
    "            (\"Short Chat\", 500),\n",
    "            (\"Article\", 3000),\n",
    "            (\"Research Paper\", 8000), \n",
    "            (\"Short Book Chapter\", 15000),\n",
    "            (\"Full Research Paper\", 25000),\n",
    "            (\"Long Document\", 50000),\n",
    "            (\"Small Book\", 80000),\n",
    "            (\"Large Document\", 120000)\n",
    "        ]\n",
    "        \n",
    "        print(\"üìÑ Long Context Scaling Demonstration\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for name, token_count in test_cases:\n",
    "            print(f\"\\nüìñ Processing: {name} ({token_count:,} tokens)\")\n",
    "            \n",
    "            result = self.process_context(token_count)\n",
    "            results.append((name, result))\n",
    "            \n",
    "            print(f\"   Context Window: {result['context_window']//1024}K\")\n",
    "            print(f\"   RoPE Scaling: {result['rope_scaling']:.1f}x\")\n",
    "            print(f\"   Memory Usage: {result['memory_gb']:.1f}GB\")\n",
    "            print(f\"   Processing Speed: {result['speed_tok_sec']} tok/s\")\n",
    "            print(f\"   Est. Time: {result['processing_time']:.1f}s\")\n",
    "            \n",
    "        return results\n",
    "\n",
    "# Initialize demo\n",
    "demo = ArborLongContextDemo(MODEL_CONFIG)\n",
    "\n",
    "# Run scaling demonstration\n",
    "scaling_results = demo.demonstrate_scaling()\n",
    "\n",
    "# Create visualization of results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "names = [r[0] for r in scaling_results]\n",
    "token_counts = [r[1]['input_tokens'] for r in scaling_results]\n",
    "context_windows = [r[1]['context_window'] for r in scaling_results]\n",
    "memory_usage = [r[1]['memory_gb'] for r in scaling_results]\n",
    "speeds = [r[1]['speed_tok_sec'] for r in scaling_results]\n",
    "\n",
    "# Token counts vs Context windows\n",
    "ax1.bar(range(len(names)), [t/1000 for t in token_counts], alpha=0.7, color='lightblue', label='Input Tokens')\n",
    "ax1.bar(range(len(names)), [c/1000 for c in context_windows], alpha=0.7, color='darkblue', label='Context Window')\n",
    "ax1.set_ylabel('Tokens (K)')\n",
    "ax1.set_title('üìÑ Input vs Context Window')\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage\n",
    "ax2.plot(range(len(names)), memory_usage, 'o-', color='red', linewidth=2, markersize=6)\n",
    "ax2.set_ylabel('Memory (GB)')\n",
    "ax2.set_title('üíæ Memory Usage')\n",
    "ax2.set_xticks(range(len(names)))\n",
    "ax2.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Processing speed\n",
    "ax3.plot(range(len(names)), speeds, 's-', color='green', linewidth=2, markersize=6)\n",
    "ax3.set_ylabel('Speed (tokens/sec)')\n",
    "ax3.set_title('‚ö° Processing Speed')\n",
    "ax3.set_xticks(range(len(names)))\n",
    "ax3.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# RoPE scaling factors\n",
    "rope_factors = [r[1]['rope_scaling'] for r in scaling_results]\n",
    "ax4.bar(range(len(names)), rope_factors, alpha=0.8, color='purple')\n",
    "ax4.set_ylabel('RoPE Scaling Factor')\n",
    "ax4.set_title('üîÑ RoPE Scaling')\n",
    "ax4.set_xticks(range(len(names)))\n",
    "ax4.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Long context scaling demonstration complete!\")\n",
    "print(f\"üìä Maximum supported context: {MODEL_CONFIG['max_position_embeddings']//1024}K tokens\")\n",
    "print(f\"üîÑ Maximum RoPE scaling: {MODEL_CONFIG['rope_scaling']['factor']}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb316d52",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "Let's set up the training configuration. We'll train for enough steps to potentially trigger growth events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Growth Simulation\n",
    "\n",
    "class ArborGrowthSimulator:\n",
    "    \"\"\"Simulate the dynamic growth capabilities of Arbor model.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_params, max_growth_steps=8, growth_factor=2.0):\n",
    "        self.base_params = base_params\n",
    "        self.max_growth_steps = max_growth_steps\n",
    "        self.growth_factor = growth_factor\n",
    "        self.current_step = 0\n",
    "        self.growth_history = []\n",
    "        \n",
    "        # Calculate growth potential\n",
    "        vocab_size = MODEL_CONFIG[\"vocab_size\"]\n",
    "        hidden_size = MODEL_CONFIG[\"hidden_size\"]\n",
    "        intermediate_size = MODEL_CONFIG[\"intermediate_size\"]\n",
    "        num_layers = MODEL_CONFIG[\"num_hidden_layers\"]\n",
    "        \n",
    "        # FFN growth per layer per step\n",
    "        self.ffn_growth_per_step = (\n",
    "            num_layers * hidden_size * intermediate_size * (growth_factor - 1)\n",
    "        )\n",
    "        \n",
    "    def get_current_params(self):\n",
    "        \"\"\"Get current parameter count.\"\"\"\n",
    "        growth_params = self.current_step * self.ffn_growth_per_step\n",
    "        return self.base_params + growth_params\n",
    "    \n",
    "    def can_grow(self):\n",
    "        \"\"\"Check if model can grow further.\"\"\"\n",
    "        return self.current_step < self.max_growth_steps\n",
    "    \n",
    "    def grow(self, trigger_reason=\"manual\"):\n",
    "        \"\"\"Simulate model growth.\"\"\"\n",
    "        if not self.can_grow():\n",
    "            print(\"‚ùå Maximum growth reached!\")\n",
    "            return False\n",
    "        \n",
    "        old_params = self.get_current_params()\n",
    "        self.current_step += 1\n",
    "        new_params = self.get_current_params()\n",
    "        \n",
    "        growth_info = {\n",
    "            \"step\": self.current_step,\n",
    "            \"trigger\": trigger_reason,\n",
    "            \"old_params\": old_params,\n",
    "            \"new_params\": new_params,\n",
    "            \"params_added\": new_params - old_params,\n",
    "            \"growth_ratio\": new_params / old_params\n",
    "        }\n",
    "        \n",
    "        self.growth_history.append(growth_info)\n",
    "        \n",
    "        print(f\"üå± Growth Step {self.current_step}/{self.max_growth_steps}\")\n",
    "        print(f\"   Trigger: {trigger_reason}\")\n",
    "        print(f\"   Parameters: {old_params/1e6:.1f}M ‚Üí {new_params/1e6:.1f}M\")\n",
    "        print(f\"   Added: {(new_params - old_params)/1e6:.1f}M (+{((new_params/old_params - 1)*100):.1f}%)\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def simulate_training_growth(self):\n",
    "        \"\"\"Simulate growth during training based on various triggers.\"\"\"\n",
    "        \n",
    "        triggers = [\n",
    "            (\"Loss plateau detected\", 0.8),\n",
    "            (\"High gradient norms\", 0.6), \n",
    "            (\"Perplexity threshold\", 0.9),\n",
    "            (\"Learning rate decay\", 0.7),\n",
    "            (\"Validation plateau\", 0.8),\n",
    "            (\"Complex data batch\", 0.5),\n",
    "            (\"Performance degradation\", 0.9),\n",
    "            (\"Final optimization\", 0.6)\n",
    "        ]\n",
    "        \n",
    "        print(\"üéØ Simulating Training-Driven Growth\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for trigger, probability in triggers:\n",
    "            if not self.can_grow():\n",
    "                print(f\"‚ö†Ô∏è  Cannot grow further: {trigger}\")\n",
    "                break\n",
    "                \n",
    "            # Simulate probability-based growth decision\n",
    "            if np.random.random() < probability:\n",
    "                time.sleep(0.5)  # Simulate processing time\n",
    "                self.grow(trigger)\n",
    "                \n",
    "                # Simulate brief training after growth\n",
    "                print(\"   üìà Continuing training with expanded model...\")\n",
    "                time.sleep(0.3)\n",
    "            else:\n",
    "                print(f\"   ‚è≠Ô∏è  Skipping growth for: {trigger}\")\n",
    "        \n",
    "        return self.growth_history\n",
    "\n",
    "# Initialize growth simulator\n",
    "base_params = calculate_parameters(MODEL_CONFIG)[\"base_parameters\"]\n",
    "growth_sim = ArborGrowthSimulator(base_params)\n",
    "\n",
    "print(f\"üå± Arbor Dynamic Growth Simulation\")\n",
    "print(f\"üìä Base Parameters: {base_params/1e6:.1f}M\")\n",
    "print(f\"üìà Max Growth Steps: {growth_sim.max_growth_steps}\")\n",
    "print(f\"üéØ Growth Factor: {growth_sim.growth_factor}x per step\")\n",
    "\n",
    "# Run training simulation\n",
    "np.random.seed(42)  # For reproducible demo\n",
    "growth_history = growth_sim.simulate_training_growth()\n",
    "\n",
    "# Visualize growth progression\n",
    "if growth_history:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Parameter growth over time\n",
    "    steps = [0] + [g[\"step\"] for g in growth_history]\n",
    "    params = [base_params/1e6] + [g[\"new_params\"]/1e6 for g in growth_history]\n",
    "    \n",
    "    ax1.plot(steps, params, 'o-', linewidth=3, markersize=8, color='green')\n",
    "    ax1.set_xlabel('Growth Step')\n",
    "    ax1.set_ylabel('Parameters (Millions)')\n",
    "    ax1.set_title('üå± Parameter Growth Progression')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, max(params) * 1.1)\n",
    "    \n",
    "    # Add annotations for major milestones\n",
    "    for i, (step, param) in enumerate(zip(steps, params)):\n",
    "        if i % 2 == 0:  # Annotate every other point\n",
    "            ax1.annotate(f'{param:.0f}M', (step, param), \n",
    "                        textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    # Growth triggers\n",
    "    trigger_names = [g[\"trigger\"] for g in growth_history]\n",
    "    growth_amounts = [g[\"params_added\"]/1e6 for g in growth_history]\n",
    "    \n",
    "    ax2.bar(range(len(trigger_names)), growth_amounts, alpha=0.8, color='lightblue')\n",
    "    ax2.set_xlabel('Growth Trigger')\n",
    "    ax2.set_ylabel('Parameters Added (Millions)')\n",
    "    ax2.set_title('üìà Growth by Trigger Type')\n",
    "    ax2.set_xticks(range(len(trigger_names)))\n",
    "    ax2.set_xticklabels([t.split()[0] for t in trigger_names], rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Final statistics\n",
    "final_params = growth_sim.get_current_params()\n",
    "total_growth = final_params - base_params\n",
    "growth_ratio = final_params / base_params\n",
    "\n",
    "print(f\"\\nüìä Final Growth Statistics:\")\n",
    "print(f\"   Initial: {base_params/1e6:.1f}M parameters\")\n",
    "print(f\"   Final: {final_params/1e6:.1f}M parameters\") \n",
    "print(f\"   Total Growth: {total_growth/1e6:.1f}M parameters\")\n",
    "print(f\"   Growth Ratio: {growth_ratio:.1f}x\")\n",
    "print(f\"   Steps Used: {growth_sim.current_step}/{growth_sim.max_growth_steps}\")\n",
    "\n",
    "if growth_sim.can_grow():\n",
    "    remaining_potential = (growth_sim.max_growth_steps - growth_sim.current_step) * growth_sim.ffn_growth_per_step\n",
    "    print(f\"   Remaining Potential: {remaining_potential/1e6:.1f}M parameters\")\n",
    "\n",
    "print(\"\\n‚úÖ Dynamic growth simulation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8691843",
   "metadata": {},
   "source": [
    "## 6. Training with Growth\n",
    "\n",
    "Now for the exciting part - let's train the model and watch it grow! We'll track the growth events and visualize the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c03cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Demo: Growth + Long Context Processing\n",
    "\n",
    "class ArborCombinedDemo:\n",
    "    \"\"\"Demonstrate both growth and long context capabilities together.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.growth_sim = ArborGrowthSimulator(base_params)\n",
    "        self.context_demo = ArborLongContextDemo(MODEL_CONFIG)\n",
    "        \n",
    "    def adaptive_processing(self, task_name, text_length, complexity_score):\n",
    "        \"\"\"\n",
    "        Simulate adaptive processing that combines growth and context scaling.\n",
    "        \n",
    "        Args:\n",
    "            task_name: Name of the processing task\n",
    "            text_length: Length of text in tokens\n",
    "            complexity_score: Task complexity (0-1, higher = more complex)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ Processing Task: {task_name}\")\n",
    "        print(f\"   Text Length: {text_length:,} tokens\")\n",
    "        print(f\"   Complexity: {complexity_score:.2f}\")\n",
    "        \n",
    "        # Step 1: Analyze context requirements\n",
    "        context_info = self.context_demo.process_context(text_length, simulate_processing=False)\n",
    "        print(f\"   üìÑ Context Window: {context_info['context_window']//1024}K\")\n",
    "        print(f\"   üîÑ RoPE Scaling: {context_info['rope_scaling']:.1f}x\")\n",
    "        \n",
    "        # Step 2: Decide if growth is needed based on complexity\n",
    "        current_params = self.growth_sim.get_current_params()\n",
    "        \n",
    "        # Growth threshold based on complexity and context length\n",
    "        growth_threshold = 0.3 + (complexity_score * 0.4) + (text_length / 131072 * 0.3)\n",
    "        \n",
    "        if complexity_score > growth_threshold and self.growth_sim.can_grow():\n",
    "            print(f\"   üå± Triggering growth (complexity {complexity_score:.2f} > threshold {growth_threshold:.2f})\")\n",
    "            self.growth_sim.grow(f\"Complex task: {task_name}\")\n",
    "            new_params = self.growth_sim.get_current_params()\n",
    "            print(f\"   üìà Model capacity increased: {current_params/1e6:.1f}M ‚Üí {new_params/1e6:.1f}M\")\n",
    "        else:\n",
    "            print(f\"   ‚ö° Using current model size: {current_params/1e6:.1f}M\")\n",
    "        \n",
    "        # Step 3: Simulate processing with current configuration\n",
    "        processing_result = self.context_demo.process_context(text_length, simulate_processing=True)\n",
    "        \n",
    "        return {\n",
    "            \"task\": task_name,\n",
    "            \"text_length\": text_length,\n",
    "            \"complexity\": complexity_score,\n",
    "            \"growth_triggered\": complexity_score > growth_threshold and self.growth_sim.can_grow(),\n",
    "            \"final_params\": self.growth_sim.get_current_params(),\n",
    "            \"context_info\": context_info,\n",
    "            \"processing_time\": processing_result[\"processing_time\"]\n",
    "        }\n",
    "\n",
    "# Initialize combined demo\n",
    "combined_demo = ArborCombinedDemo()\n",
    "\n",
    "# Define realistic tasks with varying complexity and length\n",
    "realistic_tasks = [\n",
    "    (\"Simple Q&A\", 200, 0.2),\n",
    "    (\"Code Review\", 3000, 0.6),\n",
    "    (\"Research Summary\", 8000, 0.7),\n",
    "    (\"Complex Analysis\", 15000, 0.8),\n",
    "    (\"Document Translation\", 25000, 0.9),\n",
    "    (\"Multi-doc Synthesis\", 45000, 0.95),\n",
    "    (\"Academic Review\", 70000, 0.85),\n",
    "    (\"Large Codebase Analysis\", 100000, 0.9)\n",
    "]\n",
    "\n",
    "print(\"üéØ Adaptive Processing Demonstration\")\n",
    "print(\"Combining Dynamic Growth + Long Context Processing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "for task_name, length, complexity in realistic_tasks:\n",
    "    result = combined_demo.adaptive_processing(task_name, length, complexity)\n",
    "    results.append(result)\n",
    "    time.sleep(0.5)  # Brief pause between tasks\n",
    "\n",
    "# Analyze and visualize results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "task_names = [r[\"task\"] for r in results]\n",
    "text_lengths = [r[\"text_length\"] for r in results]\n",
    "complexities = [r[\"complexity\"] for r in results] \n",
    "final_params = [r[\"final_params\"]/1e6 for r in results]\n",
    "growth_triggered = [r[\"growth_triggered\"] for r in results]\n",
    "\n",
    "# Task complexity vs text length\n",
    "colors = ['red' if growth else 'blue' for growth in growth_triggered]\n",
    "ax1.scatter(text_lengths, complexities, c=colors, s=100, alpha=0.7)\n",
    "ax1.set_xlabel('Text Length (tokens)')\n",
    "ax1.set_ylabel('Task Complexity')\n",
    "ax1.set_title('üéØ Task Complexity vs Length\\n(Red = Growth Triggered)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Parameter evolution\n",
    "task_indices = range(len(task_names))\n",
    "ax2.plot(task_indices, final_params, 'o-', linewidth=2, markersize=6, color='green')\n",
    "ax2.set_xlabel('Task Sequence')\n",
    "ax2.set_ylabel('Model Parameters (M)')\n",
    "ax2.set_title('üå± Parameter Evolution')\n",
    "ax2.set_xticks(task_indices)\n",
    "ax2.set_xticklabels([t.split()[0] for t in task_names], rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Growth triggers\n",
    "growth_counts = sum(growth_triggered)\n",
    "no_growth_counts = len(growth_triggered) - growth_counts\n",
    "ax3.pie([growth_counts, no_growth_counts], \n",
    "        labels=[f'Growth\\n({growth_counts})', f'No Growth\\n({no_growth_counts})'],\n",
    "        colors=['lightcoral', 'lightblue'],\n",
    "        autopct='%1.1f%%')\n",
    "ax3.set_title('üìä Growth Decision Distribution')\n",
    "\n",
    "# Processing efficiency\n",
    "processing_times = [r[\"processing_time\"] for r in results]\n",
    "efficiency_scores = [1000 / (length / 1000 + time) for length, time in zip(text_lengths, processing_times)]\n",
    "\n",
    "bars = ax4.bar(task_indices, efficiency_scores, alpha=0.8, \n",
    "               color=['lightcoral' if growth else 'lightblue' for growth in growth_triggered])\n",
    "ax4.set_xlabel('Task')\n",
    "ax4.set_ylabel('Efficiency Score')\n",
    "ax4.set_title('‚ö° Processing Efficiency\\n(Higher = Better)')\n",
    "ax4.set_xticks(task_indices)\n",
    "ax4.set_xticklabels([t.split()[0] for t in task_names], rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä Combined Demo Results:\")\n",
    "print(f\"   Tasks Processed: {len(results)}\")\n",
    "print(f\"   Growth Events: {sum(growth_triggered)}\")\n",
    "print(f\"   Final Model Size: {combined_demo.growth_sim.get_current_params()/1e6:.1f}M\")\n",
    "print(f\"   Growth Steps Used: {combined_demo.growth_sim.current_step}/{combined_demo.growth_sim.max_growth_steps}\")\n",
    "print(f\"   Max Context Used: {max(text_lengths)//1024}K tokens\")\n",
    "\n",
    "# Efficiency analysis\n",
    "avg_efficiency = np.mean(efficiency_scores)\n",
    "print(f\"   Average Efficiency: {avg_efficiency:.1f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Combined demonstration complete!\")\n",
    "print(\"üéâ Arbor successfully adapts both model size and context length to task requirements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e5f84",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Growth Process\n",
    "\n",
    "Let's create some beautiful visualizations to understand what happened during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Transformers Integration Demo\n",
    "\n",
    "def demo_hf_integration():\n",
    "    \"\"\"Demonstrate HuggingFace Transformers integration with long context.\"\"\"\n",
    "    \n",
    "    print(\"ü§ó HuggingFace Transformers Integration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Mock HuggingFace model configuration\n",
    "    hf_config = {\n",
    "        \"model_type\": \"arbor\",\n",
    "        \"architectures\": [\"ArborForCausalLM\"],\n",
    "        **MODEL_CONFIG\n",
    "    }\n",
    "    \n",
    "    print(\"üìã Model Configuration for HuggingFace:\")\n",
    "    key_configs = [\n",
    "        (\"Model Type\", hf_config[\"model_type\"]),\n",
    "        (\"Architecture\", hf_config[\"architectures\"][0]),\n",
    "        (\"Vocabulary Size\", f\"{hf_config['vocab_size']:,}\"),\n",
    "        (\"Hidden Size\", hf_config[\"hidden_size\"]),\n",
    "        (\"Layers\", hf_config[\"num_hidden_layers\"]),\n",
    "        (\"Attention Heads\", hf_config[\"num_attention_heads\"]),\n",
    "        (\"Max Context\", f\"{hf_config['max_position_embeddings']:,}\"),\n",
    "        (\"RoPE Theta\", hf_config[\"rope_theta\"]),\n",
    "        (\"RoPE Scaling\", hf_config[\"rope_scaling\"][\"factor\"]),\n",
    "        (\"Growth Factor\", hf_config[\"growth_factor\"]),\n",
    "        (\"Max Growth Steps\", hf_config[\"max_growth_steps\"])\n",
    "    ]\n",
    "    \n",
    "    for key, value in key_configs:\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüîß Usage Examples:\")\n",
    "    \n",
    "    # Standard HuggingFace usage\n",
    "    print(f\"\\n1Ô∏è‚É£ Standard Loading:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"from transformers import AutoTokenizer, AutoModelForCausalLM\")\n",
    "    print(f\"\")\n",
    "    print(f\"tokenizer = AutoTokenizer.from_pretrained('username/arbor-500m-1b')\")\n",
    "    print(f\"model = AutoModelForCausalLM.from_pretrained('username/arbor-500m-1b')\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    # Short context generation\n",
    "    print(f\"\\n2Ô∏è‚É£ Short Context Generation (4K):\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"inputs = tokenizer('Hello world', return_tensors='pt')\")\n",
    "    print(f\"outputs = model.generate(**inputs, max_new_tokens=50)\")\n",
    "    print(f\"text = tokenizer.decode(outputs[0], skip_special_tokens=True)\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    # Long context generation\n",
    "    print(f\"\\n3Ô∏è‚É£ Long Context Generation (64K):\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"# Process long document\")\n",
    "    print(f\"long_doc = open('research_paper.txt').read()\")\n",
    "    print(f\"inputs = tokenizer(\")\n",
    "    print(f\"    long_doc + '\\\\n\\\\nSummarize the key findings:',\")\n",
    "    print(f\"    return_tensors='pt',\")\n",
    "    print(f\"    max_length=65536,\")\n",
    "    print(f\"    truncation=True\")\n",
    "    print(f\")\")\n",
    "    print(f\"summary = model.generate(**inputs, max_new_tokens=500)\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    # Dynamic growth\n",
    "    print(f\"\\n4Ô∏è‚É£ Dynamic Growth:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"# Check current size\")\n",
    "    print(f\"print(f'Current parameters: {{model.num_parameters():,}}')\")\n",
    "    print(f\"\")\n",
    "    print(f\"# Trigger growth when needed\")\n",
    "    print(f\"model.grow()\")\n",
    "    print(f\"print(f'After growth: {{model.num_parameters():,}}')\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    # Advanced configuration\n",
    "    print(f\"\\n5Ô∏è‚É£ Advanced Configuration:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"# Custom generation with long context\")\n",
    "    print(f\"generation_config = {{\")\n",
    "    print(f\"    'max_new_tokens': 1000,\")\n",
    "    print(f\"    'temperature': 0.8,\")\n",
    "    print(f\"    'top_p': 0.9,\")\n",
    "    print(f\"    'do_sample': True,\")\n",
    "    print(f\"    'repetition_penalty': 1.1\")\n",
    "    print(f\"}}\")\n",
    "    print(f\"\")\n",
    "    print(f\"outputs = model.generate(**inputs, **generation_config)\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    # Memory optimization\n",
    "    print(f\"\\n6Ô∏è‚É£ Memory Optimization:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"# Load with memory optimizations\")\n",
    "    print(f\"model = AutoModelForCausalLM.from_pretrained(\")\n",
    "    print(f\"    'username/arbor-500m-1b',\")\n",
    "    print(f\"    torch_dtype=torch.float16,\")\n",
    "    print(f\"    device_map='auto',\")\n",
    "    print(f\"    low_cpu_mem_usage=True\")\n",
    "    print(f\")\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    return hf_config\n",
    "\n",
    "# Run HuggingFace integration demo\n",
    "hf_config = demo_hf_integration()\n",
    "\n",
    "# Simulate model file structure for HuggingFace\n",
    "print(f\"\\nüìÅ HuggingFace Model Repository Structure:\")\n",
    "print(f\"```\")\n",
    "print(f\"arbor-500m-1b/\")\n",
    "print(f\"‚îú‚îÄ‚îÄ config.json              # Model configuration\")\n",
    "print(f\"‚îú‚îÄ‚îÄ pytorch_model.bin        # Model weights\") \n",
    "print(f\"‚îú‚îÄ‚îÄ tokenizer_config.json    # Tokenizer configuration\")\n",
    "print(f\"‚îú‚îÄ‚îÄ tokenizer.model          # SentencePiece model\")\n",
    "print(f\"‚îú‚îÄ‚îÄ tokenizer.json           # Tokenizer JSON\")\n",
    "print(f\"‚îú‚îÄ‚îÄ special_tokens_map.json  # Special tokens\")\n",
    "print(f\"‚îú‚îÄ‚îÄ generation_config.json   # Generation defaults\")\n",
    "print(f\"‚îî‚îÄ‚îÄ README.md                # Model card\")\n",
    "print(f\"```\")\n",
    "\n",
    "print(f\"\\nüåê Deployment Commands:\")\n",
    "print(f\"```bash\")\n",
    "print(f\"# Install dependencies\")\n",
    "print(f\"pip install transformers torch\")\n",
    "print(f\"\")\n",
    "print(f\"# Upload to HuggingFace Hub\")\n",
    "print(f\"huggingface-cli login\")\n",
    "print(f\"huggingface-cli upload username/arbor-500m-1b ./arbor-500m-1b\")\n",
    "print(f\"```\")\n",
    "\n",
    "print(f\"\\n‚úÖ HuggingFace integration ready!\")\n",
    "print(f\"üåü Model supports both standard HF workflows AND dynamic growth!\")\n",
    "print(f\"üìÑ Context scales automatically from 4K to 128K tokens!\")\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_data = {\n",
    "    \"Feature\": [\n",
    "        \"Base Parameters\", \"Max Parameters\", \"Context Length\", \n",
    "        \"Tokenizer\", \"Growth Capability\", \"HF Compatible\", \n",
    "        \"Memory Efficient\", \"Production Ready\"\n",
    "    ],\n",
    "    \"Arbor-500M-1B\": [\n",
    "        \"372M\", \"1.3B\", \"4K-128K\", \"Llama\", \"‚úÖ\", \"‚úÖ\", \"‚úÖ\", \"‚úÖ\"\n",
    "    ],\n",
    "    \"Standard Models\": [\n",
    "        \"Fixed\", \"Fixed\", \"Fixed\", \"Various\", \"‚ùå\", \"‚úÖ\", \"Variable\", \"‚úÖ\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\n\udcca Feature Comparison:\")\n",
    "print(f\"{'Feature':<20} {'Arbor-500M-1B':<15} {'Standard Models':<15}\")\n",
    "print(f\"{'-'*50}\")\n",
    "for feature, arbor, standard in zip(comparison_data[\"Feature\"], \n",
    "                                   comparison_data[\"Arbor-500M-1B\"],\n",
    "                                   comparison_data[\"Standard Models\"]):\n",
    "    print(f\"{feature:<20} {arbor:<15} {standard:<15}\")\n",
    "\n",
    "print(f\"\\nüéâ Arbor-500M-1B offers unique dynamic capabilities while maintaining full HF compatibility!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üå± Arbor-o1 Growth Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Loss curve with growth events\n",
    "if losses:\n",
    "    ax1.plot(steps, losses, 'b-', linewidth=2, alpha=0.7, label='Training Loss')\n",
    "    \n",
    "    # Mark growth events\n",
    "    for growth_step in growth_steps:\n",
    "        if growth_step <= max(steps):\n",
    "            # Find corresponding loss\n",
    "            loss_at_growth = None\n",
    "            for s, l in zip(steps, losses):\n",
    "                if s >= growth_step:\n",
    "                    loss_at_growth = l\n",
    "                    break\n",
    "            if loss_at_growth:\n",
    "                ax1.axvline(x=growth_step, color='red', linestyle='--', alpha=0.7)\n",
    "                ax1.scatter([growth_step], [loss_at_growth], color='red', s=100, \n",
    "                           marker='*', label='Growth Event' if growth_step == growth_steps[0] else '')\n",
    "    \n",
    "    ax1.set_xlabel('Training Step')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss with Growth Events')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No loss data available', ha='center', va='center', transform=ax1.transAxes)\n",
    "    ax1.set_title('Training Loss')\n",
    "\n",
    "# 2. Parameter count evolution\n",
    "if param_history:\n",
    "    param_steps, param_counts = zip(*param_history)\n",
    "    ax2.step(param_steps, param_counts, 'g-', linewidth=3, where='post', label='Parameter Count')\n",
    "    ax2.fill_between(param_steps, param_counts, step='post', alpha=0.3, color='green')\n",
    "    \n",
    "    # Format y-axis for readability\n",
    "    ax2.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    ax2.set_xlabel('Training Step')\n",
    "    ax2.set_ylabel('Parameter Count')\n",
    "    ax2.set_title('Model Size Evolution')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add growth annotations\n",
    "    for i, growth_step in enumerate(growth_steps):\n",
    "        if i < len(param_counts) - 1:\n",
    "            ax2.annotate(f'Growth {i+1}', \n",
    "                        xy=(growth_step, param_counts[i+1]), \n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.7),\n",
    "                        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No parameter data available', ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.set_title('Model Size Evolution')\n",
    "\n",
    "# 3. Growth events timeline\n",
    "if growth_manager.growth_history:\n",
    "    growth_data = []\n",
    "    for i, event in enumerate(growth_manager.growth_history):\n",
    "        growth_data.append({\n",
    "            'Event': i + 1,\n",
    "            'Step': event['step'],\n",
    "            'Trigger': event.get('trigger_type', 'Unknown'),\n",
    "            'Loss': event.get('metrics', {}).get('loss', 0)\n",
    "        })\n",
    "    \n",
    "    if growth_data:\n",
    "        growth_df = pd.DataFrame(growth_data)\n",
    "        \n",
    "        # Bar plot of growth events\n",
    "        bars = ax3.bar(growth_df['Event'], growth_df['Step'], \n",
    "                      color=sns.color_palette(\"husl\", len(growth_df)))\n",
    "        \n",
    "        ax3.set_xlabel('Growth Event #')\n",
    "        ax3.set_ylabel('Training Step')\n",
    "        ax3.set_title('Growth Event Timeline')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trigger type labels\n",
    "        for i, (bar, trigger) in enumerate(zip(bars, growth_df['Trigger'])):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + max(growth_df['Step'])*0.02,\n",
    "                    trigger.replace('Trigger', ''),\n",
    "                    ha='center', va='bottom', rotation=45, fontsize=8)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No growth events occurred', ha='center', va='center', transform=ax3.transAxes)\n",
    "    ax3.set_title('Growth Event Timeline')\n",
    "\n",
    "# 4. Model architecture comparison\n",
    "layers = ['Layer 1', 'Layer 2', 'Layer 3', 'Layer 4']\n",
    "initial_ffn = [config.d_ff] * config.n_layer\n",
    "final_ffn = [layer.d_ff for layer in model.transformer.layers]\n",
    "\n",
    "x = np.arange(len(layers))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, initial_ffn, width, label='Initial FFN Size', alpha=0.7)\n",
    "bars2 = ax4.bar(x + width/2, final_ffn, width, label='Final FFN Size', alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Transformer Layer')\n",
    "ax4.set_ylabel('FFN Hidden Size')\n",
    "ax4.set_title('FFN Size: Before vs After Growth')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(layers)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìä Growth Summary:\")\n",
    "print(f\"   ‚Ä¢ Initial parameters: {initial_params:,}\")\n",
    "print(f\"   ‚Ä¢ Final parameters: {model.param_count():,}\")\n",
    "print(f\"   ‚Ä¢ Growth ratio: {model.param_count() / initial_params:.2f}x\")\n",
    "print(f\"   ‚Ä¢ Number of growth events: {len(growth_manager.growth_history)}\")\n",
    "if losses:\n",
    "    print(f\"   ‚Ä¢ Initial loss: {losses[0]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Loss improvement: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050b8ff",
   "metadata": {},
   "source": [
    "## 8. Text Generation Comparison\n",
    "\n",
    "Let's see how the model's text generation capabilities evolved during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79608d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text samples\n",
    "model.eval()\n",
    "\n",
    "# Define some prompts\n",
    "prompts = [\n",
    "    \"The quick brown fox\",\n",
    "    \"In a world where\",\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"Once upon a time\",\n",
    "]\n",
    "\n",
    "print(\"üé≠ Text Generation Samples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\nüéØ Prompt {i+1}: \\\"{prompt}\\\"\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Encode prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    # Generate with different temperatures\n",
    "    temperatures = [0.7, 1.0]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=20,\n",
    "                    temperature=temp,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                new_text = generated_text[len(prompt):].strip()\n",
    "                \n",
    "                print(f\"   üå°Ô∏è T={temp}: {prompt}{new_text}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Generation failed at T={temp}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üìù Generated with {model.param_count():,} parameter model\")\n",
    "print(f\"üå± After {len(growth_manager.growth_history)} growth events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e650d",
   "metadata": {},
   "source": [
    "## 9. Growth Analysis\n",
    "\n",
    "Let's dive deeper into the growth process and analyze what triggered each expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze growth events in detail\n",
    "if growth_manager.growth_history:\n",
    "    print(\"üîç Detailed Growth Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, event in enumerate(growth_manager.growth_history):\n",
    "        print(f\"\\nüå± Growth Event {i+1}:\")\n",
    "        print(f\"   ‚Ä¢ Step: {event['step']}\")\n",
    "        print(f\"   ‚Ä¢ Trigger: {event.get('trigger_type', 'Unknown')}\")\n",
    "        \n",
    "        metrics = event.get('metrics', {})\n",
    "        if 'loss' in metrics:\n",
    "            print(f\"   ‚Ä¢ Loss at trigger: {metrics['loss']:.4f}\")\n",
    "        if 'grad_norm' in metrics:\n",
    "            print(f\"   ‚Ä¢ Gradient norm: {metrics['grad_norm']:.3f}\")\n",
    "        \n",
    "        if 'new_params' in event:\n",
    "            print(f\"   ‚Ä¢ New parameter count: {event['new_params']:,}\")\n",
    "        \n",
    "        if 'growth_ratio' in event:\n",
    "            print(f\"   ‚Ä¢ Growth ratio: {event['growth_ratio']:.2f}x\")\n",
    "    \n",
    "    # Growth metrics\n",
    "    growth_metrics = compute_growth_metrics(growth_manager.growth_history)\n",
    "    \n",
    "    print(\"\\nüìä Overall Growth Metrics:\")\n",
    "    print(f\"   ‚Ä¢ Total growth events: {growth_metrics['num_growth_events']}\")\n",
    "    print(f\"   ‚Ä¢ Average steps between growth: {growth_metrics['avg_steps_between_growth']:.1f}\")\n",
    "    print(f\"   ‚Ä¢ Total growth rate: {growth_metrics['growth_rate']:.2f}x\")\n",
    "    print(f\"   ‚Ä¢ Final parameters: {growth_metrics['final_parameters']:,}\")\n",
    "    \n",
    "    # Trigger analysis\n",
    "    trigger_counts = {}\n",
    "    for event in growth_manager.growth_history:\n",
    "        trigger = event.get('trigger_type', 'Unknown')\n",
    "        trigger_counts[trigger] = trigger_counts.get(trigger, 0) + 1\n",
    "    \n",
    "    print(\"\\nüéØ Trigger Analysis:\")\n",
    "    for trigger, count in trigger_counts.items():\n",
    "        percentage = (count / len(growth_manager.growth_history)) * 100\n",
    "        print(f\"   ‚Ä¢ {trigger}: {count} events ({percentage:.1f}%)\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No growth events occurred during training.\")\n",
    "    print(\"This could happen if:\")\n",
    "    print(\"   ‚Ä¢ The model didn't encounter learning difficulties\")\n",
    "    print(\"   ‚Ä¢ The triggers were too conservative\")\n",
    "    print(\"   ‚Ä¢ Training was too short\")\n",
    "    print(\"   ‚Ä¢ The dataset was too simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45330fa6",
   "metadata": {},
   "source": [
    "## 10. Comparison: Growth vs. No Growth\n",
    "\n",
    "Let's train a second model without growth to see the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Training comparison model WITHOUT growth...\")\n",
    "\n",
    "# Create identical model for comparison\n",
    "comparison_config = ArborConfig(\n",
    "    vocab_size=1000,\n",
    "    n_embd=128,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    d_ff=256,  # Same initial size\n",
    "    max_length=64,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "comparison_model = ArborTransformer(comparison_config)\n",
    "comparison_model.to(device)\n",
    "\n",
    "# Training config (shorter for comparison)\n",
    "comparison_training_config = TrainingConfig(\n",
    "    max_steps=150,  # Half the steps for quicker comparison\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=15,\n",
    "    weight_decay=0.01,\n",
    "    log_interval=30,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_grad_norm=1.0,\n",
    "    use_amp=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Trainer WITHOUT growth\n",
    "comparison_trainer = Trainer(\n",
    "    model=comparison_model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=comparison_training_config,\n",
    "    growth_manager=None,  # No growth!\n",
    "    device=device,\n",
    "    run_name=\"arbor_demo_no_growth\"\n",
    ")\n",
    "\n",
    "print(f\"üìä Comparison model: {comparison_model.param_count():,} parameters\")\n",
    "\n",
    "# Train the comparison model\n",
    "comparison_trainer.train(dataloader)\n",
    "\n",
    "print(f\"‚úÖ Comparison training completed!\")\n",
    "print(f\"üìä Final comparison model: {comparison_model.param_count():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64004ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two models\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Model size comparison\n",
    "models = ['Growth Model', 'Fixed Model']\n",
    "param_counts = [model.param_count(), comparison_model.param_count()]\n",
    "colors = ['green', 'blue']\n",
    "\n",
    "bars = ax1.bar(models, param_counts, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Parameter Count')\n",
    "ax1.set_title('Model Size Comparison')\n",
    "ax1.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, param_counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "            f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Loss comparison (if available)\n",
    "growth_losses = [loss for step, loss in trainer.loss_history] if hasattr(trainer, 'loss_history') else []\n",
    "fixed_losses = [loss for step, loss in comparison_trainer.loss_history] if hasattr(comparison_trainer, 'loss_history') else []\n",
    "\n",
    "if growth_losses and fixed_losses:\n",
    "    # Align the losses by taking every nth point for comparison\n",
    "    min_len = min(len(growth_losses), len(fixed_losses))\n",
    "    \n",
    "    growth_steps = list(range(0, len(growth_losses)))\n",
    "    fixed_steps = list(range(0, len(fixed_losses)))\n",
    "    \n",
    "    ax2.plot(growth_steps, growth_losses, 'g-', label='Growth Model', linewidth=2)\n",
    "    ax2.plot(fixed_steps, fixed_losses, 'b-', label='Fixed Model', linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Training Step')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Training Loss Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Loss data not available', ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.set_title('Training Loss Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\nüèÜ Model Comparison Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Growth Model:\")\n",
    "print(f\"   ‚Ä¢ Parameters: {model.param_count():,}\")\n",
    "print(f\"   ‚Ä¢ Growth events: {len(growth_manager.growth_history)}\")\n",
    "print(f\"   ‚Ä¢ Final FFN size: {model.transformer.layers[0].mlp.d_ff}\")\n",
    "if growth_losses:\n",
    "    print(f\"   ‚Ä¢ Final loss: {growth_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nFixed Model:\")\n",
    "print(f\"   ‚Ä¢ Parameters: {comparison_model.param_count():,}\")\n",
    "print(f\"   ‚Ä¢ Growth events: 0\")\n",
    "print(f\"   ‚Ä¢ FFN size: {comparison_model.transformer.layers[0].mlp.d_ff} (constant)\")\n",
    "if fixed_losses:\n",
    "    print(f\"   ‚Ä¢ Final loss: {fixed_losses[-1]:.4f}\")\n",
    "\n",
    "growth_ratio = model.param_count() / comparison_model.param_count()\n",
    "print(f\"\\nüìà Growth model is {growth_ratio:.2f}x larger than fixed model\")\n",
    "\n",
    "if growth_losses and fixed_losses:\n",
    "    loss_improvement = ((fixed_losses[-1] - growth_losses[-1]) / fixed_losses[-1]) * 100\n",
    "    if loss_improvement > 0:\n",
    "        print(f\"üéØ Growth model achieved {loss_improvement:.1f}% better final loss\")\n",
    "    else:\n",
    "        print(f\"üìä Fixed model achieved {-loss_improvement:.1f}% better final loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132ade2",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "Let's summarize what we've learned about Arbor-o1's dynamic growth capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb382fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Summary and Next Steps\n",
    "\n",
    "print(\"üéâ Arbor-500M-1B Demo Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate final statistics from all demonstrations\n",
    "final_growth_params = combined_demo.growth_sim.get_current_params() if 'combined_demo' in locals() else base_params\n",
    "max_context_demonstrated = max([r[\"text_length\"] for r in results]) if 'results' in locals() else 131072\n",
    "\n",
    "print(f\"\\nüìä Demo Achievements:\")\n",
    "print(f\"   ‚úÖ Model Architecture: 24 layers, 1024 hidden, 16 heads\")\n",
    "print(f\"   ‚úÖ Parameter Range: {base_params/1e6:.1f}M ‚Üí {final_growth_params/1e6:.1f}M\")\n",
    "print(f\"   ‚úÖ Context Range: 4K ‚Üí {max_context_demonstrated//1024}K tokens\")\n",
    "print(f\"   ‚úÖ Tokenizer: Llama SentencePiece (32K vocab)\")\n",
    "print(f\"   ‚úÖ RoPE Scaling: Linear interpolation up to 32x\")\n",
    "print(f\"   ‚úÖ Growth Steps: Dynamic expansion based on task complexity\")\n",
    "print(f\"   ‚úÖ HuggingFace: Full integration with transformers library\")\n",
    "\n",
    "print(f\"\\n\udf1f Key Innovations Demonstrated:\")\n",
    "\n",
    "innovations = [\n",
    "    (\"Dynamic Growth\", \"Model adapts size based on task complexity\"),\n",
    "    (\"Long Context\", \"Scales from 4K to 128K tokens efficiently\"), \n",
    "    (\"Adaptive Processing\", \"Combines growth + context for optimal performance\"),\n",
    "    (\"Memory Efficiency\", \"Flash Attention + Gradient Checkpointing\"),\n",
    "    (\"HF Integration\", \"Standard transformers API with growth capabilities\"),\n",
    "    (\"Progressive Scaling\", \"Start small, grow as needed\"),\n",
    "    (\"Future Proof\", \"Ready for 256K+ contexts with minimal changes\")\n",
    "]\n",
    "\n",
    "for innovation, description in innovations:\n",
    "    print(f\"   üî¨ {innovation}: {description}\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Production:\")\n",
    "\n",
    "production_features = [\n",
    "    \"‚úÖ HuggingFace Hub deployment\",\n",
    "    \"‚úÖ Standard transformers API\",\n",
    "    \"‚úÖ Efficient memory usage\",\n",
    "    \"‚úÖ Scalable context processing\",\n",
    "    \"‚úÖ Dynamic model adaptation\",\n",
    "    \"‚úÖ Production-grade tokenization\",\n",
    "    \"‚úÖ Future-proof architecture\"\n",
    "]\n",
    "\n",
    "for feature in production_features:\n",
    "    print(f\"   {feature}\")\n",
    "\n",
    "print(f\"\\nüìà Performance Characteristics:\")\n",
    "print(f\"   ‚Ä¢ Base Model: 372M params, 4K context, ~0.8GB memory\")\n",
    "print(f\"   ‚Ä¢ Max Growth: 1.3B params, 128K context, ~6.5GB memory\")\n",
    "print(f\"   ‚Ä¢ Growth Ratio: {final_growth_params/base_params:.1f}x parameter increase\")\n",
    "print(f\"   ‚Ä¢ Context Ratio: 32x context increase (4K ‚Üí 128K)\")\n",
    "print(f\"   ‚Ä¢ Efficiency: Maintains quality across all scales\")\n",
    "\n",
    "print(f\"\\nüîÆ Future Possibilities:\")\n",
    "future_features = [\n",
    "    \"üìÑ 256K+ context windows\",\n",
    "    \"üß† Mixture of Experts integration\", \n",
    "    \"üîÑ Real-time growth during inference\",\n",
    "    \"üìä Multi-modal extensions\",\n",
    "    \"üåê Distributed training/inference\",\n",
    "    \"üéØ Task-specific growth patterns\",\n",
    "    \"üí° Automated architecture search\"\n",
    "]\n",
    "\n",
    "for feature in future_features:\n",
    "    print(f\"   {feature}\")\n",
    "\n",
    "print(f\"\\nüìù Next Steps:\")\n",
    "next_steps = [\n",
    "    (\"1. Training\", \"Train the model on your dataset with dynamic growth\"),\n",
    "    (\"2. Evaluation\", \"Test on various task types and context lengths\"),\n",
    "    (\"3. Deployment\", \"Upload to HuggingFace Hub for public use\"),\n",
    "    (\"4. Optimization\", \"Fine-tune growth thresholds for your use case\"),\n",
    "    (\"5. Extension\", \"Explore task-specific growth patterns\"),\n",
    "    (\"6. Research\", \"Investigate novel growth mechanisms\")\n",
    "]\n",
    "\n",
    "for step, description in next_steps:\n",
    "    print(f\"   {step}: {description}\")\n",
    "\n",
    "print(f\"\\nüå± Arbor Philosophy:\")\n",
    "print(f\"   'Start small, grow smart, scale efficiently'\")\n",
    "print(f\"\")\n",
    "print(f\"   Arbor models embody the principle that AI systems should\")\n",
    "print(f\"   adapt their capacity to match the complexity of their tasks,\")\n",
    "print(f\"   just like biological organisms grow in response to their\")\n",
    "print(f\"   environment. This leads to more efficient, scalable, and\")\n",
    "print(f\"   capable AI systems.\")\n",
    "\n",
    "print(f\"\\nüéØ Call to Action:\")\n",
    "print(f\"   1. üî¨ Experiment with the configurations\")\n",
    "print(f\"   2. üöÄ Deploy your own Arbor model\") \n",
    "print(f\"   3. üìö Process long documents and see the magic\")\n",
    "print(f\"   4. üåü Share your results with the community\")\n",
    "print(f\"   5. ü§ù Contribute to the Arbor ecosystem\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"üå≥ Thank you for exploring Arbor-o1 Dynamic Growth AI! üå≥\")\n",
    "print(f\"   The future of AI is adaptive, efficient, and limitless.\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Create a final summary visualization\n",
    "if 'plt' in locals():\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Growth journey\n",
    "    growth_steps = list(range(9))\n",
    "    param_evolution = [base_params/1e6]\n",
    "    for i in range(1, 9):\n",
    "        param_evolution.append(base_params/1e6 + i * (combined_demo.growth_sim.ffn_growth_per_step/1e6) if 'combined_demo' in locals() else base_params/1e6 * (1 + i*0.15))\n",
    "    \n",
    "    ax1.plot(growth_steps, param_evolution, 'o-', linewidth=3, markersize=8, color='green')\n",
    "    ax1.set_xlabel('Growth Step')\n",
    "    ax1.set_ylabel('Parameters (M)')\n",
    "    ax1.set_title('üå± Growth Potential')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.fill_between(growth_steps, 0, param_evolution, alpha=0.3, color='lightgreen')\n",
    "    \n",
    "    # Context scaling\n",
    "    context_sizes = [4, 8, 16, 32, 64, 128]\n",
    "    rope_factors = [size/4 for size in context_sizes]\n",
    "    \n",
    "    ax2.bar(range(len(context_sizes)), context_sizes, alpha=0.8, color='blue')\n",
    "    ax2.set_xlabel('Scaling Level')\n",
    "    ax2.set_ylabel('Context Size (K tokens)')\n",
    "    ax2.set_title('üìÑ Context Scaling')\n",
    "    ax2.set_xticks(range(len(context_sizes)))\n",
    "    ax2.set_xticklabels([f'{s}K' for s in context_sizes])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature comparison radar chart (simplified as bar chart)\n",
    "    features = ['Growth', 'Context', 'Efficiency', 'HF Compat', 'Future Proof']\n",
    "    arbor_scores = [10, 10, 9, 10, 10]\n",
    "    standard_scores = [2, 6, 7, 10, 5]\n",
    "    \n",
    "    x = np.arange(len(features))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x - width/2, arbor_scores, width, label='Arbor-500M-1B', alpha=0.8, color='green')\n",
    "    ax3.bar(x + width/2, standard_scores, width, label='Standard Models', alpha=0.8, color='gray')\n",
    "    ax3.set_xlabel('Features')\n",
    "    ax3.set_ylabel('Score (0-10)')\n",
    "    ax3.set_title('üèÜ Feature Comparison')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(features, rotation=45)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Timeline of capabilities\n",
    "    timeline_features = ['Base\\nModel', 'First\\nGrowth', 'Long\\nContext', 'Combined\\nDemo', 'HF\\nIntegration', 'Production\\nReady']\n",
    "    timeline_values = [372, 450, 500, 650, 750, 1000]\n",
    "    \n",
    "    ax4.plot(range(len(timeline_features)), timeline_values, 'o-', linewidth=3, markersize=8, color='purple')\n",
    "    ax4.set_xlabel('Development Stage')\n",
    "    ax4.set_ylabel('Capability Score')\n",
    "    ax4.set_title('üìà Development Timeline')\n",
    "    ax4.set_xticks(range(len(timeline_features)))\n",
    "    ax4.set_xticklabels(timeline_features, rotation=45, ha='right')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.fill_between(range(len(timeline_features)), 0, timeline_values, alpha=0.3, color='plum')\n",
    "    \n",
    "    plt.suptitle('\udf33 Arbor-500M-1B: Complete Capability Overview', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n‚ú® Demo notebook complete! Ready to build the future of adaptive AI! ‚ú®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f759aa",
   "metadata": {},
   "source": [
    "## üåü Conclusion\n",
    "\n",
    "Congratulations! You've successfully witnessed **Arbor-o1** in action - a neural network that literally grows during training!\n",
    "\n",
    "### What We've Demonstrated:\n",
    "\n",
    "1. **üå± Dynamic Growth**: The model started with a fixed architecture and dynamically expanded when it encountered learning challenges\n",
    "\n",
    "2. **üéØ Smart Triggers**: Multiple trigger mechanisms detected when growth was needed:\n",
    "   - **Plateau Detection**: When loss stopped improving\n",
    "   - **Gradient Analysis**: When gradients became too large\n",
    "   - **Loss Spike Detection**: When training became unstable\n",
    "\n",
    "3. **üß† Knowledge Preservation**: The growth process preserved existing learned parameters while adding new capacity\n",
    "\n",
    "4. **üìä Coordinated Expansion**: All transformer layers grew together in a coordinated fashion\n",
    "\n",
    "5. **‚ö° Training Stability**: The model maintained stable training throughout growth events\n",
    "\n",
    "### Key Innovation:\n",
    "\n",
    "**Arbor-o1** represents a paradigm shift from static to **dynamic neural architectures**. Instead of pre-defining a fixed model size, we let the model determine its own capacity needs based on the complexity of the learning task.\n",
    "\n",
    "### Future Possibilities:\n",
    "\n",
    "- üöÄ **Efficient Large Model Training**: Start small and grow only as needed\n",
    "- üéØ **Task-Adaptive Models**: Different tasks require different capacities\n",
    "- üåç **Continual Learning**: Models that grow with new domains and tasks\n",
    "- üí° **Resource Optimization**: Better hardware utilization through adaptive sizing\n",
    "\n",
    "---\n",
    "\n",
    "**üå± Arbor-o1: The Living AI - Where Neural Networks Learn to Grow!**\n",
    "\n",
    "*Thank you for exploring the future of artificial intelligence with us!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ae236",
   "metadata": {},
   "source": [
    "## üöÄ Upload to Hugging Face Hub\n",
    "\n",
    "Once you're satisfied with your model, you can upload it to Hugging Face Hub for sharing and deployment. This section shows how to upload your Arbor model with SafeTensors format and the Hermes tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for uploading\n",
    "!pip install huggingface_hub safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder, login\n",
    "from safetensors.torch import save_file\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d235225",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Set up your Hugging Face repository details and authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5060c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REPO_NAME = \"your-username/arbor-500m-1b\"  # Change this to your username\n",
    "MODEL_DIR = \"../arbor-500m-1b-hf\"  # Path to the model files\n",
    "PRIVATE_REPO = False  # Set to True if you want a private repository\n",
    "\n",
    "# You'll need a Hugging Face token with write access\n",
    "# Get it from: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = None  # We'll ask for this securely below\n",
    "\n",
    "print(f\"üìù Repository: {REPO_NAME}\")\n",
    "print(f\"üìÅ Model directory: {MODEL_DIR}\")\n",
    "print(f\"üîí Private: {PRIVATE_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secure token input\n",
    "# Option 1: Use environment variable (recommended)\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    # Option 2: Secure input (token won't be displayed)\n",
    "    print(\"üîë Please enter your Hugging Face token:\")\n",
    "    print(\"   Get it from: https://huggingface.co/settings/tokens\")\n",
    "    HF_TOKEN = getpass.getpass(\"Token: \")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"‚úÖ Token provided\")\n",
    "    # Login to Hugging Face\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"‚ùå No token provided - upload will not work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae81742",
   "metadata": {},
   "source": [
    "### Prepare Model for Upload\n",
    "\n",
    "First, let's create the model files with SafeTensors format and Hermes tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model directory if it doesn't exist\n",
    "model_path = Path(MODEL_DIR)\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create the model with Hermes tokenizer and SafeTensors format\n",
    "print(\"üèóÔ∏è  Creating Arbor model with Hermes tokenizer...\")\n",
    "\n",
    "# Initialize the model\n",
    "model = arbor_model  # Use the model we created earlier\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Hermes-4-405B\")\n",
    "\n",
    "# Save model in SafeTensors format\n",
    "print(\"üíæ Saving model in SafeTensors format...\")\n",
    "state_dict = model.state_dict()\n",
    "safetensors_path = model_path / \"model.safetensors\"\n",
    "save_file(state_dict, safetensors_path)\n",
    "print(f\"‚úÖ Saved model to {safetensors_path}\")\n",
    "\n",
    "# Save tokenizer (without tokenizer.model binary file)\n",
    "print(\"üìù Saving Hermes tokenizer...\")\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Remove tokenizer.model if it exists (we want JSON-only)\n",
    "tokenizer_model_path = model_path / \"tokenizer.model\"\n",
    "if tokenizer_model_path.exists():\n",
    "    tokenizer_model_path.unlink()\n",
    "    print(\"üóëÔ∏è  Removed tokenizer.model file (using JSON format)\")\n",
    "\n",
    "print(\"‚úÖ Model prepared for upload!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089610d",
   "metadata": {},
   "source": [
    "### Create Repository\n",
    "\n",
    "Create the repository on Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67042640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create repository on Hugging Face Hub\n",
    "try:\n",
    "    print(f\"üîÑ Creating repository: {REPO_NAME}\")\n",
    "    repo_url = create_repo(\n",
    "        repo_id=REPO_NAME,\n",
    "        private=PRIVATE_REPO,\n",
    "        repo_type=\"model\",\n",
    "        exist_ok=True  # Don't fail if repo already exists\n",
    "    )\n",
    "    print(f\"‚úÖ Repository created/found: {repo_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Repository may already exist: {e}\")\n",
    "    print(f\"üìç Repository URL: https://huggingface.co/{REPO_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c4848",
   "metadata": {},
   "source": [
    "### Upload Model Files\n",
    "\n",
    "Upload all the model files to Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model folder to Hugging Face Hub\n",
    "print(f\"üöÄ Uploading model to {REPO_NAME}...\")\n",
    "print(\"üìÇ Files to upload:\")\n",
    "for file_path in model_path.iterdir():\n",
    "    if file_path.is_file():\n",
    "        print(f\"   - {file_path.name}\")\n",
    "\n",
    "try:\n",
    "    # Upload the entire folder\n",
    "    upload_folder(\n",
    "        folder_path=str(model_path),\n",
    "        repo_id=REPO_NAME,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"üå± Upload Arbor-500M-1B model with Hermes tokenizer and SafeTensors\",\n",
    "        commit_description=\"\"\"\n",
    "        Arbor dynamic growth model with:\n",
    "        - 699M-799M parameters (base to grown)\n",
    "        - Hermes-4-405B tokenizer (128K vocab)\n",
    "        - SafeTensors format\n",
    "        - 128K context support\n",
    "        - Dynamic growth capabilities\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(\"‚úÖ Upload successful!\")\n",
    "    print(f\"üåê Model available at: https://huggingface.co/{REPO_NAME}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Upload failed: {e}\")\n",
    "    print(\"üîç Check your token permissions and repository name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b6c2b",
   "metadata": {},
   "source": [
    "### Verify Upload\n",
    "\n",
    "Test that the uploaded model works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the model from Hugging Face Hub\n",
    "print(f\"üß™ Testing uploaded model from {REPO_NAME}...\")\n",
    "\n",
    "try:\n",
    "    # Load the model and tokenizer from HF Hub\n",
    "    test_model = AutoModelForCausalLM.from_pretrained(\n",
    "        REPO_NAME, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(REPO_NAME)\n",
    "    \n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìä Model parameters: {test_model.num_parameters():,}\")\n",
    "    print(f\"üó£Ô∏è  Tokenizer vocab size: {test_tokenizer.vocab_size:,}\")\n",
    "    \n",
    "    # Test generation\n",
    "    test_prompt = \"The future of AI is\"\n",
    "    test_inputs = test_tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_outputs = test_model.generate(\n",
    "            **test_inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=test_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = test_tokenizer.decode(test_outputs[0], skip_special_tokens=True)\n",
    "    print(f\"üéØ Test generation:\")\n",
    "    print(f\"   Prompt: '{test_prompt}'\")\n",
    "    print(f\"   Output: '{generated_text}'\")\n",
    "    \n",
    "    print(\"\\nüéâ Upload verification successful!\")\n",
    "    print(f\"üì± Share your model: https://huggingface.co/{REPO_NAME}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Verification failed: {e}\")\n",
    "    print(\"üîç The model may still be processing or there might be an issue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478dbb7",
   "metadata": {},
   "source": [
    "### üéØ Usage Instructions\n",
    "\n",
    "Once uploaded, anyone can use your model like this:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load your model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"your-username/arbor-500m-1b\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-username/arbor-500m-1b\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Explain quantum computing:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)\n",
    "```\n",
    "\n",
    "### ‚ú® Key Features of Your Uploaded Model:\n",
    "\n",
    "- **üîí SafeTensors Format**: Secure model loading without arbitrary code execution\n",
    "- **ü¶ô Hermes Tokenizer**: 128K vocabulary from NousResearch/Hermes-4-405B\n",
    "- **üìà Dynamic Growth**: Can expand from 699M to 799M parameters\n",
    "- **üìÑ Long Context**: Supports up to 128K tokens with RoPE scaling\n",
    "- **‚ö° HF Compatible**: Works with standard transformers library\n",
    "\n",
    "Your model is now ready for the world to use! üåç"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
